[
  {
    "chunk_id": "5f12cc4a-12fe-4c70-b6b8-52df52c75226",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 1,
    "retrieval_text": "9 2019 1 0 2 y a M 4 2 ] L C . s c [ 2 v 5 0 8 4 0 . 0 1 8 1 : v i X r a BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout}@google.com Abstract We introduce a new language representa- tion model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be \ufb01ne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speci\ufb01c architecture modi\ufb01cations. There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and \ufb01ne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speci\ufb01c architectures that include the pre-trained representations as addi- tional features. The \ufb01ne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-speci\ufb01c parameters, and is trained on the downstream tasks by simply \ufb01ne-tuning all pre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "raw_text": "9 2019 1 0 2 y a M 4 2 ] L C . s c [ 2 v 5 0 8 4 0 . 0 1 8 1 : v i X r a BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout}@google.com Abstract We introduce a new language representa- tion model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be \ufb01ne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speci\ufb01c architecture modi\ufb01cations. There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and \ufb01ne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speci\ufb01c architectures that include the pre-trained representations as addi- tional features. The \ufb01ne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-speci\ufb01c parameters, and is trained on the downstream tasks by simply \ufb01ne-tuning all pre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
  },
  {
    "chunk_id": "c3798864-09d3-4850-b896-428febcc8031",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 1,
    "retrieval_text": "1 Introduction Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce \ufb01ne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the \ufb01ne-tuning approaches. The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying \ufb01ne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions. In this paper, we improve the \ufb01ne-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidi- rectionality constraint by using a \u201cmasked lan- guage model\u201d (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to- right language model pre-training, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a \u201cnext sentence prediction\u201d task that jointly pre- trains text-pair representations. The contributions of our paper are as follows: \u2022 We demonstrate the importance of bidirectional pre-training for language representations. Un- like Radford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. \u2022 We show that pre-trained representations reduce the need for many heavily-engineered task- speci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outper- forming many task-speci\ufb01c architectures. \u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained mod- els are available at https://github.com/ google-research/bert.",
    "raw_text": "1 Introduction Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce \ufb01ne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the \ufb01ne-tuning approaches. The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying \ufb01ne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions. In this paper, we improve the \ufb01ne-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidi- rectionality constraint by using a \u201cmasked lan- guage model\u201d (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to- right language model pre-training, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a \u201cnext sentence prediction\u201d task that jointly pre- trains text-pair representations. The contributions of our paper are as follows: \u2022 We demonstrate the importance of bidirectional pre-training for language representations. Un- like Radford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. \u2022 We show that pre-trained representations reduce the need for many heavily-engineered task- speci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outper- forming many task-speci\ufb01c architectures. \u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained mod- els are available at https://github.com/ google-research/bert."
  },
  {
    "chunk_id": "0752bd1a-d251-4d54-b6ce-f1ee8b7a7953",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 2,
    "retrieval_text": "2 Related Work There is a long history of pre-training general lan- guage representations, and we brie\ufb02y review the most widely-used approaches in this section. 2.1 Unsupervised Feature-based Approaches Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, of- fering signi\ufb01cant improvements over embeddings learned from scratch (Turian et al., 2010). To pre- train word embedding vectors, left-to-right lan- guage modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to dis- criminate correct from incorrect words in left and right context (Mikolov et al., 2013). These approaches have been generalized to coarser granularities, such as sentence embed- dings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sen- tence words given a representation of the previous sentence (Kiros et al., 2015), or denoising auto- encoder derived objectives (Hill et al., 2016). ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding re- search along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual rep- resentation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-speci\ufb01c architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including ques- tion answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to pre- dict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation mod- els.",
    "raw_text": "2 Related Work There is a long history of pre-training general lan- guage representations, and we brie\ufb02y review the most widely-used approaches in this section. 2.1 Unsupervised Feature-based Approaches Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, of- fering signi\ufb01cant improvements over embeddings learned from scratch (Turian et al., 2010). To pre- train word embedding vectors, left-to-right lan- guage modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to dis- criminate correct from incorrect words in left and right context (Mikolov et al., 2013). These approaches have been generalized to coarser granularities, such as sentence embed- dings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sen- tence words given a representation of the previous sentence (Kiros et al., 2015), or denoising auto- encoder derived objectives (Hill et al., 2016). ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding re- search along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual rep- resentation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-speci\ufb01c architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including ques- tion answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to pre- dict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation mod- els."
  },
  {
    "chunk_id": "1703388f-7d27-4dff-b024-6470652c85e1",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 2,
    "retrieval_text": "2.2 Unsupervised Fine-tuning Approaches As with the feature-based approaches, the \ufb01rst works in this direction only pre-trained word em- bedding parameters from unlabeled text (Col- lobert and Weston, 2008). More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and \ufb01ne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model- Starv/End cer Masked Sentence A Masked Sentence B Unlabeled Sentence A and B Pair Pre-training Question KAA Fine-Tuning t Question Answer Pair Paragraph / Figure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec- tures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers). ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015). mal difference between the pre-trained architec- ture and the \ufb01nal downstream architecture. 2.3 Transfer Learning from Supervised Data There has also been work showing effective trans- fer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demon- strated the importance of transfer learning from large pre-trained models, where an effective recipe is to \ufb01ne-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014). Model Architecture BERT\u2019s model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our im- plementation is almost identical to the original, we will omit an exhaustive background descrip- tion of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d2",
    "raw_text": "2.2 Unsupervised Fine-tuning Approaches As with the feature-based approaches, the \ufb01rst works in this direction only pre-trained word em- bedding parameters from unlabeled text (Col- lobert and Weston, 2008). More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and \ufb01ne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model- Starv/End cer Masked Sentence A Masked Sentence B Unlabeled Sentence A and B Pair Pre-training Question KAA Fine-Tuning t Question Answer Pair Paragraph / Figure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec- tures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers). ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015). mal difference between the pre-trained architec- ture and the \ufb01nal downstream architecture. 2.3 Transfer Learning from Supervised Data There has also been work showing effective trans- fer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demon- strated the importance of transfer learning from large pre-trained models, where an effective recipe is to \ufb01ne-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014). Model Architecture BERT\u2019s model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our im- plementation is almost identical to the original, we will omit an exhaustive background descrip- tion of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \u201cThe Annotated Transformer.\u201d2"
  },
  {
    "chunk_id": "5bc3a158-e8c6-412f-81ec-40d7c73090b8",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 3,
    "retrieval_text": "3 BERT We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: pre-training and \ufb01ne-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks. For \ufb01ne- tuning, the BERT model is \ufb01rst initialized with the pre-trained parameters, and all of the param- eters are \ufb01ne-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate \ufb01ne-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section. A distinctive feature of BERT is its uni\ufb01ed ar- chitecture across different tasks. There is mini- In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Param- eters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.4 1https://github.com/tensor\ufb02ow/tensor2tensor 2http://nlp.seas.harvard.edu/2018/04/03/attention.html 3In all cases we set the feed-forward/\ufb01lter size to be 4H, i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans- Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together. We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The \ufb01rst token of every sequence is always a special clas- si\ufb01cation token ([CLS]). The \ufb01nal hidden state corresponding to this token is used as the ag- gregate sequence representation for classi\ufb01cation tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the \ufb01nal hidden vector of the special [CLS] token as C \u2208 RH, and the \ufb01nal hidden vector for the ith input token as Ti \u2208 RH. For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualiza- tion of this construction can be seen in Figure 2.",
    "raw_text": "3 BERT We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: pre-training and \ufb01ne-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks. For \ufb01ne- tuning, the BERT model is \ufb01rst initialized with the pre-trained parameters, and all of the param- eters are \ufb01ne-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate \ufb01ne-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section. A distinctive feature of BERT is its uni\ufb01ed ar- chitecture across different tasks. There is mini- In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Param- eters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.4 1https://github.com/tensor\ufb02ow/tensor2tensor 2http://nlp.seas.harvard.edu/2018/04/03/attention.html 3In all cases we set the feed-forward/\ufb01lter size to be 4H, i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans- Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (Question, Answer )) in one token sequence. Throughout this work, a \u201csentence\u201d can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together. We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The \ufb01rst token of every sequence is always a special clas- si\ufb01cation token ([CLS]). The \ufb01nal hidden state corresponding to this token is used as the ag- gregate sequence representation for classi\ufb01cation tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the \ufb01nal hidden vector of the special [CLS] token as C \u2208 RH, and the \ufb01nal hidden vector for the ith input token as Ti \u2208 RH. For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualiza- tion of this construction can be seen in Figure 2."
  },
  {
    "chunk_id": "52ce610c-712f-4e74-b2fe-060d92736b0f",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 4,
    "retrieval_text": "3.1 Pre-training BERT Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsuper- vised tasks, described in this section. This step is presented in the left part of Figure 1. Task #1: Masked LM Intuitively, it is reason- able to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirec- tional conditioning would allow each word to in- directly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context. former is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation. In order to train a deep bidirectional representa- tion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the \ufb01nal hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece to- kens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input. Although this allows us to obtain a bidirec- tional pre-trained model, a downside is that we are creating a mismatch between pre-training and \ufb01ne-tuning, since the [MASK] token does not ap- pear during \ufb01ne-tuning. To mitigate this, we do not always replace \u201cmasked\u201d words with the ac- tual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2. Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Ques- tion Answering (QA) and Natural Language Infer- ence (NLI) are based on understanding the rela- tionship between two sentences, which is not di- rectly captured by language modeling. In order to train a model that understands sentence rela- tionships, we pre-train for a binarized next sen- tence prediction task that can be trivially gener- ated from any monolingual corpus. Speci\ufb01cally, when choosing the sentences A and B for each pre- training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence predic- tion (NSP).5 Despite its simplicity, we demon- strate in Section 5.1 that pre-training towards this task is very bene\ufb01cial to both QA and NLI. 6 5The \ufb01nal model achieves 97%-98% accuracy on NSP. 6The vector C is not a meaningful sentence representation without \ufb01ne-tuning, since it was trained with NSP. Input {cs} my dog is cute [SEP] he | likes play ##ing | [SEP] Token Embeddings Eris) En Ek0g ES cute E sep) Exe Elikes Enay Ex sing E sep) + + + + + + + + + + + Segment Embeddings E, Ey E. E, E, E, E, E. EB E, E. + + + + + + + + + + + Position Embeddings E, E E, E, E, E. E. E, E, E, Exo Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta- tion embeddings and the position embeddings. The NSP task is closely related to representation- learning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all pa- rameters to initialize end-task model parameters. (4) a degenerate text-\u2205 pair in text classi\ufb01cation or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classi\ufb01cation, such as en- tailment or sentiment analysis. Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is criti- cal to use a document-level corpus rather than a shuf\ufb02ed sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences. Compared to pre-training, \ufb01ne-tuning is rela- tively inexpensive. All of the results in the pa- per can be replicated in at most 1 hour on a sin- gle Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7 We de- scribe the task-speci\ufb01c details in the correspond- ing subsections of Section 4. More details can be found in Appendix A.5.",
    "raw_text": "3.1 Pre-training BERT Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsuper- vised tasks, described in this section. This step is presented in the left part of Figure 1. Task #1: Masked LM Intuitively, it is reason- able to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirec- tional conditioning would allow each word to in- directly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context. former is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation. In order to train a deep bidirectional representa- tion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the \ufb01nal hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece to- kens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input. Although this allows us to obtain a bidirec- tional pre-trained model, a downside is that we are creating a mismatch between pre-training and \ufb01ne-tuning, since the [MASK] token does not ap- pear during \ufb01ne-tuning. To mitigate this, we do not always replace \u201cmasked\u201d words with the ac- tual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2. Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Ques- tion Answering (QA) and Natural Language Infer- ence (NLI) are based on understanding the rela- tionship between two sentences, which is not di- rectly captured by language modeling. In order to train a model that understands sentence rela- tionships, we pre-train for a binarized next sen- tence prediction task that can be trivially gener- ated from any monolingual corpus. Speci\ufb01cally, when choosing the sentences A and B for each pre- training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence predic- tion (NSP).5 Despite its simplicity, we demon- strate in Section 5.1 that pre-training towards this task is very bene\ufb01cial to both QA and NLI. 6 5The \ufb01nal model achieves 97%-98% accuracy on NSP. 6The vector C is not a meaningful sentence representation without \ufb01ne-tuning, since it was trained with NSP. Input {cs} my dog is cute [SEP] he | likes play ##ing | [SEP] Token Embeddings Eris) En Ek0g ES cute E sep) Exe Elikes Enay Ex sing E sep) + + + + + + + + + + + Segment Embeddings E, Ey E. E, E, E, E, E. EB E, E. + + + + + + + + + + + Position Embeddings E, E E, E, E, E. E. E, E, E, Exo Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta- tion embeddings and the position embeddings. The NSP task is closely related to representation- learning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all pa- rameters to initialize end-task model parameters. (4) a degenerate text-\u2205 pair in text classi\ufb01cation or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classi\ufb01cation, such as en- tailment or sentiment analysis. Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is criti- cal to use a document-level corpus rather than a shuf\ufb02ed sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences. Compared to pre-training, \ufb01ne-tuning is rela- tively inexpensive. All of the results in the pa- per can be replicated in at most 1 hour on a sin- gle Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7 We de- scribe the task-speci\ufb01c details in the correspond- ing subsections of Section 4. More details can be found in Appendix A.5."
  },
  {
    "chunk_id": "a8566bb5-2800-4f15-bc0d-420ddeff8e97",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 5,
    "retrieval_text": "4 Experiments 3.2 Fine-tuning BERT In this section, we present BERT \ufb01ne-tuning re- sults on 11 NLP tasks. Fine-tuning is straightforward since the self- attention mechanism in the Transformer al- lows BERT to model many downstream tasks\u2014 whether they involve single text or text pairs\u2014by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs be- fore applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidi- rectional cross attention between two sentences. For each task, we simply plug in the task- speci\ufb01c inputs and outputs into BERT and \ufb01ne- tune all the parameters end-to-end. At the in- put, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphras- ing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and 4.1 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a col- lection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1. To \ufb01ne-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the \ufb01nal hid- den vector C \u2208 RH corresponding to the \ufb01rst input token ([CLS]) as the aggregate representa- tion. The only new parameters introduced during \ufb01ne-tuning are classi\ufb01cation layer weights W \u2208 RK\u00d7H, where K is the number of labels. We com- pute a standard classi\ufb01cation loss with C and W, i.e., log(softmax(CW T)). 7For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%. 8See (10) in https://gluebenchmark.com/faq. MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average 392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k - 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1 System Pre-OpenAI SOTA BiLSTM+ELMo+Attn OpenAI GPT BERTBASE BERTLARGE Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different than the of\ufb01cial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single- model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components. We use a batch size of 32 and \ufb01ne-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best \ufb01ne-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that \ufb01ne- tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but per- form different \ufb01ne-tuning data shuf\ufb02ing and clas- si\ufb01er layer initialization.9 Results are presented in Table 1. Both BERTBASE and BERTLARGE outperform all sys- tems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy im- provement over the prior state of the art. Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the at- tention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the of\ufb01cial GLUE leaderboard10, BERTLARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing. We \ufb01nd that BERTLARGE signi\ufb01cantly outper- forms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2.",
    "raw_text": "4 Experiments 3.2 Fine-tuning BERT In this section, we present BERT \ufb01ne-tuning re- sults on 11 NLP tasks. Fine-tuning is straightforward since the self- attention mechanism in the Transformer al- lows BERT to model many downstream tasks\u2014 whether they involve single text or text pairs\u2014by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs be- fore applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidi- rectional cross attention between two sentences. For each task, we simply plug in the task- speci\ufb01c inputs and outputs into BERT and \ufb01ne- tune all the parameters end-to-end. At the in- put, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphras- ing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and 4.1 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a col- lection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1. To \ufb01ne-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the \ufb01nal hid- den vector C \u2208 RH corresponding to the \ufb01rst input token ([CLS]) as the aggregate representa- tion. The only new parameters introduced during \ufb01ne-tuning are classi\ufb01cation layer weights W \u2208 RK\u00d7H, where K is the number of labels. We com- pute a standard classi\ufb01cation loss with C and W, i.e., log(softmax(CW T)). 7For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%. 8See (10) in https://gluebenchmark.com/faq. MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average 392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k - 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1 System Pre-OpenAI SOTA BiLSTM+ELMo+Attn OpenAI GPT BERTBASE BERTLARGE Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different than the of\ufb01cial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single- model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components. We use a batch size of 32 and \ufb01ne-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best \ufb01ne-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that \ufb01ne- tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but per- form different \ufb01ne-tuning data shuf\ufb02ing and clas- si\ufb01er layer initialization.9 Results are presented in Table 1. Both BERTBASE and BERTLARGE outperform all sys- tems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy im- provement over the prior state of the art. Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the at- tention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the of\ufb01cial GLUE leaderboard10, BERTLARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing. We \ufb01nd that BERTLARGE signi\ufb01cantly outper- forms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2."
  },
  {
    "chunk_id": "36132f17-a990-4b46-ad3a-71968256b2dd",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 6,
    "retrieval_text": "4.2 SQuAD v1.1 Wikipedia containing the answer, the task is to predict the answer text span in the passage. As shown in Figure 1, in the question answer- ing task, we represent the input question and pas- sage as a single packed sequence, with the ques- tion using the A embedding and the passage using the B embedding. We only introduce a start vec- tor S \u20ac R\u201d and an end vector E \u20ac R\u201d during fine-tuning. The probability of word i being the start of the answer span is computed as a dot prod- uct between T; and S' followed by a softmax over all of the words in the paragraph: P; = 27 The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position 7 is defined as S-T; + E-T;, and the maximum scoring span where j > i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of Se-5 and a batch size of 32. Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by \ufb01rst \ufb01ne-tuning on TriviaQA (Joshi et al., 2017) befor \ufb01ne-tuning on SQuAD. The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd- sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from 9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE. 10https://gluebenchmark.com/leaderboard Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble sys- tem in terms of F1 score. Without TriviaQA \ufb01ne- 11QANet is described in Yu et al. (2018), but the system has improved substantially after publication. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human - - 82.3 91.2 #1 Ensemble - nlnet - - 86.0 91.7 #2 Ensemble - QANet - - 84.5 90.5 Published BiDAF+ELMo (Single) - 85.6 - 85.8 R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5 Ours BERTBASE (Single) 80.8 88.5 - - BERTLARGE (Single) 84.1 90.9 - - BERTLARGE (Ensemble) 85.8 91.8 - - BERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8 BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2 Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training check- points and \ufb01ne-tuning seeds. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human 86.3 89.0 86.9 89.5 #1 Single - MIR-MRC (F-Net) - - 74.8 78.0 #2 Single - nlnet - - 74.2 77.1 Published unet (Ensemble) - - 71.4 74.9 SLQA+ (Single) - 71.4 74.4 Ours BERTLARGE (Single) 78.7 81.9 80.0 83.1 Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components. tuning data, we only lose 0.1-0.4 F1, still outper- forming all existing systems by a wide margin.12",
    "raw_text": "4.2 SQuAD v1.1 Wikipedia containing the answer, the task is to predict the answer text span in the passage. As shown in Figure 1, in the question answer- ing task, we represent the input question and pas- sage as a single packed sequence, with the ques- tion using the A embedding and the passage using the B embedding. We only introduce a start vec- tor S \u20ac R\u201d and an end vector E \u20ac R\u201d during fine-tuning. The probability of word i being the start of the answer span is computed as a dot prod- uct between T; and S' followed by a softmax over all of the words in the paragraph: P; = 27 The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position 7 is defined as S-T; + E-T;, and the maximum scoring span where j > i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of Se-5 and a batch size of 32. Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by \ufb01rst \ufb01ne-tuning on TriviaQA (Joshi et al., 2017) befor \ufb01ne-tuning on SQuAD. The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd- sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from 9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE. 10https://gluebenchmark.com/leaderboard Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble sys- tem in terms of F1 score. Without TriviaQA \ufb01ne- 11QANet is described in Yu et al. (2018), but the system has improved substantially after publication. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human - - 82.3 91.2 #1 Ensemble - nlnet - - 86.0 91.7 #2 Ensemble - QANet - - 84.5 90.5 Published BiDAF+ELMo (Single) - 85.6 - 85.8 R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5 Ours BERTBASE (Single) 80.8 88.5 - - BERTLARGE (Single) 84.1 90.9 - - BERTLARGE (Ensemble) 85.8 91.8 - - BERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8 BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2 Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training check- points and \ufb01ne-tuning seeds. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human 86.3 89.0 86.9 89.5 #1 Single - MIR-MRC (F-Net) - - 74.8 78.0 #2 Single - nlnet - - 74.2 77.1 Published unet (Ensemble) - - 71.4 74.9 SLQA+ (Single) - 71.4 74.4 Ours BERTLARGE (Single) 78.7 81.9 80.0 83.1 Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components. tuning data, we only lose 0.1-0.4 F1, still outper- forming all existing systems by a wide margin.12"
  },
  {
    "chunk_id": "032d8e55-af76-48bc-9b11-0a6e9f2219aa",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 7,
    "retrieval_text": "4.3 SQuAD v2.0 The SQuAD 2.0 task extends the SQuAD 1.1 problem de\ufb01nition by allowing for the possibility that no short answer exists in the provided para- graph, making the problem more realistic. We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat ques- tions that do not have an answer as having an an- swer span with start and end at the [CLS] to- ken. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: snull = S\u00b7C + E\u00b7C to the score of the best non-null span 12The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the \ufb01rst 400 tokens in documents, that contain at least one of the provided possible answers. System Dev Test ESIM+GloVe 51.9 52.7 ESIM+ELMo 59.1 59.2 OpenAI GPT - 78.0 BERTBASE 81.6 - BERTLARGE 86.6 86.3 Human (expert)\u2020 - 85.0 Human (5 annotations)\u2020 - 88.0 Table 4: SWAG Dev and Test accuracies. \u2020Human per- formance is measured with 100 samples, as reported in the SWAG paper. \u02c6si,j = maxj\u2265iS\u00b7Ti + E\u00b7Tj. We predict a non-null answer when \u02c6si,j > snull + \u03c4, where the thresh- old \u03c4 is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We \ufb01ne-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48. The results compared to prior leaderboard en- tries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, exclud- ing systems that use BERT as one of their com- ponents. We observe a +5.1 F1 improvement over the previous best system. 4.4 SWAG The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair com- pletion examples that evaluate grounded common- sense inference (Zellers et al., 2018). Given a sen- tence, the task is to choose the most plausible con- tinuation among four choices. When \ufb01ne-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-speci\ufb01c parameters introduced is a vec- tor whose dot product with the [CLS] token rep- resentation C denotes a score for each choice which is normalized with a softmax layer. We \ufb01ne-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Re- sults are presented in Table 4. BERTLARGE out- performs the authors\u2019 baseline ESIM+ELMo sys- tem by +27.1% and OpenAI GPT by 8.3%.",
    "raw_text": "4.3 SQuAD v2.0 The SQuAD 2.0 task extends the SQuAD 1.1 problem de\ufb01nition by allowing for the possibility that no short answer exists in the provided para- graph, making the problem more realistic. We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat ques- tions that do not have an answer as having an an- swer span with start and end at the [CLS] to- ken. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: snull = S\u00b7C + E\u00b7C to the score of the best non-null span 12The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the \ufb01rst 400 tokens in documents, that contain at least one of the provided possible answers. System Dev Test ESIM+GloVe 51.9 52.7 ESIM+ELMo 59.1 59.2 OpenAI GPT - 78.0 BERTBASE 81.6 - BERTLARGE 86.6 86.3 Human (expert)\u2020 - 85.0 Human (5 annotations)\u2020 - 88.0 Table 4: SWAG Dev and Test accuracies. \u2020Human per- formance is measured with 100 samples, as reported in the SWAG paper. \u02c6si,j = maxj\u2265iS\u00b7Ti + E\u00b7Tj. We predict a non-null answer when \u02c6si,j > snull + \u03c4, where the thresh- old \u03c4 is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We \ufb01ne-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48. The results compared to prior leaderboard en- tries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, exclud- ing systems that use BERT as one of their com- ponents. We observe a +5.1 F1 improvement over the previous best system. 4.4 SWAG The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair com- pletion examples that evaluate grounded common- sense inference (Zellers et al., 2018). Given a sen- tence, the task is to choose the most plausible con- tinuation among four choices. When \ufb01ne-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-speci\ufb01c parameters introduced is a vec- tor whose dot product with the [CLS] token rep- resentation C denotes a score for each choice which is normalized with a softmax layer. We \ufb01ne-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Re- sults are presented in Table 4. BERTLARGE out- performs the authors\u2019 baseline ESIM+ELMo sys- tem by +27.1% and OpenAI GPT by 8.3%."
  },
  {
    "chunk_id": "db345458-f888-4101-ab42-2e3510760b86",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 7,
    "retrieval_text": "5 Ablation Studies In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional Dev Set Tasks MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1) BERTBASE 84.4 88.4 86.7 92.7 88.5 No NSP 83.9 84.9 86.5 92.6 87.9 LTR & No NSP 82.1 84.3 77.5 92.1 77.8 + BiLSTM 82.1 84.1 75.7 91.6 84.9 Table 5: Ablation over the pre-training tasks using the BERTBASE architecture. \u201cNo NSP\u201d is trained without the next sentence prediction task. \u201cLTR & No NSP\u201d is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a ran- domly initialized BiLSTM on top of the \u201cLTR + No NSP\u201d model during \ufb01ne-tuning. ablation studies can be found in Appendix C. 5.1 Effect of Pre-training Tasks We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data, \ufb01ne-tuning scheme, and hyperpa- rameters as BERTBASE: No NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task. LTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at \ufb01ne-tuning, because removing it introduced a pre-train/\ufb01ne-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input repre- sentation, and our \ufb01ne-tuning scheme. We \ufb01rst examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance signi\ufb01cantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by com- paring \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD. For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right- side context. In order to make a good faith at- tempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does signi\ufb01cantly improve results on SQuAD, but the results are still far worse than those of the pre- trained bidirectional models. The BiLSTM hurts performance on the GLUE tasks. We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two mod- els, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.",
    "raw_text": "5 Ablation Studies In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional Dev Set Tasks MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1) BERTBASE 84.4 88.4 86.7 92.7 88.5 No NSP 83.9 84.9 86.5 92.6 87.9 LTR & No NSP 82.1 84.3 77.5 92.1 77.8 + BiLSTM 82.1 84.1 75.7 91.6 84.9 Table 5: Ablation over the pre-training tasks using the BERTBASE architecture. \u201cNo NSP\u201d is trained without the next sentence prediction task. \u201cLTR & No NSP\u201d is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a ran- domly initialized BiLSTM on top of the \u201cLTR + No NSP\u201d model during \ufb01ne-tuning. ablation studies can be found in Appendix C. 5.1 Effect of Pre-training Tasks We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data, \ufb01ne-tuning scheme, and hyperpa- rameters as BERTBASE: No NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task. LTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at \ufb01ne-tuning, because removing it introduced a pre-train/\ufb01ne-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input repre- sentation, and our \ufb01ne-tuning scheme. We \ufb01rst examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance signi\ufb01cantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by com- paring \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD. For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right- side context. In order to make a good faith at- tempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does signi\ufb01cantly improve results on SQuAD, but the results are still far worse than those of the pre- trained bidirectional models. The BiLSTM hurts performance on the GLUE tasks. We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two mod- els, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer."
  },
  {
    "chunk_id": "48c71684-fb48-4238-b440-a6fd3f956862",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 8,
    "retrieval_text": "5.2 Effect of Model Size In this section, we explore the effect of model size on \ufb01ne-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training pro- cedure as described previously. Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of \ufb01ne-tuning. We can see that larger models lead to a strict ac- curacy improvement across all four datasets, even for MRPC which only has 3,600 labeled train- ing examples, and is substantially different from the pre-training tasks. It is also perhaps surpris- ing that we are able to achieve such signi\ufb01cant improvements on top of models which are al- ready quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERTBASE contains 110M parameters and BERTLARGE con- tains 340M parameters. It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the \ufb01rst work to demonstrate convinc- ingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been suf\ufb01- ciently pre-trained. Peters et al. (2018b) presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) men- tioned in passing that increasing hidden dimen- sion size from 200 to 600 helped, but increasing further to 1,000 did not bring further improve- ments. Both of these prior works used a feature- based approach \u2014 we hypothesize that when the model is \ufb01ne-tuned directly on the downstream tasks and uses only a very small number of ran- domly initialized additional parameters, the task- speci\ufb01c models can bene\ufb01t from the larger, more expressive pre-trained representations even when downstream task data is very small.",
    "raw_text": "5.2 Effect of Model Size In this section, we explore the effect of model size on \ufb01ne-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training pro- cedure as described previously. Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of \ufb01ne-tuning. We can see that larger models lead to a strict ac- curacy improvement across all four datasets, even for MRPC which only has 3,600 labeled train- ing examples, and is substantially different from the pre-training tasks. It is also perhaps surpris- ing that we are able to achieve such signi\ufb01cant improvements on top of models which are al- ready quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERTBASE contains 110M parameters and BERTLARGE con- tains 340M parameters. It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the \ufb01rst work to demonstrate convinc- ingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been suf\ufb01- ciently pre-trained. Peters et al. (2018b) presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) men- tioned in passing that increasing hidden dimen- sion size from 200 to 600 helped, but increasing further to 1,000 did not bring further improve- ments. Both of these prior works used a feature- based approach \u2014 we hypothesize that when the model is \ufb01ne-tuned directly on the downstream tasks and uses only a very small number of ran- domly initialized additional parameters, the task- speci\ufb01c models can bene\ufb01t from the larger, more expressive pre-trained representations even when downstream task data is very small."
  },
  {
    "chunk_id": "8daa35ed-d6ef-4be2-b996-e6b4da3a4f96",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 9,
    "retrieval_text": "5.3 Feature-based Approach with BERT All of the BERT results presented so far have used the \ufb01ne-tuning approach, where a simple classi\ufb01- cation layer is added to the pre-trained model, and all parameters are jointly \ufb01ne-tuned on a down- stream task. However, the feature-based approach, where \ufb01xed features are extracted from the pre- trained model, has certain advantages. First, not all tasks can be easily represented by a Trans- former encoder architecture, and therefore require a task-speci\ufb01c model architecture to be added. Second, there are major computational bene\ufb01ts to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we for- mulate this as a tagging task but do not use a CRF Hyperparams Dev Set Accuracy #L #H #A LM (ppl) MNLI-m MRPC SST-2 3 768 12 5.84 77.9 79.8 88.4 6 768 3 5.24 80.6 82.2 90.7 6 768 12 4.68 81.9 84.8 91.3 12 768 12 3.99 84.4 86.7 92.9 12 1024 16 3.54 85.7 86.9 93.3 24 1024 16 3.23 86.6 87.8 93.7 Table 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of at- tention heads. \u201cLM (ppl)\u201d is the masked LM perplexity of held-out training data. System ELMo (Peters et al., 2018a) 95.7 92.2 CVT (Clark et al., 2018) - 92.6 CSE (Akbik et al., 2018) - 93.1 Fine-tuning approach BERTLARGE 96.6 92.8 BERTBASE 96.4 92.4 Feature-based approach (BERTBASE) Embeddings 91.0 - Second-to-Last Hidden 95.6 - Last Hidden 94.9 - Weighted Sum Last Four Hidden 95.9 - Concat Last Four Hidden 96.1 - Weighted Sum All 12 Layers 95.5 - Dev F1 Test F1 Table 7: CoNLL-2003 Named Entity Recognition re- sults. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters. layer in the output. We use the representation of the \ufb01rst sub-token as the input to the token-level classi\ufb01er over the NER label set. To ablate the \ufb01ne-tuning approach, we apply the feature-based approach by extracting the activa- tions from one or more layers without \ufb01ne-tuning any parameters of BERT. These contextual em- beddings are used as input to a randomly initial- ized two-layer 768-dimensional BiLSTM before the classi\ufb01cation layer. Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art meth- ods. The best performing method concatenates the token representations from the top four hidden lay- ers of the pre-trained Transformer, which is only 0.3 F1 behind \ufb01ne-tuning the entire model. This demonstrates that BERT is effective for both \ufb01ne- tuning and feature-based approaches.",
    "raw_text": "5.3 Feature-based Approach with BERT All of the BERT results presented so far have used the \ufb01ne-tuning approach, where a simple classi\ufb01- cation layer is added to the pre-trained model, and all parameters are jointly \ufb01ne-tuned on a down- stream task. However, the feature-based approach, where \ufb01xed features are extracted from the pre- trained model, has certain advantages. First, not all tasks can be easily represented by a Trans- former encoder architecture, and therefore require a task-speci\ufb01c model architecture to be added. Second, there are major computational bene\ufb01ts to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we for- mulate this as a tagging task but do not use a CRF Hyperparams Dev Set Accuracy #L #H #A LM (ppl) MNLI-m MRPC SST-2 3 768 12 5.84 77.9 79.8 88.4 6 768 3 5.24 80.6 82.2 90.7 6 768 12 4.68 81.9 84.8 91.3 12 768 12 3.99 84.4 86.7 92.9 12 1024 16 3.54 85.7 86.9 93.3 24 1024 16 3.23 86.6 87.8 93.7 Table 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of at- tention heads. \u201cLM (ppl)\u201d is the masked LM perplexity of held-out training data. System ELMo (Peters et al., 2018a) 95.7 92.2 CVT (Clark et al., 2018) - 92.6 CSE (Akbik et al., 2018) - 93.1 Fine-tuning approach BERTLARGE 96.6 92.8 BERTBASE 96.4 92.4 Feature-based approach (BERTBASE) Embeddings 91.0 - Second-to-Last Hidden 95.6 - Last Hidden 94.9 - Weighted Sum Last Four Hidden 95.9 - Concat Last Four Hidden 96.1 - Weighted Sum All 12 Layers 95.5 - Dev F1 Test F1 Table 7: CoNLL-2003 Named Entity Recognition re- sults. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters. layer in the output. We use the representation of the \ufb01rst sub-token as the input to the token-level classi\ufb01er over the NER label set. To ablate the \ufb01ne-tuning approach, we apply the feature-based approach by extracting the activa- tions from one or more layers without \ufb01ne-tuning any parameters of BERT. These contextual em- beddings are used as input to a randomly initial- ized two-layer 768-dimensional BiLSTM before the classi\ufb01cation layer. Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art meth- ods. The best performing method concatenates the token representations from the top four hidden lay- ers of the pre-trained Transformer, which is only 0.3 F1 behind \ufb01ne-tuning the entire model. This demonstrates that BERT is effective for both \ufb01ne- tuning and feature-based approaches."
  },
  {
    "chunk_id": "9f0efe75-5d43-4667-98e0-18a30d12dbd6",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 9,
    "retrieval_text": "6 Conclusion Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to bene\ufb01t from deep unidirectional architec- tures. Our major contribution is further general- izing these \ufb01ndings to deep bidirectional architec- tures, allowing the same pre-trained model to suc- cessfully tackle a broad set of NLP tasks. References Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638\u20131649. Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level lan- guage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444. Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817\u20131853. Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The \ufb01fth PASCAL recognizing textual entailment challenge. In TAC. NIST. John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspon- dence learning. In Proceedings of the 2006 confer- ence on empirical methods in natural language pro- cessing, pages 120\u2013128. Association for Computa- tional Linguistics. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP. Association for Computational Linguis- tics. Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467\u2013479. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1\u201314, Vancou- ver, Canada. Association for Computational Lin- guistics. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robin- son. 2013. One billion word benchmark for measur- ing progress in statistical language modeling. arXiv preprint arXiv:1312.3005. Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora question pairs. Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In ACL. Kevin Clark, Minh-Thang Luong, Christopher D Man- ning, and Quoc Le. 2018. Semi-supervised se- quence modeling with cross-view training. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 1914\u2013 1925. Ronan Collobert and Jason Weston. 2008. A uni\ufb01ed architecture for natural language processing: Deep neural networks with multitask learning. In Pro- ceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 670\u2013680, Copen- hagen, Denmark. Association for Computational Linguistics. Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural informa- tion processing systems, pages 3079\u20133087. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09. William B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via \ufb01lling in the . arXiv preprint arXiv:1801.07736. Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units. CoRR, abs/1606.08415. Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics. Jeremy Howard and Sebastian Ruder. 2018. Universal language model \ufb01ne-tuning for text classi\ufb01cation. In ACL. Association for Computational Linguistics. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehen- sion. In IJCAI. Yacine Jernite, Samuel R. Bowman, and David Son- tag. 2017. Discourse-based objectives for fast un- supervised sentence representation learning. CoRR, abs/1705.00557. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In ACL. Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. Advances in neural information processing systems, pages 3294\u20133302. In Quoc Le and Tomas Mikolov. 2014. Distributed rep- resentations of sentences and documents. In Inter- national Conference on Machine Learning, pages 1188\u20131196. Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47. In Lajanugen Logeswaran and Honglak Lee. 2018. An ef\ufb01cient framework for learning sentence represen- tations. In International Conference on Learning Representations. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In NIPS. Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional LSTM. In CoNLL. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems 26, pages 3111\u20133119. Curran Associates, Inc.",
    "raw_text": "6 Conclusion Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to bene\ufb01t from deep unidirectional architec- tures. Our major contribution is further general- izing these \ufb01ndings to deep bidirectional architec- tures, allowing the same pre-trained model to suc- cessfully tackle a broad set of NLP tasks. References Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638\u20131649. Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level lan- guage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444. Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817\u20131853. Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The \ufb01fth PASCAL recognizing textual entailment challenge. In TAC. NIST. John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspon- dence learning. In Proceedings of the 2006 confer- ence on empirical methods in natural language pro- cessing, pages 120\u2013128. Association for Computa- tional Linguistics. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP. Association for Computational Linguis- tics. Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467\u2013479. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1\u201314, Vancou- ver, Canada. Association for Computational Lin- guistics. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robin- son. 2013. One billion word benchmark for measur- ing progress in statistical language modeling. arXiv preprint arXiv:1312.3005. Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora question pairs. Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In ACL. Kevin Clark, Minh-Thang Luong, Christopher D Man- ning, and Quoc Le. 2018. Semi-supervised se- quence modeling with cross-view training. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 1914\u2013 1925. Ronan Collobert and Jason Weston. 2008. A uni\ufb01ed architecture for natural language processing: Deep neural networks with multitask learning. In Pro- ceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 670\u2013680, Copen- hagen, Denmark. Association for Computational Linguistics. Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural informa- tion processing systems, pages 3079\u20133087. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09. William B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via \ufb01lling in the . arXiv preprint arXiv:1801.07736. Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units. CoRR, abs/1606.08415. Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics. Jeremy Howard and Sebastian Ruder. 2018. Universal language model \ufb01ne-tuning for text classi\ufb01cation. In ACL. Association for Computational Linguistics. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehen- sion. In IJCAI. Yacine Jernite, Samuel R. Bowman, and David Son- tag. 2017. Discourse-based objectives for fast un- supervised sentence representation learning. CoRR, abs/1705.00557. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In ACL. Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. Advances in neural information processing systems, pages 3294\u20133302. In Quoc Le and Tomas Mikolov. 2014. Distributed rep- resentations of sentences and documents. In Inter- national Conference on Machine Learning, pages 1188\u20131196. Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47. In Lajanugen Logeswaran and Honglak Lee. 2018. An ef\ufb01cient framework for learning sentence represen- tations. In International Conference on Learning Representations. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In NIPS. Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional LSTM. In CoNLL. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems 26, pages 3111\u20133119. Curran Associates, Inc."
  },
  {
    "chunk_id": "86ffd2e0-9c69-4888-97be-95455073432e",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 11,
    "retrieval_text": "Andriy Mnih and Geoffrey E Hinton. 2009. A scal- able hierarchical distributed language model. D. Koller, D. Schuurmans, Y. Bengio, and L. Bot- tou, editors, Advances in Neural Information Pro- cessing Systems 21, pages 1081\u20131088. Curran As- sociates, Inc. In Ankur P Parikh, Oscar T\u00a8ackstr\u00a8om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532\u2013 1543. Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In ACL. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word rep- resentations. In NAACL. Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 1499\u20131509. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 2383\u20132392. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention \ufb02ow for machine comprehension. In ICLR. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642. Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv:1810.06638. Wilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415\u2013433. Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL. Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Compu- tational Linguistics, ACL \u201910, pages 384\u2013394. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000\u20136010. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoen- coders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM. Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzing and Interpreting Neural Networks for NLP, pages 353\u2013355. Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi- granularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin- guistics. Alex Warstadt, Amanpreet Singh, and Samuel R Bow- man. 2018. Neural network acceptability judg- ments. arXiv preprint arXiv:1805.12471. Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google\u2019s neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320\u20133328. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehen- sion. In ICLR. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19\u201327.",
    "raw_text": "Andriy Mnih and Geoffrey E Hinton. 2009. A scal- able hierarchical distributed language model. D. Koller, D. Schuurmans, Y. Bengio, and L. Bot- tou, editors, Advances in Neural Information Pro- cessing Systems 21, pages 1081\u20131088. Curran As- sociates, Inc. In Ankur P Parikh, Oscar T\u00a8ackstr\u00a8om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532\u2013 1543. Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In ACL. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word rep- resentations. In NAACL. Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 1499\u20131509. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 2383\u20132392. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention \ufb02ow for machine comprehension. In ICLR. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642. Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv:1810.06638. Wilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415\u2013433. Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL. Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Compu- tational Linguistics, ACL \u201910, pages 384\u2013394. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000\u20136010. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoen- coders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM. Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzing and Interpreting Neural Networks for NLP, pages 353\u2013355. Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi- granularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin- guistics. Alex Warstadt, Amanpreet Singh, and Samuel R Bow- man. 2018. Neural network acceptability judg- ments. arXiv preprint arXiv:1805.12471. Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google\u2019s neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320\u20133328. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehen- sion. In ICLR. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19\u201327."
  },
  {
    "chunk_id": "a7c87e54-e6ea-45a4-86b0-f70c05f17af3",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 12,
    "retrieval_text": "Appendix for \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u201d We organize the appendix into three sections: \u2022 Additional implementation details for BERT are presented in Appendix A; \u2022 Additional details for our experiments are presented in Appendix B; and \u2022 Additional ablation studies are presented in Appendix C. We present additional ablation studies for BERT including: \u2013 Effect of Number of Training Steps; and \u2013 Ablation for Different Masking Proce- dures. A Additional Details for BERT A.1 Illustration of the Pre-training Tasks We provide examples of the pre-training tasks in the following. Masked LM and the Masking Procedure As- suming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further il- lustrated by \u2022 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 my dog is [MASK] my dog is [MASK] \u2022 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple dog is apple \u2022 10% of the time: Keep the word un- changed, e.g., my dog is hairy \u2192 my dog is hairy. The purpose of this is to bias the representation towards the actual observed word. The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model\u2019s language understanding capability. In Section C.2, we evaluate the impact this proce- dure. Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model BERT (Ours) OpenAl GPT Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are \ufb01ne-tuning approaches, while ELMo is a feature-based approach. to converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left- to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost. Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples. Input = [CLS] the man went to [MASK] store [SEP] epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warmup over the \ufb01rst 10,000 steps, and linear decay of the learning rate. We use a dropout prob- ability of 0.1 on all layers. We use a gelu acti- vation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood. he bought a gallon [MASK] milk [SEP] Label = IsNext Input = [CLS] the man [MASK] to the store [SEP] Training of BERTBASE was performed on 4 Cloud TPUs in Pod con\ufb01guration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pre- training took 4 days to complete. penguin [MASK] are flight ##less birds [SEP] Label = NotNext",
    "raw_text": "Appendix for \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u201d We organize the appendix into three sections: \u2022 Additional implementation details for BERT are presented in Appendix A; \u2022 Additional details for our experiments are presented in Appendix B; and \u2022 Additional ablation studies are presented in Appendix C. We present additional ablation studies for BERT including: \u2013 Effect of Number of Training Steps; and \u2013 Ablation for Different Masking Proce- dures. A Additional Details for BERT A.1 Illustration of the Pre-training Tasks We provide examples of the pre-training tasks in the following. Masked LM and the Masking Procedure As- suming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further il- lustrated by \u2022 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy \u2192 my dog is [MASK] my dog is [MASK] \u2022 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my dog is apple dog is apple \u2022 10% of the time: Keep the word un- changed, e.g., my dog is hairy \u2192 my dog is hairy. The purpose of this is to bias the representation towards the actual observed word. The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model\u2019s language understanding capability. In Section C.2, we evaluate the impact this proce- dure. Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model BERT (Ours) OpenAl GPT Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are \ufb01ne-tuning approaches, while ELMo is a feature-based approach. to converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left- to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost. Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples. Input = [CLS] the man went to [MASK] store [SEP] epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warmup over the \ufb01rst 10,000 steps, and linear decay of the learning rate. We use a dropout prob- ability of 0.1 on all layers. We use a gelu acti- vation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood. he bought a gallon [MASK] milk [SEP] Label = IsNext Input = [CLS] the man [MASK] to the store [SEP] Training of BERTBASE was performed on 4 Cloud TPUs in Pod con\ufb01guration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pre- training took 4 days to complete. penguin [MASK] are flight ##less birds [SEP] Label = NotNext"
  },
  {
    "chunk_id": "81718f62-9e11-4808-af50-dc87c95991da",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 13,
    "retrieval_text": "A.2 Pre-training Procedure To generate each training input sequence, we sam- ple two spans of text from the corpus, which we refer to as \u201csentences\u201d even though they are typ- ically much longer than single sentences (but can be shorter also). The \ufb01rst sentence receives the A embedding and the second receives the B embed- ding. 50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the \u201cnext sentence pre- diction\u201d task. They are sampled such that the com- bined length is \u2264 512 tokens. The LM masking is applied after WordPiece tokenization with a uni- form masking rate of 15%, and no special consid- eration given to partial word pieces. Longer sequences are disproportionately expen- sive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings. A.3 Fine-tuning Procedure For \ufb01ne-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of train- ing epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-speci\ufb01c, but we found the following range of possible values to work well across all tasks: \u2022 Batch size: 16, 32 We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 13https://cloudplatform.googleblog.com/2018/06/Cloud- TPU-now-offers-preemptible-pricing-and-global- availability.html \u2022 Learning rate (Adam): 5e-5, 3e-5, 2e-5 \u2022 Number of epochs: 2, 3, 4 We also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is rea- sonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set.",
    "raw_text": "A.2 Pre-training Procedure To generate each training input sequence, we sam- ple two spans of text from the corpus, which we refer to as \u201csentences\u201d even though they are typ- ically much longer than single sentences (but can be shorter also). The \ufb01rst sentence receives the A embedding and the second receives the B embed- ding. 50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the \u201cnext sentence pre- diction\u201d task. They are sampled such that the com- bined length is \u2264 512 tokens. The LM masking is applied after WordPiece tokenization with a uni- form masking rate of 15%, and no special consid- eration given to partial word pieces. Longer sequences are disproportionately expen- sive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings. A.3 Fine-tuning Procedure For \ufb01ne-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of train- ing epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-speci\ufb01c, but we found the following range of possible values to work well across all tasks: \u2022 Batch size: 16, 32 We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 13https://cloudplatform.googleblog.com/2018/06/Cloud- TPU-now-offers-preemptible-pricing-and-global- availability.html \u2022 Learning rate (Adam): 5e-5, 3e-5, 2e-5 \u2022 Number of epochs: 2, 3, 4 We also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is rea- sonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set."
  },
  {
    "chunk_id": "55136f35-51e0-456e-8eea-f0f4c957ddf9",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 14,
    "retrieval_text": "A.4 Comparison of BERT, ELMo ,and OpenAI GPT Here we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons be- tween the model architectures are shown visually in Figure 3. Note that in addition to the architec- ture differences, BERT and OpenAI GPT are \ufb01ne- tuning approaches, while ELMo is a feature-based approach. The most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text cor- pus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained: \u2022 GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words). \u2022 GPT uses a sentence separator ([SEP]) and classi\ufb01er token ([CLS]) which are only in- troduced at \ufb01ne-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training. \u2022 GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words. \u2022 GPT used the same learning rate of 5e-5 for all \ufb01ne-tuning experiments; BERT chooses a task-speci\ufb01c \ufb01ne-tuning learning rate which performs the best on the development set. To isolate the effect of these differences, we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable. A.5 Illustrations of Fine-tuning on Different Tasks The illustration of \ufb01ne-tuning BERT on different tasks can be seen in Figure 4. Our task-speci\ufb01c models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the \ufb01gure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classi\ufb01cation out- put, and [SEP] is the special symbol to separate non-consecutive token sequences.",
    "raw_text": "A.4 Comparison of BERT, ELMo ,and OpenAI GPT Here we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons be- tween the model architectures are shown visually in Figure 3. Note that in addition to the architec- ture differences, BERT and OpenAI GPT are \ufb01ne- tuning approaches, while ELMo is a feature-based approach. The most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text cor- pus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained: \u2022 GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words). \u2022 GPT uses a sentence separator ([SEP]) and classi\ufb01er token ([CLS]) which are only in- troduced at \ufb01ne-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training. \u2022 GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words. \u2022 GPT used the same learning rate of 5e-5 for all \ufb01ne-tuning experiments; BERT chooses a task-speci\ufb01c \ufb01ne-tuning learning rate which performs the best on the development set. To isolate the effect of these differences, we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable. A.5 Illustrations of Fine-tuning on Different Tasks The illustration of \ufb01ne-tuning BERT on different tasks can be seen in Figure 4. Our task-speci\ufb01c models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the \ufb01gure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classi\ufb01cation out- put, and [SEP] is the special symbol to separate non-consecutive token sequences."
  },
  {
    "chunk_id": "c326f9d8-36b0-440b-8345-4880612baa28",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 14,
    "retrieval_text": "B Detailed Experimental Setup B.1 Detailed Descriptions for the GLUE Benchmark Experiments. Our GLUE results in Table1 are obtained from https://gluebenchmark.com/ leaderboard and https://blog. openai.com/language-unsupervised. The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in Wang et al. (2018a): MNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classi\ufb01- cation task (Williams et al., 2018). Given a pair of sentences, the goal is to predict whether the sec- ond sentence is an entailment, contradiction, or neutral with respect to the \ufb01rst one. QQP Quora Question Pairs is a binary classi\ufb01- cation task where the goal is to determine if two questions asked on Quora are semantically equiv- alent (Chen et al., 2018). QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been converted to a binary classi\ufb01cation task (Wang et al., 2018a). The positive examples are (ques- tion, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer. Class Label (ea) - Sentence 1 Sentence 2 Single Sentence (a) Sentence Pair Classification Tasks: MNLI, QQP, QNLI, STS-B, MRPC, RTE, SWAG (b) Single Sentence Classification Tasks: SST-2, CoLA Start/End Span Question Paragraph Single Sentence (c) Question Answering Tasks: SQUAD v1.1 (d) Single Sentence Tagging Tasks: CoNLL-2003 NER Figure 4: Illustrations of Fine-tuning BERT on Different Tasks. SST-2 The Stanford Sentiment Treebank is a binary single-sentence classi\ufb01cation task consist- ing of sentences extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013). for whether the sentences in the pair are semanti- cally equivalent (Dolan and Brockett, 2005). RTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14 CoLA The Corpus of Linguistic Acceptability is a binary single-sentence classi\ufb01cation task, where the goal is to predict whether an English sentence is linguistically \u201cacceptable\u201d or not (Warstadt et al., 2018). STS-B The Semantic Textual Similarity Bench- mark is a collection of sentence pairs drawn from news headlines and other sources (Cer et al., 2017). They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning. MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations WNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that\u2019s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma- 14Note that we only report single-task \ufb01ne-tuning results in this paper. A multitask \ufb01ne-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI. 15https://gluebenchmark.com/faq jority class.",
    "raw_text": "B Detailed Experimental Setup B.1 Detailed Descriptions for the GLUE Benchmark Experiments. Our GLUE results in Table1 are obtained from https://gluebenchmark.com/ leaderboard and https://blog. openai.com/language-unsupervised. The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in Wang et al. (2018a): MNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classi\ufb01- cation task (Williams et al., 2018). Given a pair of sentences, the goal is to predict whether the sec- ond sentence is an entailment, contradiction, or neutral with respect to the \ufb01rst one. QQP Quora Question Pairs is a binary classi\ufb01- cation task where the goal is to determine if two questions asked on Quora are semantically equiv- alent (Chen et al., 2018). QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been converted to a binary classi\ufb01cation task (Wang et al., 2018a). The positive examples are (ques- tion, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer. Class Label (ea) - Sentence 1 Sentence 2 Single Sentence (a) Sentence Pair Classification Tasks: MNLI, QQP, QNLI, STS-B, MRPC, RTE, SWAG (b) Single Sentence Classification Tasks: SST-2, CoLA Start/End Span Question Paragraph Single Sentence (c) Question Answering Tasks: SQUAD v1.1 (d) Single Sentence Tagging Tasks: CoNLL-2003 NER Figure 4: Illustrations of Fine-tuning BERT on Different Tasks. SST-2 The Stanford Sentiment Treebank is a binary single-sentence classi\ufb01cation task consist- ing of sentences extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013). for whether the sentences in the pair are semanti- cally equivalent (Dolan and Brockett, 2005). RTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14 CoLA The Corpus of Linguistic Acceptability is a binary single-sentence classi\ufb01cation task, where the goal is to predict whether an English sentence is linguistically \u201cacceptable\u201d or not (Warstadt et al., 2018). STS-B The Semantic Textual Similarity Bench- mark is a collection of sentence pairs drawn from news headlines and other sources (Cer et al., 2017). They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning. MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations WNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that\u2019s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma- 14Note that we only report single-task \ufb01ne-tuning results in this paper. A multitask \ufb01ne-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI. 15https://gluebenchmark.com/faq jority class."
  },
  {
    "chunk_id": "c8f46ed1-7b83-4f09-8475-677cf64de746",
    "modality": "text",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": 16,
    "retrieval_text": "C Additional Ablation Studies C.1 Effect of Number of Training Steps Figure 5 presents MNLI Dev accuracy after \ufb01ne- tuning from a checkpoint that has been pre-trained for k steps. This allows us to answer the following questions: 1. Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high \ufb01ne-tuning accuracy? Answer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps. 2. Question: Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word? Answer: The MLM model does converge slightly slower than the LTR model. How- ever, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately. C.2 Ablation for Different Masking Procedures In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies. 84 y c a r 82 u c c A v e 80 D I L N M 78 BERTBASE (Masked LM) 76 BERTBASE (Left-to-Right) 200 400 600 800 1,000 Pre-training Steps (Thousands) Figure 5: Ablation over number of training steps. This shows the MNLI accuracy after \ufb01ne-tuning, starting from model parameters that have been pre-trained for k steps. The x-axis is the value of k. Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and \ufb01ne-tuning, as the [MASK] symbol never ap- pears during the \ufb01ne-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both \ufb01ne-tuning and feature-based ap- proaches, as we expect the mismatch will be am- pli\ufb01ed for the feature-based approach as the model will not have the chance to adjust the representa- tions. Masking Rates Dev Set Results MASK SAME RND MNLI NER Fine-tune Fine-tune Feature-based 80% 10% 10% 84.2 95.4 94.9 100% 0% 0% 84.3 94.9 94.0 80% 0% 20% 84.1 95.2 94.6 80% 20% 0% 84.4 95.2 94.7 0% 20% 80% 83.7 94.8 94.6 0% 0% 100% 83.6 94.9 94.6 Table 8: Ablation over different masking strategies. The results are presented in Table 8. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token. The numbers in the left part of the table repre- sent the probabilities of the speci\ufb01c strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3. From the table it can be seen that \ufb01ne-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strat- egy was problematic when applying the feature- based approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well.",
    "raw_text": "C Additional Ablation Studies C.1 Effect of Number of Training Steps Figure 5 presents MNLI Dev accuracy after \ufb01ne- tuning from a checkpoint that has been pre-trained for k steps. This allows us to answer the following questions: 1. Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high \ufb01ne-tuning accuracy? Answer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps. 2. Question: Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word? Answer: The MLM model does converge slightly slower than the LTR model. How- ever, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately. C.2 Ablation for Different Masking Procedures In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies. 84 y c a r 82 u c c A v e 80 D I L N M 78 BERTBASE (Masked LM) 76 BERTBASE (Left-to-Right) 200 400 600 800 1,000 Pre-training Steps (Thousands) Figure 5: Ablation over number of training steps. This shows the MNLI accuracy after \ufb01ne-tuning, starting from model parameters that have been pre-trained for k steps. The x-axis is the value of k. Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and \ufb01ne-tuning, as the [MASK] symbol never ap- pears during the \ufb01ne-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both \ufb01ne-tuning and feature-based ap- proaches, as we expect the mismatch will be am- pli\ufb01ed for the feature-based approach as the model will not have the chance to adjust the representa- tions. Masking Rates Dev Set Results MASK SAME RND MNLI NER Fine-tune Fine-tune Feature-based 80% 10% 10% 84.2 95.4 94.9 100% 0% 0% 84.3 94.9 94.0 80% 0% 20% 84.1 95.2 94.6 80% 20% 0% 84.4 95.2 94.7 0% 20% 80% 83.7 94.8 94.6 0% 0% 100% 83.6 94.9 94.6 Table 8: Ablation over different masking strategies. The results are presented in Table 8. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token. The numbers in the left part of the table repre- sent the probabilities of the speci\ufb01c strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3. From the table it can be seen that \ufb01ne-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strat- egy was problematic when applying the feature- based approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well."
  },
  {
    "chunk_id": "bf67f289-7fa8-4f99-a708-157279120202",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the letter 'C' key. The keyboard is a standard QWERTY layout, and the 'C' key is highlighted in a way that makes it stand out from the rest of the keys. There are no visible texts or labels on the keys. The image has a pixelated quality, suggesting it may be a low-resolution photograph or a digital rendering with a simplistic color palette. ",
    "raw_text": null
  },
  {
    "chunk_id": "f21735d6-d775-4587-9f5b-6687fb589953",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the letter 'T' key. The keyboard appears to be a standard QWERTY layout, commonly used for English language input. The keys are white with black lettering, and there is a visible space bar to the right of the 'T' key. The image has a watermark or overlay text that reads \"T\" in a bold, sans-serif font, which seems to be part of the keyboard design or an artistic choice rather than actual text on the keyboard. The background behind the keyboard is not clearly visible due to the close-up nature of the photograph. ",
    "raw_text": null
  },
  {
    "chunk_id": "63104a01-3681-4bdd-b7ee-3ac04d31ae69",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image displays a close-up of a computer keyboard with a focus on the letter 'T' key. The keyboard is a standard QWERTY layout, and the keys are labeled with white lettering against a black background. The 'T' key is highlighted in a different color, possibly indicating it as the current selection or focus. To the right of the keyboard, there is a small graphic element resembling a magnifying glass, which suggests that this image may be part of a search function or a zoom-in feature. The background behind the keyboard is not clearly visible due to the close-up nature of the photograph. There are no texts or additional graphics present in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "443ddc07-1147-4de8-88b6-27e0fea32380",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image displays a close-up of a computer keyboard with a focus on the \"T\" key. The key is labeled with the letter \"T\" and has a small icon that appears to be a magnifying glass. Below the \"T\" key, there is a text box with the word \"SEPTEMBER\" in it, suggesting that this key might be used for selecting or setting the month of September. The keyboard itself is a standard QWERTY layout, and the image has a pixelated quality to it, indicating it may have been taken from a video or a digital screen. ",
    "raw_text": null
  },
  {
    "chunk_id": "e4d44133-0a1d-4948-bcd1-43d325b1e99a",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image displays a close-up of a computer keyboard with a focus on the keys labeled \"T\" and \"M\". The \"T\" key is located to the left of the \"M\" key, both of which are part of the top row of the keyboard. The \"T\" key has an underscore symbol (`_`) beneath it, indicating that it is a special key used for specific functions in certain software applications. The \"M\" key also has a special character, which appears to be a superscripted letter \"m\".\n\nThe image is taken from a slightly elevated angle, with the keyboard keys appearing larger and more prominent than the rest of the keyboard. The background is not clearly visible due to the close-up nature of the shot. There are no other distinguishable features or objects in the image that provide additional context or information about the setting or environment where the photo was taken. ",
    "raw_text": null
  },
  {
    "chunk_id": "8049d744-577b-46d7-a93b-1cb47d42f102",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image displays a close-up of a computer keyboard with a focus on the \"T\" key. The keyboard is a standard QWERTY layout, and the keys are arranged in rows and columns typical for English language keyboards. The \"T\" key is prominently featured in the center of the image, with its label clearly visible. The background is out of focus, emphasizing the keyboard as the main subject. There are no texts or additional graphics present in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "c41081da-1846-4a95-83de-3ee0967544da",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image is a photograph of a sign with text on it. The sign appears to be placed outdoors, as suggested by the natural light and shadows visible on its surface. The text on the sign reads \"BERT,\" which seems to be the name or title associated with the content displayed. Below the text, there are several rows of what appear to be bubbles or spheres, each containing a smaller sphere within it. These structures resemble a periodic table or a diagram of some sort, but without additional context, their specific purpose or meaning is unclear. The background behind the sign is not clearly visible due to the focus on the sign itself. ",
    "raw_text": null
  },
  {
    "chunk_id": "605d51be-6137-4c45-b15d-2bbcb43d281c",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the letter 'C' key. The keyboard is a standard QWERTY layout, and the 'C' key is prominently displayed in the center of the image. The keys are white with black lettering, and there is a slight reflection on the surface of the keys, suggesting they may be made of plastic or a similar material. In the upper right corner of the image, there appears to be a watermark or logo that resembles a stylized 'C' in a darker shade of gray, possibly indicating copyright or ownership by a company or individual named \"C\". The background is not clearly visible due to the close-up nature of the shot. ",
    "raw_text": null
  },
  {
    "chunk_id": "ad970338-1aea-4dc4-8410-53190a43a203",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image displays a close-up of a computer keyboard with a focus on the 'T' key. The keyboard is a standard QWERTY layout, with keys labeled from 'A' to 'Z', including numbers and function keys. The 'T' key is highlighted in a way that suggests it is being used or selected, possibly indicating that it is part of a command or action. There are no visible texts or additional graphics on the keyboard itself. The background behind the keyboard is not clearly distinguishable due to the close-up nature of the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "ffb830e8-62de-4c94-ad11-88d48b2d84a5",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the \"T\" key. The \"T\" key is highlighted by a white border, indicating that it is the selected key. To the right of the \"T\" key, there is a small graphic representation of a thermometer with a temperature reading of \"N/A,\" suggesting that the temperature is not applicable or not available for selection. The background of the image is blurred, but it appears to be a light blue color, which could suggest a computer screen or a keyboard surface. There are no visible texts or additional graphics in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "9fd5a3da-dda8-4371-848a-97e7298bd3e0",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the \"T\" key. The key is labeled with the text \"T SEP,\" which suggests that the key is for the \"Tab\" function in a specific language or software interface. The keyboard appears to be a standard QWERTY layout, and the keys are arranged in rows and columns typical of such a design. The image has a pixelated quality, indicating it may have been taken from a video or a digital display. ",
    "raw_text": null
  },
  {
    "chunk_id": "33289c97-ae48-4787-ad76-7e82fa4550b8",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the \"M\" key. The \"M\" key is highlighted by a white border, indicating that it is the selected key. To the right of the \"M\" key, there is a small graphic of a key with an \"M\" on it, and to its left, there's a symbol that appears to be a superscripted \"TM\". The keyboard itself has a standard layout with keys labeled from \"A\" to \"Z\", numbers from \"0\" to \"9\", function keys, and various special keys such as the space bar, shift keys, and arrow keys. The background of the image is not clearly visible due to the close-up nature of the photograph. ",
    "raw_text": null
  },
  {
    "chunk_id": "685ae000-d919-4af3-a27e-9adab4602b90",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital screenshot of a computer interface with a focus on a keyboard key labeled \"T\". The key is highlighted in a blue color, indicating that it is selected or active. The keyboard appears to be a standard QWERTY layout, but only the \"T\" key is visible and emphasized. In the upper right corner of the image, there's a magnifying glass icon, suggesting a search function or feature within the software application being used. The background of the interface is a gradient of blue shades, providing a contrast to the white keyboard keys. There are no other elements or text visible in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "7a4de24b-6be8-4298-8dde-b49361fc628b",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image is a photograph of an indoor setting that appears to be a room with a tiled floor and walls. On the wall, there is a sign with the word \"BERT\" written in capital letters. Below the sign, there is a horizontal yellow line. To the right of this line, there is a vertical yellow line that extends upwards from the bottom edge of the image. The room has a grayish-blue tiled floor and walls, and the lighting suggests an artificial source, possibly fluorescent, casting a soft glow on the surfaces. There are no other objects or texts visible in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "39193e7f-5031-4540-b18d-e6c96c566112",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of three separate panels, each depicting a different stage of a process or system. The style of the image suggests it may be a screenshot from a presentation or educational material, possibly related to computer science or network engineering.\n\n1. Top Panel: This panel shows a flowchart with a series of steps and decision points. It includes various shapes such as rectangles, diamonds, and circles, which are commonly used in flowcharts to represent processes, decisions, and actions. The text within the flowchart is not fully legible due to the resolution of the image.\n\n2. Middle Panel: This panel appears to be a network diagram with nodes connected by lines, representing a network topology. Each node has labels such as \"DJ,\" \"GW,\" \"R1,\" and \"R2,\" which could refer to routers or other network devices. The lines connecting the nodes indicate the flow of data or communication paths between them.\n\n3. Bottom Panel: This panel is a zoomed-in view of the middle panel, focusing on a specific section of the network diagram. It shows more detail about the connections and possibly additional network elements such as switches or servers. The text \"DJ,\" \"GW,\" \"R1,\" and \"R2\" are visible again in this panel, reinforcing that these labels represent different network devices.\n\nThe image does not provide enough information to determine the exact nature of the process or system being depicted, nor can I provide specific details about the text within the flowchart due to the resolution. ",
    "raw_text": null
  },
  {
    "chunk_id": "c3fb1e83-f376-4929-bcb9-82245550c5b1",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the letter 'C' key. The keyboard is a standard QWERTY layout, and the 'C' key is highlighted in a light blue color, indicating it as the current selection or focus. The background behind the keyboard is blurred but appears to be a neutral gray color, which suggests that the image might have been taken from above the keyboard. There are no visible texts or additional graphics on the keyboard itself. The style of the image is a straightforward photograph with a shallow depth of field, emphasizing the 'C' key while the rest of the keyboard and background are out of focus. ",
    "raw_text": null
  },
  {
    "chunk_id": "8a219b54-f81c-4c38-9e41-ba8e78d98995",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a computer keyboard with a key labeled \"T\" highlighted. The keyboard is depicted against a light blue background. The key \"T\" is located on the top row of the keyboard, which typically corresponds to the letter 'T' in English QWERTY layouts. The image has a watermark or logo at the bottom right corner that appears to be a stylized representation of a computer mouse with a hand holding it. The style of the image is simple and utilitarian, likely intended for instructional purposes to show the location of the 'T' key on a keyboard. ",
    "raw_text": null
  },
  {
    "chunk_id": "18673594-c3a9-4dad-92f1-7ad6c6b9ce10",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the letter 'T' key. The keyboard is white with black keys, and the 'T' key has a green label with the letter 'T' in white font. To the right of the 'T' key, there is a small graphic of a house with a chimney, which appears to be a logo or emblem. Below this graphic, there are two lines of text: \"N\" on the first line and \"T\" on the second line. The background behind the keyboard is not clearly visible due to the close-up nature of the image. The style of the image is straightforward and utilitarian, likely intended for informational or instructional purposes related to computer keyboards. ",
    "raw_text": null
  },
  {
    "chunk_id": "642e57f2-b54b-42da-b25b-ad018919a48c",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the \"T\" key. The key is labeled with the text \"T\" at the top and \"sep\" below it, indicating that the key is for the \"Tab\" function. The keyboard appears to be a standard QWERTY layout, with the \"T\" key located in the top left corner of the keyboard. The keys are arranged in rows and columns, typical of a computer keyboard, with the \"T\" key being part of the first row. The image is taken from an angle that shows the side profile of the keyboard, emphasizing the \"T\" key as the main subject. ",
    "raw_text": null
  },
  {
    "chunk_id": "d6b4a4d8-b62f-4bb6-9399-84e886928c0c",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a computer keyboard with a key labeled \"T\" highlighted. The keyboard is depicted in a simplified manner, with only the keys visible and no additional context provided. The key labeled \"T\" has an asterisk symbol next to it, which might indicate that it's a special or function key. There are no other keys labeled with letters or symbols, and the background of the image is plain white. The style of the image is reminiscent of a computer interface, possibly from a video game or a software application. ",
    "raw_text": null
  },
  {
    "chunk_id": "213412d2-1393-46f7-9c2a-1fac0921c5a7",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the number pad area. The keys are labeled with numbers from 1 to 9, and there is a key labeled \"T\" in the top right corner, which is typically used for the \"Tab\" function. The keys have a standard layout with the numbers arranged in a grid pattern, and the \"T\" key is positioned at the top right of the number pad. The keyboard appears to be a standard QWERTY layout, but only the number pad section is visible in this image. The background is not clearly visible due to the close-up nature of the photo. ",
    "raw_text": null
  },
  {
    "chunk_id": "019d6add-000a-4071-94a1-d8aaf9eb2394",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image appears to be a screenshot of a computer screen displaying a graphical user interface (GUI) with a blue background. At the top of the screen, there is a title bar that reads \"BERT.\" Below the title, there is a horizontal yellow line that spans across the width of the window. The main content of the GUI consists of a grid-like structure with numerous small squares, each containing a white dot. These dots are arranged in a pattern that suggests a two-dimensional layout, possibly representing a map or a board game.\n\nThe bottom part of the screen shows a horizontal yellow line with a series of white dots aligned along it, similar to the ones in the grid above. The style of the image is digital and resembles a graphical representation that could be used for planning or organizing tasks. There are no texts other than the title \"BERT\" at the top. ",
    "raw_text": null
  },
  {
    "chunk_id": "e1dd57b3-7a5d-49b4-9e31-2be018bd847a",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the number pad area. The keys are labeled with numbers from 1 to 9, and there is a key labeled \"T\" in the top right corner, which typically stands for the Tab key. The keys have a standard layout with the numbers arranged in a grid pattern, and the \"T\" key is positioned at the top right of the number pad. The keyboard appears to be a standard QWERTY layout, commonly used on desktop computers. The image has a plain background and is taken from an angle that shows the side view of the keyboard. There are no visible texts or additional graphics in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "b441b51f-6af7-4748-8cdf-a36c13040cef",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the letter 'C' key. The keyboard is a standard QWERTY layout, which is commonly used for English language keyboards. The 'C' key is located in the center of the keyboard, between the 'A' and 'D' keys. The image is taken from an angle that slightly obscures the 'C' key, making it appear as if it is being pressed down or has been pressed down. There are no visible texts or additional graphics on the keyboard itself. The background is not clearly distinguishable due to the close-up nature of the photo. ",
    "raw_text": null
  },
  {
    "chunk_id": "54e6daa7-a06b-4752-b410-3e65be439f92",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the number pad area. The keys are labeled with numbers from 1 to 9, and there is a key with the letter \"T\" on it. The keyboard appears to be a standard QWERTY layout, which is commonly used for English language keyboards. The image has a watermark or logo in the bottom right corner that resembles a shield with a checkmark inside it, but the details of this logo are not clear due to the resolution and angle of the photo. The style of the image is a straightforward photograph without any additional graphics or text overlaying the keyboard. ",
    "raw_text": null
  },
  {
    "chunk_id": "4d1f8207-df65-47a3-8dcb-e8401f3a9b26",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the \"T\" key. The \"T\" key is highlighted by being slightly elevated above the other keys, indicating that it has been pressed down. The keyboard appears to be a standard QWERTY layout, with the \"T\" key located in the top left corner. The image is taken from an angle where the keyboard is partially obscured by the camera's perspective, and the background is not clearly visible due to the close-up nature of the shot. There are no texts or additional graphics present in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "acaf0fb5-a19b-4a3b-aa99-0672de9e7048",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image appears to be a screenshot of a computer screen displaying a graphical user interface (GUI) with a text overlay. The GUI has a dark background with various icons and elements that suggest it is a software application, possibly related to data analysis or visualization given the presence of graphs and charts.\n\nAt the top of the image, there is a title bar with the text \"Bert\" in white font on a black background. Below this, there is a horizontal menu bar with several icons that are not clearly distinguishable due to the resolution of the image.\n\nThe main content of the GUI includes a large graph or chart occupying most of the space. The chart has multiple layers and appears to be displaying some form of data visualization, possibly related to finance or economics given the context provided by the text overlay. The specific details of the chart are not clear due to the image's resolution.\n\nOverlaying the graph is a semi-transparent text box with the word \"Bert\" in white font on a black background. This text box is positioned centrally and spans across most of the width of the image, partially obscuring the underlying chart. The text box seems to be superimposed over the GUI, suggesting it might be part of an instruction or a label within the software application.\n\nThe overall style of the image is utilitarian and functional, typical of a computer interface designed for data analysis or visualization tasks. ",
    "raw_text": null
  },
  {
    "chunk_id": "e932fb0f-a900-4744-8783-dadc62275765",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the letter 'C' key. The keyboard is a standard QWERTY layout, and the 'C' key is in the center of the image. To the left of the 'C', there is a key labeled 'A', and to its right, there is a key labeled 'D'. Above the 'C' key, there is a key labeled 'Shift', which is typically used to modify the function of other keys. The background of the keyboard is white, and the keys are black with white lettering. The image has a watermark or overlay text that reads \"C\", indicating the focus on the 'C' key. The style of the image is a straightforward product photograph with a clear focus on the keyboard and its layout. ",
    "raw_text": null
  },
  {
    "chunk_id": "937fe7a5-2101-4dc3-bff0-cfbd02476fc1",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a computer keyboard with a key labeled \"T\" highlighted. The keyboard is depicted against a light blue background. The keys are arranged in the standard QWERTY layout, which is typical for English-speaking countries. The \"T\" key is located in the top row, second from the right. There are no visible texts or additional graphics on the keyboard itself. The style of the image is simple and straightforward, focusing solely on the keyboard without any additional context or embellishments. ",
    "raw_text": null
  },
  {
    "chunk_id": "bc10929b-0bbc-41ba-b3fa-370c01abb6ad",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the letter 'T' key. The keyboard is a standard QWERTY layout, and the keys are white with black lettering. The 'T' key is highlighted in a different color, possibly indicating it as the current selection or focus. To the right of the 'T' key, there is a small graphic of a globe with the letters \"N\" and \"S\" on it, suggesting that this keyboard layout might be for a language where 'T' is pronounced differently than in English. The background behind the keyboard is not clearly visible due to the close-up nature of the image. There are no texts or additional graphics present in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "da1ac1ea-7830-4914-975e-b4161ba7307d",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the \"T\" key. The key is labeled with the letter \"T\" and has a small icon that appears to be a magnifying glass. Below the key, there's a label that reads \"SEP,\" which might indicate a special function or shortcut associated with this key. The keyboard itself is a standard QWERTY layout, commonly used in English-speaking countries. The image is taken from an angle where the keys are slightly tilted to the left, and the focus is on the \"T\" key, making it appear larger than the other keys. The background of the image is not clearly visible due to the close-up nature of the shot. ",
    "raw_text": null
  },
  {
    "chunk_id": "d0c85147-7b6a-4230-a3fa-a1298e56411e",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a key labeled \"T\". The keyboard appears to be a standard QWERTY layout, but the focus is on the \"T\" key, which has a small graphic of a molecule next to it. The molecule consists of three atoms: one large atom in the center and two smaller atoms attached to it at the top corners. Above the keyboard, there's a text that reads \"Molecular Modeling,\" suggesting that this image might be related to a discussion or demonstration about molecular modeling. The style of the image is informative, likely used for educational purposes to illustrate the concept of molecular modeling in a computer-based context. ",
    "raw_text": null
  },
  {
    "chunk_id": "1d786f29-d164-416c-9c92-9e4b92d62560",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the number pad area. The keys are labeled with numbers from 1 to 9, and there is a key with the letter \"T\" in the top right corner, which is typically used for the \"Tab\" function. The keys have a standard layout with the numbers arranged in a grid pattern, and the \"T\" key is positioned at the top right of the number pad. The keyboard appears to be a standard QWERTY layout, but only the number pad section is visible in this image. The background is not clearly visible due to the close-up nature of the photograph. ",
    "raw_text": null
  },
  {
    "chunk_id": "0bd127bf-5ca6-4653-b271-97649f418ce2",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image appears to be a screenshot of a computer interface displaying a graphical representation of a network or system with various nodes connected by lines. The nodes are labeled with text, but the specific content of the labels is not clear due to the resolution of the image. One node prominently displays the word \"BERT,\" which could refer to Bidirectional Encoder Representations from Transformers, a type of language model used in natural language processing.\n\nThe layout suggests a hierarchical or layered structure, with some nodes connected directly to each other and others branching out into more complex connections. The lines connecting the nodes represent relationships or interactions between different parts of the system. The overall style of the image is reminiscent of a flowchart or a diagram used in technical documentation or presentations.\n\nThe background is a gradient of gray tones, providing a neutral backdrop that allows the colored nodes and lines to stand out. There are no visible texts other than the labels on the nodes, which are too small to read clearly. The image does not contain any additional text or annotations that provide context or explanations about the system depicted. ",
    "raw_text": null
  },
  {
    "chunk_id": "6f37b3ae-c0d0-4979-9bba-b7cf7b718db8",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the number pad area. The keys are labeled with numbers from 1 to 9, arranged in a grid pattern. There is also a key labeled \"TN\" which stands for \"To New,\" indicating that this is likely a custom or special function key. The keyboard appears to be a standard QWERTY layout, and the image has a pixelated quality, suggesting it may have been taken from a video or a digital display. ",
    "raw_text": null
  },
  {
    "chunk_id": "047c6cf4-2668-49b7-b898-2b04d4c46372",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a close-up of a computer keyboard with a focus on the letter 'C' key. The keyboard appears to be a standard QWERTY layout, with the 'C' key located in the lower left corner of the keyboard. The key is white with a black letter 'C' printed on it. In the upper right corner of the image, there is a small, simple graphic of a computer mouse cursor pointing at the 'C' key. The background of the image is not clearly visible due to the close-up nature of the photograph. There are no texts or additional graphics present in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "1cfc5676-31de-4d35-ad0e-e0e499cb606c",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image shows a computer keyboard with a key labeled \"T2\" highlighted in green. The keyboard is set against a gray background that appears to be a desk surface. The \"T2\" key is located on the top row of the keyboard, between the \"Shift\" and \"Ctrl\" keys. The keyboard has standard QWERTY layout with white lettering on black keys. There are no visible texts or additional graphics in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "e317b1a3-c1dd-409a-a81a-31d8fe6fec7c",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image displays a close-up of a computer keyboard with a focus on the \"T\" key. The \"T\" key is prominently visible and appears to be the only key in focus. The keyboard has a standard layout with keys labeled from \"A\" to \"Z,\" including function keys, arrow keys, and the numeric keypad. The background of the image is blurred, emphasizing the keyboard as the main subject. There are no texts or additional graphics present in the image. The style of the image is a straightforward product photograph with a shallow depth of field, focusing on the \"T\" key while the rest of the keyboard and the background are out of focus. ",
    "raw_text": null
  },
  {
    "chunk_id": "99d3d040-2bbc-4c35-bfea-868f6a3b971f",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image appears to be a screenshot of a computer screen displaying a graphical user interface (GUI) with a text overlay. The GUI has a dark background with various icons and elements that suggest it is a software application, possibly related to data analysis or visualization given the presence of graphs and charts.\n\nAt the top of the image, there is a title bar with the text \"Bert\" in white font on a black background. Below this, there is a horizontal menu bar with several icons that are not clearly labeled due to the resolution of the image. The main area of the GUI shows a graphical representation of data, which includes a scatter plot with multiple clusters of dots and lines connecting them.\n\nThe text \"Bert\" in the title bar is likely a reference to the name of the software or application being used. The content of the graph and the specific details of the icons are not clear due to the image's resolution and size. The style of the image is digital, with a focus on functionality over aesthetics. ",
    "raw_text": null
  },
  {
    "chunk_id": "4fef6784-4949-4936-839f-50f7913c8e9b",
    "modality": "image",
    "source_pdf": "1810.04805v2.pdf",
    "page_number": null,
    "retrieval_text": " The image is a graphical representation of data, specifically a line graph with two lines. The left vertical axis is labeled \"BERT,\" and the right vertical axis is labeled \"NLM.\" Both axes have numerical values ranging from 0 to 100, indicating some form of measurement or scale for the data represented by the lines.\n\nThe horizontal axis represents a range of numbers from 0 to 1000, which likely corresponds to steps in a process or stages of development. The graph shows two lines: one is labeled \"BERT,\" and the other is labeled \"NLM.\" Both lines start at the bottom left corner of the graph and end at the top right corner, indicating that they represent data points for both BERT and NLM across the range of numbers on the horizontal axis.\n\nThe line representing BERT starts at 0 and increases steadily to a peak around 850, then decreases slightly before ending at 1000. The line representing NLM starts at 0, rises sharply to a peak around 900, and then drops off sharply to end at 1000.\n\nThere are two annotations on the graph: \"BERT\" is marked with a blue arrow pointing to the left vertical axis, and \"NLM\" is marked with a red arrow pointing to the right vertical axis. The text \"BERT\" and \"NLM\" are written in black font above their respective lines.\n\nThe background of the image is white, and there is a watermark or logo at the bottom right corner that reads \"Pretraining steps: thousands.\" This suggests that the graph may be related to machine learning or artificial intelligence, where pretraining refers to an initial phase of training a model with large amounts of data. ",
    "raw_text": null
  },
  {
    "chunk_id": "b4cf880f-202e-4096-9415-d78b4597fc1a",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 1,
    "retrieval_text": "3 2023 2 0 2 c e D 8 1 ] L C . s c [ 1 v 7 9 9 0 1 . 2 1 3 2 : v i X r a Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gao 1 , Yun Xiong 2 , Xinyu Gao 2 , Kangxiang Jia 2 , Jinliu Pan 2 , Yuxi Bi 3 , Yi Dai1 , Jiawei Sun1 and Haofen Wang 1,3 \u2217 1 Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University 2 Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University 3 College of Design and Innovation,Tongji University gaoyunfan1602@gmail.com Abstract Large language models (LLMs) demonstrate pow- erful capabilities, but they still face challenges in practical applications, such as hallucinations, slow knowledge updates, and lack of transparency in answers. Retrieval-Augmented Generation (RAG) refers to the retrieval of relevant information from external knowledge bases before answering ques- tions with LLMs. RAG has been demonstrated to significantly enhance answer accuracy, reduce model hallucination, particularly for knowledge- intensive tasks. By citing sources, users can verify the accuracy of answers and increase trust in model outputs. It also facilitates knowledge updates and the introduction of domain-specific knowl- edge. RAG effectively combines the parameter- ized knowledge of LLMs with non-parameterized external knowledge bases, making it one of the most important methods for implementing large language models. This paper outlines the develop- ment paradigms of RAG in the era of LLMs, sum- marizing three paradigms: Naive RAG, Advanced RAG, and Modular RAG. It then provides a sum- mary and organization of the three main compo- nents of RAG: retriever, generator, and augmenta- tion methods, along with key technologies in each component. Furthermore, it discusses how to eval- uate the effectiveness of RAG models, introducing two evaluation methods for RAG, emphasizing key metrics and abilities for evaluation, and presenting the latest automatic evaluation framework. Finally, potential future research directions are introduced from three aspects: vertical optimization, horizon- tal scalability, and the technical stack and ecosys- models[Brown et al., 2020, OpenAI, 2023], the LLama series models[Touvron et al., 2023], Gemini[Google, 2023], and other large language models demonstrate impressive lan- guage and knowledge mastery, surpassing human benchmark levels in multiple evaluation benchmarks[Wang et al., 2019, Hendrycks et al., 2020, Srivastava et al., 2022]. However, large language models also exhibit numerous shortcomings. They often fabricate facts[Zhang et al., 2023b] and lack knowledge when dealing with specific domains or highly specialized queries[Kandpal et al., 2023]. For instance, when the infor- mation sought extends beyond the model\u2019s training data or requires the latest data, LLM may fail to provide accurate answers. This limitation poses challenges when deploying generative artificial intelligence in real-world production environments, as blindly using a black-box LLM may not suffice. Traditionally, neural networks adapt to specific domains or proprietary information by fine-tuning models to param- eterize knowledge. While this technique yields significant results, it demands substantial computational resources, in- curs high costs, and requires specialized technical expertise, making it less adaptable to the evolving information land- scape. Parametric knowledge and non-parametric knowledge play distinct roles. Parametric knowledge is acquired through training LLMs and stored in the neural network weights, rep- resenting the model\u2019s understanding and generalization of the training data, forming the foundation for generated re- sponses. Non-parametric knowledge, on the other hand, re- sides in external knowledge sources such as vector databases, not encoded directly into the model but treated as updatable supplementary information. Non-parametric knowledge em- powers LLMs to access and leverage the latest or domain- specific information, enhancing the accuracy and relevance of responses. tem of RAG.1",
    "raw_text": "3 2023 2 0 2 c e D 8 1 ] L C . s c [ 1 v 7 9 9 0 1 . 2 1 3 2 : v i X r a Retrieval-Augmented Generation for Large Language Models: A Survey Yunfan Gao 1 , Yun Xiong 2 , Xinyu Gao 2 , Kangxiang Jia 2 , Jinliu Pan 2 , Yuxi Bi 3 , Yi Dai1 , Jiawei Sun1 and Haofen Wang 1,3 \u2217 1 Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University 2 Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University 3 College of Design and Innovation,Tongji University gaoyunfan1602@gmail.com Abstract Large language models (LLMs) demonstrate pow- erful capabilities, but they still face challenges in practical applications, such as hallucinations, slow knowledge updates, and lack of transparency in answers. Retrieval-Augmented Generation (RAG) refers to the retrieval of relevant information from external knowledge bases before answering ques- tions with LLMs. RAG has been demonstrated to significantly enhance answer accuracy, reduce model hallucination, particularly for knowledge- intensive tasks. By citing sources, users can verify the accuracy of answers and increase trust in model outputs. It also facilitates knowledge updates and the introduction of domain-specific knowl- edge. RAG effectively combines the parameter- ized knowledge of LLMs with non-parameterized external knowledge bases, making it one of the most important methods for implementing large language models. This paper outlines the develop- ment paradigms of RAG in the era of LLMs, sum- marizing three paradigms: Naive RAG, Advanced RAG, and Modular RAG. It then provides a sum- mary and organization of the three main compo- nents of RAG: retriever, generator, and augmenta- tion methods, along with key technologies in each component. Furthermore, it discusses how to eval- uate the effectiveness of RAG models, introducing two evaluation methods for RAG, emphasizing key metrics and abilities for evaluation, and presenting the latest automatic evaluation framework. Finally, potential future research directions are introduced from three aspects: vertical optimization, horizon- tal scalability, and the technical stack and ecosys- models[Brown et al., 2020, OpenAI, 2023], the LLama series models[Touvron et al., 2023], Gemini[Google, 2023], and other large language models demonstrate impressive lan- guage and knowledge mastery, surpassing human benchmark levels in multiple evaluation benchmarks[Wang et al., 2019, Hendrycks et al., 2020, Srivastava et al., 2022]. However, large language models also exhibit numerous shortcomings. They often fabricate facts[Zhang et al., 2023b] and lack knowledge when dealing with specific domains or highly specialized queries[Kandpal et al., 2023]. For instance, when the infor- mation sought extends beyond the model\u2019s training data or requires the latest data, LLM may fail to provide accurate answers. This limitation poses challenges when deploying generative artificial intelligence in real-world production environments, as blindly using a black-box LLM may not suffice. Traditionally, neural networks adapt to specific domains or proprietary information by fine-tuning models to param- eterize knowledge. While this technique yields significant results, it demands substantial computational resources, in- curs high costs, and requires specialized technical expertise, making it less adaptable to the evolving information land- scape. Parametric knowledge and non-parametric knowledge play distinct roles. Parametric knowledge is acquired through training LLMs and stored in the neural network weights, rep- resenting the model\u2019s understanding and generalization of the training data, forming the foundation for generated re- sponses. Non-parametric knowledge, on the other hand, re- sides in external knowledge sources such as vector databases, not encoded directly into the model but treated as updatable supplementary information. Non-parametric knowledge em- powers LLMs to access and leverage the latest or domain- specific information, enhancing the accuracy and relevance of responses. tem of RAG.1"
  },
  {
    "chunk_id": "bd4bef45-0a32-4a5f-8737-50411a61c050",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 1,
    "retrieval_text": "1 Introduction The large language models (LLMs) are more pow- erful than anything we have seen in Natural Lan- guage Processing (NLP) before. The GPT series \u2217Corresponding Author 1Resources are available at: https://github.com/Tongji-KGLLM/ RAG-Survey Purely parameterized language models (LLMs) store their world knowledge, which is acquired from vast corpora, in the parameters of the model. Nevertheless, such models have their limitations. Firstly, it is difficult to retain all the knowl- edge from the training corpus, especially for less common and more specific knowledge. Secondly, since the model parameters cannot be updated dynamically, the parametric knowledge is susceptible to becoming outdated over time. Lastly, an expansion in parameters leads to increased com- putational expenses for both training and inference. To ad- dress the limitations of purely parameterized models, lan- guage models can adopt a semi-parameterized approach by integrating a non-parameterized corpus database with pa- rameterized models. This approach is known as Retrieval- Augmented Generation (RAG). The term Retrieval-Augmented Generation (RAG) was first introduced by [Lewis et al., 2020]. It combines a pre- trained retriever with a pre-trained seq2seq model (generator) and undergoes end-to-end fine-tuning to capture knowledge in a more interpretable and modular way. Before the advent of large models, RAG primarily focused on direct optimiza- tion of end-to-end models. Dense retrievals on the retrieval side, such as the use of vector-based Dense Passage Retrieval (DPR)[Karpukhin et al., 2020], and training smaller models on the generation side are common practices. Due to the overall smaller parameter size, both the retriever and gener- ator often undergo synchronized end-to-end training or fine- tuning[Izacard et al., 2022]. After the emergence of LLM like ChatGPT, generative lan- guage models became predominant, showcasing impressive performance across various language tasks[Bai et al., 2022, OpenAI, 2023, Touvron et al., 2023, Google, 2023]. How- ever, LLMs still face challenges such as hallucina- tions [Yao et al., 2023, Bang et al., 2023], knowledge up- dates, and data-related issues. This affects the relia- bility of LLMs, making them struggle in certain seri- ous task scenarios, especially in knowledge-intensive tasks requiring access to a vast amount of knowledge, such as open-domain question answering[Chen and Yih, 2020, Reddy et al., 2019, Kwiatkowski et al., 2019] and common- sense reasoning[Clark et al., 2019, Bisk et al., 2020]. Im- plicit knowledge within parameters may be incomplete and insufficient. Subsequent research found that introducing RAG into large models\u2019 In-Context Learning (ICL) can alleviate the afore- mentioned issues, with significant and easily implementable effects. During the inference process, RAG dynamically re- trieves information from external knowledge sources, using the retrieved data as references to organize answers. This sub- stantially improves the accuracy and relevance of responses, effectively addressing the hallucination issues present in LLMs. This technique quickly gained traction after the ad- vent of LLM and has become one of the hottest technologies for improving chatbots and making LLM more practical. By separating factual knowledge from the training parameters of LLMs, RAG cleverly combines the powerful capabilities of generative models with the flexibility of retrieval modules, providing an effective solution to the incomplete and insuf- ficient knowledge problem inherent in purely parameterized models. The paper systematically reviews and analyzes the current research approaches and future development paths of RAG, summarizing them into three main paradigms: Naive RAG, Advanced RAG, and Modular RAG. Subsequently, the paper provides a consolidated summary of the three core compo- nents: Retrieval, Augmented, and Generation, highlighting the improvement directions and current technological char- acteristics of RAG. In the section on augmentation methods, the current work is organized into three aspects: the augmen- tation stages of RAG, augmentation data sources, and aug- mentation process. Furthermore, the paper summarizes the evaluation system, applicable scenarios, and other relevant content related to RAG. Through this article, readers gain a more comprehensive and systematic understanding of large models and retrieval-Augmented generation. They become familiar with the evolutionary path and key technologies of knowledge retrieval augment, enabling them to discern the advantages and disadvantages of different techniques, iden- tify applicable scenarios, and explore current typical applica- tion cases in practice.It is noteworthy that in previous work, Feng el al.[2023b] systematically reviewed the methods, ap- plications, and future trends of combining large models with knowledge, with a primary focus on knowledge editing and retrieval augmentation methods. Zhu et al.[2023] introduced the latest advancements in augmenting retrieval systems for Large Language Models, with a specific focus on the retrieval system. Meanwhile, Asai et al.[2023a] focusing on ques- tions such as \u201cWhat\u201d, \u201cWhen\u201d, \u201cHow\u201d, analyzed and eluci- dated the key processes in Retrieval-based Language Mod- els. In comparison with them, this paper aims to systemati- cally outline the entire process of Retrieval-Augmented Gen- eration (RAG) and focuses specifically on research related to augmenting the generation of large language models through knowledge retrieval. The development of RAG algorithms and models is il- lustrated in Fig 1. On a timeline, most of the research re- lated to RAG emerged after 2020, with a significant turn- ing point in December 2022 when ChatGPT was released. Since the release of ChatGPT, research in the field of natu- ral language processing has entered the era of large models. Naive RAG techniques quickly gained prominence, leading to a rapid increase in the number of related studies.In terms of enhancement strategies, research on reinforcement during the pre-training and supervised fine-tuning stages has been ongoing since the concept of RAG was introduced. However, most of the research on reinforcement during the inference stage emerged during the era of LLMs. This is primarily due to the high training costs associated with high-performance large models. Researchers have attempted to enhance model generation by incorporating external knowledge in a cost- effective manner through the inclusion of RAG modules dur- ing the inference stage. Regarding the use of augmented data, early RAG primarily focused on the application of un- structured data, particularly in the context of open-domain question answering. Subsequently, the range of knowledge sources for retrieval expanded, with the use of high-quality data as knowledge sources effectively addressing issues such as internalization of incorrect knowledge and hallucinations in large models. This includes structured knowledge, with knowledge graphs being a representative example. Recently, there has been increased attention on self-retrieval, which in- volves mining the knowledge of LLMs themselves to enhance",
    "raw_text": "1 Introduction The large language models (LLMs) are more pow- erful than anything we have seen in Natural Lan- guage Processing (NLP) before. The GPT series \u2217Corresponding Author 1Resources are available at: https://github.com/Tongji-KGLLM/ RAG-Survey Purely parameterized language models (LLMs) store their world knowledge, which is acquired from vast corpora, in the parameters of the model. Nevertheless, such models have their limitations. Firstly, it is difficult to retain all the knowl- edge from the training corpus, especially for less common and more specific knowledge. Secondly, since the model parameters cannot be updated dynamically, the parametric knowledge is susceptible to becoming outdated over time. Lastly, an expansion in parameters leads to increased com- putational expenses for both training and inference. To ad- dress the limitations of purely parameterized models, lan- guage models can adopt a semi-parameterized approach by integrating a non-parameterized corpus database with pa- rameterized models. This approach is known as Retrieval- Augmented Generation (RAG). The term Retrieval-Augmented Generation (RAG) was first introduced by [Lewis et al., 2020]. It combines a pre- trained retriever with a pre-trained seq2seq model (generator) and undergoes end-to-end fine-tuning to capture knowledge in a more interpretable and modular way. Before the advent of large models, RAG primarily focused on direct optimiza- tion of end-to-end models. Dense retrievals on the retrieval side, such as the use of vector-based Dense Passage Retrieval (DPR)[Karpukhin et al., 2020], and training smaller models on the generation side are common practices. Due to the overall smaller parameter size, both the retriever and gener- ator often undergo synchronized end-to-end training or fine- tuning[Izacard et al., 2022]. After the emergence of LLM like ChatGPT, generative lan- guage models became predominant, showcasing impressive performance across various language tasks[Bai et al., 2022, OpenAI, 2023, Touvron et al., 2023, Google, 2023]. How- ever, LLMs still face challenges such as hallucina- tions [Yao et al., 2023, Bang et al., 2023], knowledge up- dates, and data-related issues. This affects the relia- bility of LLMs, making them struggle in certain seri- ous task scenarios, especially in knowledge-intensive tasks requiring access to a vast amount of knowledge, such as open-domain question answering[Chen and Yih, 2020, Reddy et al., 2019, Kwiatkowski et al., 2019] and common- sense reasoning[Clark et al., 2019, Bisk et al., 2020]. Im- plicit knowledge within parameters may be incomplete and insufficient. Subsequent research found that introducing RAG into large models\u2019 In-Context Learning (ICL) can alleviate the afore- mentioned issues, with significant and easily implementable effects. During the inference process, RAG dynamically re- trieves information from external knowledge sources, using the retrieved data as references to organize answers. This sub- stantially improves the accuracy and relevance of responses, effectively addressing the hallucination issues present in LLMs. This technique quickly gained traction after the ad- vent of LLM and has become one of the hottest technologies for improving chatbots and making LLM more practical. By separating factual knowledge from the training parameters of LLMs, RAG cleverly combines the powerful capabilities of generative models with the flexibility of retrieval modules, providing an effective solution to the incomplete and insuf- ficient knowledge problem inherent in purely parameterized models. The paper systematically reviews and analyzes the current research approaches and future development paths of RAG, summarizing them into three main paradigms: Naive RAG, Advanced RAG, and Modular RAG. Subsequently, the paper provides a consolidated summary of the three core compo- nents: Retrieval, Augmented, and Generation, highlighting the improvement directions and current technological char- acteristics of RAG. In the section on augmentation methods, the current work is organized into three aspects: the augmen- tation stages of RAG, augmentation data sources, and aug- mentation process. Furthermore, the paper summarizes the evaluation system, applicable scenarios, and other relevant content related to RAG. Through this article, readers gain a more comprehensive and systematic understanding of large models and retrieval-Augmented generation. They become familiar with the evolutionary path and key technologies of knowledge retrieval augment, enabling them to discern the advantages and disadvantages of different techniques, iden- tify applicable scenarios, and explore current typical applica- tion cases in practice.It is noteworthy that in previous work, Feng el al.[2023b] systematically reviewed the methods, ap- plications, and future trends of combining large models with knowledge, with a primary focus on knowledge editing and retrieval augmentation methods. Zhu et al.[2023] introduced the latest advancements in augmenting retrieval systems for Large Language Models, with a specific focus on the retrieval system. Meanwhile, Asai et al.[2023a] focusing on ques- tions such as \u201cWhat\u201d, \u201cWhen\u201d, \u201cHow\u201d, analyzed and eluci- dated the key processes in Retrieval-based Language Mod- els. In comparison with them, this paper aims to systemati- cally outline the entire process of Retrieval-Augmented Gen- eration (RAG) and focuses specifically on research related to augmenting the generation of large language models through knowledge retrieval. The development of RAG algorithms and models is il- lustrated in Fig 1. On a timeline, most of the research re- lated to RAG emerged after 2020, with a significant turn- ing point in December 2022 when ChatGPT was released. Since the release of ChatGPT, research in the field of natu- ral language processing has entered the era of large models. Naive RAG techniques quickly gained prominence, leading to a rapid increase in the number of related studies.In terms of enhancement strategies, research on reinforcement during the pre-training and supervised fine-tuning stages has been ongoing since the concept of RAG was introduced. However, most of the research on reinforcement during the inference stage emerged during the era of LLMs. This is primarily due to the high training costs associated with high-performance large models. Researchers have attempted to enhance model generation by incorporating external knowledge in a cost- effective manner through the inclusion of RAG modules dur- ing the inference stage. Regarding the use of augmented data, early RAG primarily focused on the application of un- structured data, particularly in the context of open-domain question answering. Subsequently, the range of knowledge sources for retrieval expanded, with the use of high-quality data as knowledge sources effectively addressing issues such as internalization of incorrect knowledge and hallucinations in large models. This includes structured knowledge, with knowledge graphs being a representative example. Recently, there has been increased attention on self-retrieval, which in- volves mining the knowledge of LLMs themselves to enhance"
  },
  {
    "chunk_id": "ce76c7d0-b40b-4f1e-93f5-37fce827884e",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 2,
    "retrieval_text": "their performance. The subsequent chapters of this paper are structured as fol- lows: Chapter 2 provides an introduction to the background of RAG.Chapter 3 introduces the mainstream paradigms of RAG.Chapter 4 analyzes the retriever in RAG.Chapter 5 fo- 2024 Pre-training Fine-tuning 2023 2022.12 2022 cers 20208 2020 Retrieval\u2014Augmented Generation Inference \u2014\u2014 Augmentation Stage Pre-training XQ (pate Data Augmentation Data WB ercructe 000 \u00a9 stnctred Date Bh covered content Figure 1: A timeline of existing RAG research. The timeline was established mainly according to the release date. cuses on introducing the generator in RAG.Chapter 6 em- phasizes the introduction of the augmentation methods in RAG.Chapter 7 introduces the evaluation system of RAG. Chapter 8 provides an outlook on the future development trends of RAG. Finally, in Chapter 9, we summarize the main contents of the survey. 1. Utilizing encoding models to retrieve relevant docu- ments based on questions, such as BM25, DPR, Col- BERT, and similar approaches[Robertson et al., 2009, Karpukhin et al., 2020, Khattab and Zaharia, 2020]. 2. Generation Phase: Using the retrieved context as a con- dition, the system generates text. 2 Background 2.2 RAG vs Fine-tuning In this chapter, we will introduce the definition of RAG, as well as the comparison between RAG and other model opti- mization techniques, such as fine-tuning. In the optimization of Large Language Models (LLMs), in addition to RAG, another important optimization technique is fine-tuning. 2.1 Definition The meaning of RAG has expanded in tandem with techno- logical developments. In the era of Large Language Mod- els, the specific definition of RAG refers to the model, when answering questions or generating text, first retrieving rele- vant information from a vast corpus of documents. Subse- quently, it utilizes this retrieved information to generate re- sponses or text, thereby enhancing the quality of predictions. The RAG method allows developers to avoid the need for retraining the entire large model for each specific task. In- stead, they can attach a knowledge base, providing additional information input to the model and improving the accuracy of its responses. RAG methods are particularly well-suited for knowledge-intensive tasks. In summary, the RAG system consists of two key stages: RAG is akin to providing a textbook to the model, allow- ing it to retrieve information based on specific queries. This approach is suitable for scenarios where the model needs to answer specific inquiries or address particular information re- trieval tasks. However, RAG is not suitable for teaching the model to understand broad domains or learn new languages, formats, or styles. Fine-tuning is similar to enabling students to internal- ize knowledge through extensive learning. This approach is useful when the model needs to replicate specific struc- tures, styles, or formats. Fine-tuning can enhance the perfor- mance of non-fine-tuned models and make interactions more efficient. It is particularly suitable for emphasizing exist- ing knowledge in the base model, modifying or customizing the model\u2019s output, and providing complex directives to the model. However, fine-tuning is not suitable for incorporating External Knowledge Required High Advanced RAG Index/pre-retrieval/post-retrieval optimization Naive RAG Add relevant contextual Paragraphs Adding few shot case COT)\u201d ry \u2019 \\ Low Promt Engineering ooo S Retriever fine-tuning _--~\"(_ Collaborative fine-tuning All of the above ' 1 1 1 ' ' \u2018 1 : : 4 q ~{. Generator fine-tuning 1 ' \\ *. Prompt preliminary attempts Model Adaptation > Required Low High Figure 2: RAG compared with other model optimization methods new knowledge into the model or for situations that demand quick iteration for new use cases. language models relying solely on training data, RAG maintains the timeliness and accuracy of responses. Fine-tuning is similar to having students internalize knowl- edge through prolonged learning. This method is applicable when the model needs to replicate specific structures, styles, or formats. Fine-tuning can achieve performance superior to non-fine-tuned models, and interactions are more efficient. Fine-tuning is particularly suitable for emphasizing existing knowledge in the base model, modifying or customizing the model\u2019s output, and instructing the model with complex di- rectives. However, fine-tuning is not suitable for adding new knowledge to the model or for scenarios that require rapid it- eration for new use cases. The specific comparison between RAG and Fine-tuning (FT) can be elucidated in Table 1. \u2022 Transparency is an advantage of RAG. By citing sources, users can verify the accuracy of the answers, increasing trust in the model\u2019s output. \u2022 RAG has customization capabilities. Models can be tai- lored to different domains by indexing relevant textual corpora, providing knowledge support for specific fields. \u2022 In terms of security and privacy management, RAG, with its built-in roles and security controls in the database, can better control data usage. In contrast, fine- tuned models may lack clear management of who can access which data. RAG and fine-tuning are not mutually exclusive but can complement each other, enhancing the model\u2019s capabilities at different levels. In certain situations, combining these two techniques can achieve optimal model performance. The en- tire process of optimizing with RAG and fine-tuning may re- quire multiple iterations to achieve satisfactory results. Existing research has demonstrated significant ad- vantages of Retrieval-Augmented Generation (RAG) compared to other methods for optimizing large lan- guage models[Shuster et al., 2021, Yasunaga et al., 2022, Wang et al., 2023c, Borgeaud et al., 2022]: \u2022 RAG is more scalable. It can handle large-scale datasets without the need to update all parameters and create training sets, making it more economically efficient. \u2022 Lastly, results produced by RAG are more trustworthy. RAG selects deterministic results from the latest data, while fine-tuned models may exhibit hallucinations and inaccuracies when dealing with dynamic data, lacking transparency and credibility.",
    "raw_text": "their performance. The subsequent chapters of this paper are structured as fol- lows: Chapter 2 provides an introduction to the background of RAG.Chapter 3 introduces the mainstream paradigms of RAG.Chapter 4 analyzes the retriever in RAG.Chapter 5 fo- 2024 Pre-training Fine-tuning 2023 2022.12 2022 cers 20208 2020 Retrieval\u2014Augmented Generation Inference \u2014\u2014 Augmentation Stage Pre-training XQ (pate Data Augmentation Data WB ercructe 000 \u00a9 stnctred Date Bh covered content Figure 1: A timeline of existing RAG research. The timeline was established mainly according to the release date. cuses on introducing the generator in RAG.Chapter 6 em- phasizes the introduction of the augmentation methods in RAG.Chapter 7 introduces the evaluation system of RAG. Chapter 8 provides an outlook on the future development trends of RAG. Finally, in Chapter 9, we summarize the main contents of the survey. 1. Utilizing encoding models to retrieve relevant docu- ments based on questions, such as BM25, DPR, Col- BERT, and similar approaches[Robertson et al., 2009, Karpukhin et al., 2020, Khattab and Zaharia, 2020]. 2. Generation Phase: Using the retrieved context as a con- dition, the system generates text. 2 Background 2.2 RAG vs Fine-tuning In this chapter, we will introduce the definition of RAG, as well as the comparison between RAG and other model opti- mization techniques, such as fine-tuning. In the optimization of Large Language Models (LLMs), in addition to RAG, another important optimization technique is fine-tuning. 2.1 Definition The meaning of RAG has expanded in tandem with techno- logical developments. In the era of Large Language Mod- els, the specific definition of RAG refers to the model, when answering questions or generating text, first retrieving rele- vant information from a vast corpus of documents. Subse- quently, it utilizes this retrieved information to generate re- sponses or text, thereby enhancing the quality of predictions. The RAG method allows developers to avoid the need for retraining the entire large model for each specific task. In- stead, they can attach a knowledge base, providing additional information input to the model and improving the accuracy of its responses. RAG methods are particularly well-suited for knowledge-intensive tasks. In summary, the RAG system consists of two key stages: RAG is akin to providing a textbook to the model, allow- ing it to retrieve information based on specific queries. This approach is suitable for scenarios where the model needs to answer specific inquiries or address particular information re- trieval tasks. However, RAG is not suitable for teaching the model to understand broad domains or learn new languages, formats, or styles. Fine-tuning is similar to enabling students to internal- ize knowledge through extensive learning. This approach is useful when the model needs to replicate specific struc- tures, styles, or formats. Fine-tuning can enhance the perfor- mance of non-fine-tuned models and make interactions more efficient. It is particularly suitable for emphasizing exist- ing knowledge in the base model, modifying or customizing the model\u2019s output, and providing complex directives to the model. However, fine-tuning is not suitable for incorporating External Knowledge Required High Advanced RAG Index/pre-retrieval/post-retrieval optimization Naive RAG Add relevant contextual Paragraphs Adding few shot case COT)\u201d ry \u2019 \\ Low Promt Engineering ooo S Retriever fine-tuning _--~\"(_ Collaborative fine-tuning All of the above ' 1 1 1 ' ' \u2018 1 : : 4 q ~{. Generator fine-tuning 1 ' \\ *. Prompt preliminary attempts Model Adaptation > Required Low High Figure 2: RAG compared with other model optimization methods new knowledge into the model or for situations that demand quick iteration for new use cases. language models relying solely on training data, RAG maintains the timeliness and accuracy of responses. Fine-tuning is similar to having students internalize knowl- edge through prolonged learning. This method is applicable when the model needs to replicate specific structures, styles, or formats. Fine-tuning can achieve performance superior to non-fine-tuned models, and interactions are more efficient. Fine-tuning is particularly suitable for emphasizing existing knowledge in the base model, modifying or customizing the model\u2019s output, and instructing the model with complex di- rectives. However, fine-tuning is not suitable for adding new knowledge to the model or for scenarios that require rapid it- eration for new use cases. The specific comparison between RAG and Fine-tuning (FT) can be elucidated in Table 1. \u2022 Transparency is an advantage of RAG. By citing sources, users can verify the accuracy of the answers, increasing trust in the model\u2019s output. \u2022 RAG has customization capabilities. Models can be tai- lored to different domains by indexing relevant textual corpora, providing knowledge support for specific fields. \u2022 In terms of security and privacy management, RAG, with its built-in roles and security controls in the database, can better control data usage. In contrast, fine- tuned models may lack clear management of who can access which data. RAG and fine-tuning are not mutually exclusive but can complement each other, enhancing the model\u2019s capabilities at different levels. In certain situations, combining these two techniques can achieve optimal model performance. The en- tire process of optimizing with RAG and fine-tuning may re- quire multiple iterations to achieve satisfactory results. Existing research has demonstrated significant ad- vantages of Retrieval-Augmented Generation (RAG) compared to other methods for optimizing large lan- guage models[Shuster et al., 2021, Yasunaga et al., 2022, Wang et al., 2023c, Borgeaud et al., 2022]: \u2022 RAG is more scalable. It can handle large-scale datasets without the need to update all parameters and create training sets, making it more economically efficient. \u2022 Lastly, results produced by RAG are more trustworthy. RAG selects deterministic results from the latest data, while fine-tuned models may exhibit hallucinations and inaccuracies when dealing with dynamic data, lacking transparency and credibility."
  },
  {
    "chunk_id": "383b074f-86b4-489d-8a86-fa64e3cdd0a7",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 4,
    "retrieval_text": "3 RAG Framework \u2022 RAG improves accuracy by associating answers with ex- ternal knowledge, reducing hallucination issues in lan- guage models and making generated responses more ac- curate and reliable. \u2022 The use of retrieval techniques allows the identifica- tion of the latest information. Compared to traditional The research paradigm of RAG is constantly evolving. This chapter primarily introduces the evolution of the RAG re- search paradigm. We categorize it into three types: Naive RAG, Advanced RAG, and Modular RAG. Although the early RAG was cost-effective and performed better than the native LLM, it still faced many shortcomings. The emergence Feature Comparison RAG Fine-tuning Directly updates the retrieval knowledge Knowledge Updates base, ensuring information remains current without the need for frequent retraining, suit- Stores static data, requiring retraining for knowledge and data updates. able for dynamic data environments. External Knowledge Proficient in utilizing external resources, particularly suitable for documents or other structured/unstructured databases. Can be applied to align the externally learned knowledge from pretraining with large lan- guage models, but may be less practical for frequently changing data sources. Data Processing Requires minimal data processing and han- dling. Relies on constructing high-quality datasets, and limited datasets may not yield significant performance improvements. Focuses on information retrieval and inte- Allows adjustments of LLM behavior, writ- Model Customization grating external knowledge but may not fully ing style, or specific domain knowledge customize model behavior or writing style. based on specific tones or terms. Answers can be traced back to specific data Like a black box, not always clear why the Interpretability sources, providing higher interpretability and model reacts a certain way, with relatively traceability. lower interpretability. Requires computational resources to support Preparation and curation of high-quality Computational Resources retrieval strategies and technologies related to databases. External data source integration training datasets, definition of fine-tuning objectives, and provision of corresponding and updates need to be maintained. computational resources are necessary. Latency Requirements Involves data retrieval, potentially leading to higher latency. LLM after fine-tuning can respond without retrieval, resulting in lower latency. Reducing Hallucinations Inherently less prone to hallucinations as each answer is grounded in retrieved evi- dence. Can help reduce hallucinations by training the model based on specific domain data but may still exhibit hallucinations when faced with unfamiliar input. Ethical and Privacy Issues Ethical and privacy concerns arise from storing and retrieving text from external databases. Ethical and privacy concerns may arise due to sensitive content in the training data. Table 1: Comparison between RAG and Fine-tuning of Advanced RAG and Modular RAG were aimed at address- ing specific deficiencies in the Naive RAG.",
    "raw_text": "3 RAG Framework \u2022 RAG improves accuracy by associating answers with ex- ternal knowledge, reducing hallucination issues in lan- guage models and making generated responses more ac- curate and reliable. \u2022 The use of retrieval techniques allows the identifica- tion of the latest information. Compared to traditional The research paradigm of RAG is constantly evolving. This chapter primarily introduces the evolution of the RAG re- search paradigm. We categorize it into three types: Naive RAG, Advanced RAG, and Modular RAG. Although the early RAG was cost-effective and performed better than the native LLM, it still faced many shortcomings. The emergence Feature Comparison RAG Fine-tuning Directly updates the retrieval knowledge Knowledge Updates base, ensuring information remains current without the need for frequent retraining, suit- Stores static data, requiring retraining for knowledge and data updates. able for dynamic data environments. External Knowledge Proficient in utilizing external resources, particularly suitable for documents or other structured/unstructured databases. Can be applied to align the externally learned knowledge from pretraining with large lan- guage models, but may be less practical for frequently changing data sources. Data Processing Requires minimal data processing and han- dling. Relies on constructing high-quality datasets, and limited datasets may not yield significant performance improvements. Focuses on information retrieval and inte- Allows adjustments of LLM behavior, writ- Model Customization grating external knowledge but may not fully ing style, or specific domain knowledge customize model behavior or writing style. based on specific tones or terms. Answers can be traced back to specific data Like a black box, not always clear why the Interpretability sources, providing higher interpretability and model reacts a certain way, with relatively traceability. lower interpretability. Requires computational resources to support Preparation and curation of high-quality Computational Resources retrieval strategies and technologies related to databases. External data source integration training datasets, definition of fine-tuning objectives, and provision of corresponding and updates need to be maintained. computational resources are necessary. Latency Requirements Involves data retrieval, potentially leading to higher latency. LLM after fine-tuning can respond without retrieval, resulting in lower latency. Reducing Hallucinations Inherently less prone to hallucinations as each answer is grounded in retrieved evi- dence. Can help reduce hallucinations by training the model based on specific domain data but may still exhibit hallucinations when faced with unfamiliar input. Ethical and Privacy Issues Ethical and privacy concerns arise from storing and retrieving text from external databases. Ethical and privacy concerns may arise due to sensitive content in the training data. Table 1: Comparison between RAG and Fine-tuning of Advanced RAG and Modular RAG were aimed at address- ing specific deficiencies in the Naive RAG."
  },
  {
    "chunk_id": "d4f8ef90-0c96-496e-a000-e5bf8271ad2f",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 5,
    "retrieval_text": "3.1 Naive RAG The Naive RAG research paradigm represents the earliest methodology gained prominence shortly after the widespread adoption of ChatGPT. The naive RAG involves traditional process: indexing, retrieval, and generation. Naive RAG is also summarized as a \u201cRetrieve\u201d-\u201cRead\u201d framework [Ma et al., 2023a]. Indexing The pipeline for obtaining data from the source and building an index for it generally occurs in an offline state. Specifi- cally, the construction of a data index involves the following steps: 1.Data Indexing:This involves cleaning and extracting the original data, converting different file formats such as PDF, HTML, Word, Markdown, etc., into plain text. 2.Chunking: This involves dividing the loaded text into smaller chunks. This is necessary because language mod- els typically have a limit on the amount of context they can handle, so it is necessary to create as small text chunks as possible. 3. Embedding and Creating Index: This is the process of encoding text into vectors through a language model. The re- sulting vectors will be used in the subsequent retrieval process to calculate the similarity between the vector and the problem vector.The embedding models require a high inference speed. Since it is necessary to encode a large amount of corpus and encode the problem in real time when the user asks a question, the parameter size of the model should not be too large.After generating the embedding, the next step is to create an in- dex, storing the original corpus chunks and embedding in the form of key-value pairs for quick and frequent searches in the future. Retrieve Given a user\u2019s input, the same encoding model as in the first stage is used to convert the query into a vector. The similarity between the question embedding and the embedding of the document blocks in the corpus is calculated. The top K docu- ment blocks are chosen as the augmented context information for the current question based on the level of similarity. Generation The given question and related documents are combined into a new prompt. The large language model is then tasked with answering the question based on the provided information. It may be decided whether to allow the large model to use its knowledge or only to answer based on the given information, depending on the needs of different tasks. If there is historical dialogue information, it can also be merged into the prompt for multi-round dialogues.",
    "raw_text": "3.1 Naive RAG The Naive RAG research paradigm represents the earliest methodology gained prominence shortly after the widespread adoption of ChatGPT. The naive RAG involves traditional process: indexing, retrieval, and generation. Naive RAG is also summarized as a \u201cRetrieve\u201d-\u201cRead\u201d framework [Ma et al., 2023a]. Indexing The pipeline for obtaining data from the source and building an index for it generally occurs in an offline state. Specifi- cally, the construction of a data index involves the following steps: 1.Data Indexing:This involves cleaning and extracting the original data, converting different file formats such as PDF, HTML, Word, Markdown, etc., into plain text. 2.Chunking: This involves dividing the loaded text into smaller chunks. This is necessary because language mod- els typically have a limit on the amount of context they can handle, so it is necessary to create as small text chunks as possible. 3. Embedding and Creating Index: This is the process of encoding text into vectors through a language model. The re- sulting vectors will be used in the subsequent retrieval process to calculate the similarity between the vector and the problem vector.The embedding models require a high inference speed. Since it is necessary to encode a large amount of corpus and encode the problem in real time when the user asks a question, the parameter size of the model should not be too large.After generating the embedding, the next step is to create an in- dex, storing the original corpus chunks and embedding in the form of key-value pairs for quick and frequent searches in the future. Retrieve Given a user\u2019s input, the same encoding model as in the first stage is used to convert the query into a vector. The similarity between the question embedding and the embedding of the document blocks in the corpus is calculated. The top K docu- ment blocks are chosen as the augmented context information for the current question based on the level of similarity. Generation The given question and related documents are combined into a new prompt. The large language model is then tasked with answering the question based on the provided information. It may be decided whether to allow the large model to use its knowledge or only to answer based on the given information, depending on the needs of different tasks. If there is historical dialogue information, it can also be merged into the prompt for multi-round dialogues."
  },
  {
    "chunk_id": "e1476c70-23da-4058-ace8-6982bf48ce7e",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 6,
    "retrieval_text": "Drawbacks in Naive RAG The Naive RAG confronts principal challenges in three ar- eas: retrieval quality, response generation quality, and the augmentation process. Regarding retrieval quality, the issues are multifaceted. The primary concern is low precision, where not all blocks within the retrieval set correlate with the query, leading to potential hallucination and mid-air drop issues. A secondary issue is low recall, which arises when not all relevant blocks are retrieved, thereby preventing the LLM from obtaining suf- ficient context to synthesize an answer. Additionally, out- dated information presents another challenge, where data re- dundancy or out-of-date data can result in inaccurate retrieval outcomes. In terms of response generation quality, the issues are equally diverse. Hallucination is a prominent issue where the model fabricates an answer that doesn\u2019t exist in the context. Irrelevance is another concern where the model generates an answer that fails to address the query. Further, toxicity or bias, where the model generates a harmful or offensive re- sponse, is another problem. Finally, the augmentation process also faces several chal- lenges. Crucially, the effective integration of the context from retrieved passages with the current generation task is of ut- most importance. If mishandled, the output might appear in- coherent or disjointed. Redundancy and repetition are another issue, particularly when multiple retrieved passages contain similar information, leading to content repetition in the gen- eration step. Moreover, determining the importance or rele- vance of multiple retrieved passages to the generation task is challenging, and the augmentation process needs to balance the value of each passage appropriately. The retrieved con- tent may also come from different writing styles or tones, and the augmentation process needs to reconcile these differences to ensure output consistency. Lastly, generation models may overly rely on augmented information, resulting in output that merely repeats the retrieved content, without providing new value or synthesized information.",
    "raw_text": "Drawbacks in Naive RAG The Naive RAG confronts principal challenges in three ar- eas: retrieval quality, response generation quality, and the augmentation process. Regarding retrieval quality, the issues are multifaceted. The primary concern is low precision, where not all blocks within the retrieval set correlate with the query, leading to potential hallucination and mid-air drop issues. A secondary issue is low recall, which arises when not all relevant blocks are retrieved, thereby preventing the LLM from obtaining suf- ficient context to synthesize an answer. Additionally, out- dated information presents another challenge, where data re- dundancy or out-of-date data can result in inaccurate retrieval outcomes. In terms of response generation quality, the issues are equally diverse. Hallucination is a prominent issue where the model fabricates an answer that doesn\u2019t exist in the context. Irrelevance is another concern where the model generates an answer that fails to address the query. Further, toxicity or bias, where the model generates a harmful or offensive re- sponse, is another problem. Finally, the augmentation process also faces several chal- lenges. Crucially, the effective integration of the context from retrieved passages with the current generation task is of ut- most importance. If mishandled, the output might appear in- coherent or disjointed. Redundancy and repetition are another issue, particularly when multiple retrieved passages contain similar information, leading to content repetition in the gen- eration step. Moreover, determining the importance or rele- vance of multiple retrieved passages to the generation task is challenging, and the augmentation process needs to balance the value of each passage appropriately. The retrieved con- tent may also come from different writing styles or tones, and the augmentation process needs to reconcile these differences to ensure output consistency. Lastly, generation models may overly rely on augmented information, resulting in output that merely repeats the retrieved content, without providing new value or synthesized information."
  },
  {
    "chunk_id": "d016d6bf-97cc-4a12-8feb-b61e92360736",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 6,
    "retrieval_text": "3.2 Advanced RAG Advanced RAG has made targeted improvements to over- come the deficiencies of Naive RAG. In terms of the quality of retrieval generation, Advanced RAG has incorporated pre- retrieval and post-retrieval methods. To address the indexing issues encountered by Naive RAG, Advanced RAG has op- timized indexing through methods such as sliding window, fine-grained segmentation, and metadata. Concurrently, it has put forward various methods to optimize the retrieval process. In terms of specific implementation, Advanced RAG can be adjusted either through a pipeline or in an end-to-end manner. Pre-Retrieval Process \u2022 Optimizing Data Indexing The purpose of optimizing data indexing is to enhance the quality of indexed content. Currently, there are five main strategies employed for this purpose: increasing the granularity of indexed data, optimizing index struc- tures, adding metadata, alignment optimization, and mixed retrieval. 1. Enhancing Data Granularity: The objective of pre-index optimization is to improve text standard- ization, consistency, and ensure factual accuracy and contextual richness to guarantee the perfor- mance of the RAG system. Text standardization in- volves removing irrelevant information and special characters to enhance the efficiency of the retriever. In terms of consistency, the primary task is to elim- inate ambiguity in entities and terms, along with eliminating duplicate or redundant information to simplify the retriever\u2019s focus. Ensuring factual ac- curacy is crucial, and whenever possible, the accu- racy of each piece of data should be verified. Con- text retention, to adapt to the system\u2019s interaction context in the real world, can be achieved by adding another layer of context with domain-specific anno- tations, coupled with continuous updates through user feedback loops. Time sensitivity is essential contextual information, and mechanisms should be designed to refresh outdated documents. In sum- mary, the focus of optimizing indexed data should be on clarity, context, and correctness to make the system efficient and reliable. The following intro- duces best practices. 2. Optimizing Index Structures: This can be achieved by adjusting the size of the chunks, alter- ing the index paths, and incorporating graph struc- ture information. The method of adjusting chunks (Small to Big) involves collecting as much relevant context as possible and minimizing noise. When constructing a RAG system, the chunk size is a key parameter. There are different evaluation frame- works comparing the size of individual chunks. LlamaIndex2 uses GPT4 to assess fidelity and rele- 2https://www.llamaindex.ai vance, and the LLaMA[Touvron et al., 2023] index has an automatic evaluation feature for different chunking methods. The method of querying across multiple index paths is closely related to previous metadata filtering and chunking methods, and may involve querying across different indexes simulta- neously. A standard index can be used to query spe- cific queries, or a standalone index can be used to search or filter based on metadata keywords, such as a specific \u201cdate\u201d index. Introducing a graph structure involves transform- ing entities into nodes and their relationships into relations. This can improve accuracy by leverag- ing the relationships between nodes, especially for multi-hop questions. Using a graph data index can increase the relevance of the retrieval. 3. Adding Metadata Information: The focus here is to embed referenced metadata into chunks, such as dates and purposes used for filtering. Adding metadata like chapters and subsections of refer- ences could also be beneficial for improving re- trieval. When we divide the index into numerous chunks, retrieval efficiency becomes a concern. Fil- tering through metadata first can enhance efficiency and relevance. 4. Alignment Optimization: This strategy primarily addresses alignment issues and differences between documents. The alignment concept involves intro- ducing hypothetical questions, creating questions which are suitable to answer with each document, and embedding (or replacing) these questions with the documents. This helps address alignment prob- lems and discrepancies between documents. 5. Mixed Retrieval: The advantage of this strategy lies in leveraging the strengths of different retrieval technologies. Intelligently combining various tech- niques, including keyword-based search, semantic search, and vector search, adapts to different query types and information needs, ensuring consistent retrieval of the most relevant and context-rich in- formation. Mixed retrieval can serve as a robust complement to retrieval strategies, enhancing the overall performance of the RAG pipeline.",
    "raw_text": "3.2 Advanced RAG Advanced RAG has made targeted improvements to over- come the deficiencies of Naive RAG. In terms of the quality of retrieval generation, Advanced RAG has incorporated pre- retrieval and post-retrieval methods. To address the indexing issues encountered by Naive RAG, Advanced RAG has op- timized indexing through methods such as sliding window, fine-grained segmentation, and metadata. Concurrently, it has put forward various methods to optimize the retrieval process. In terms of specific implementation, Advanced RAG can be adjusted either through a pipeline or in an end-to-end manner. Pre-Retrieval Process \u2022 Optimizing Data Indexing The purpose of optimizing data indexing is to enhance the quality of indexed content. Currently, there are five main strategies employed for this purpose: increasing the granularity of indexed data, optimizing index struc- tures, adding metadata, alignment optimization, and mixed retrieval. 1. Enhancing Data Granularity: The objective of pre-index optimization is to improve text standard- ization, consistency, and ensure factual accuracy and contextual richness to guarantee the perfor- mance of the RAG system. Text standardization in- volves removing irrelevant information and special characters to enhance the efficiency of the retriever. In terms of consistency, the primary task is to elim- inate ambiguity in entities and terms, along with eliminating duplicate or redundant information to simplify the retriever\u2019s focus. Ensuring factual ac- curacy is crucial, and whenever possible, the accu- racy of each piece of data should be verified. Con- text retention, to adapt to the system\u2019s interaction context in the real world, can be achieved by adding another layer of context with domain-specific anno- tations, coupled with continuous updates through user feedback loops. Time sensitivity is essential contextual information, and mechanisms should be designed to refresh outdated documents. In sum- mary, the focus of optimizing indexed data should be on clarity, context, and correctness to make the system efficient and reliable. The following intro- duces best practices. 2. Optimizing Index Structures: This can be achieved by adjusting the size of the chunks, alter- ing the index paths, and incorporating graph struc- ture information. The method of adjusting chunks (Small to Big) involves collecting as much relevant context as possible and minimizing noise. When constructing a RAG system, the chunk size is a key parameter. There are different evaluation frame- works comparing the size of individual chunks. LlamaIndex2 uses GPT4 to assess fidelity and rele- 2https://www.llamaindex.ai vance, and the LLaMA[Touvron et al., 2023] index has an automatic evaluation feature for different chunking methods. The method of querying across multiple index paths is closely related to previous metadata filtering and chunking methods, and may involve querying across different indexes simulta- neously. A standard index can be used to query spe- cific queries, or a standalone index can be used to search or filter based on metadata keywords, such as a specific \u201cdate\u201d index. Introducing a graph structure involves transform- ing entities into nodes and their relationships into relations. This can improve accuracy by leverag- ing the relationships between nodes, especially for multi-hop questions. Using a graph data index can increase the relevance of the retrieval. 3. Adding Metadata Information: The focus here is to embed referenced metadata into chunks, such as dates and purposes used for filtering. Adding metadata like chapters and subsections of refer- ences could also be beneficial for improving re- trieval. When we divide the index into numerous chunks, retrieval efficiency becomes a concern. Fil- tering through metadata first can enhance efficiency and relevance. 4. Alignment Optimization: This strategy primarily addresses alignment issues and differences between documents. The alignment concept involves intro- ducing hypothetical questions, creating questions which are suitable to answer with each document, and embedding (or replacing) these questions with the documents. This helps address alignment prob- lems and discrepancies between documents. 5. Mixed Retrieval: The advantage of this strategy lies in leveraging the strengths of different retrieval technologies. Intelligently combining various tech- niques, including keyword-based search, semantic search, and vector search, adapts to different query types and information needs, ensuring consistent retrieval of the most relevant and context-rich in- formation. Mixed retrieval can serve as a robust complement to retrieval strategies, enhancing the overall performance of the RAG pipeline."
  },
  {
    "chunk_id": "d546ee1c-2f7f-4690-a7b2-28ede7d956d8",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 7,
    "retrieval_text": "Embedding \u2022 Fine-turning Embedding: Fine-tuning embedding models directly impacts the effectiveness of RAG. The purpose of fine-tuning is to enhance the relevance be- tween retrieved content and query. The role of fine- tuning embedding is akin to adjusting ears before gener- ating speech, optimizing the influence of retrieval con- tent on the generated output. Generally, methods for fine-tuning embedding fall into the categories of ad- justing embedding in domain-specific contexts and op- timizing retrieval steps. Especially in professional do- mains dealing with evolving or rare terms, these cus- tomized embedding methods can improve retrieval rel- evance. The BGE[BAAI, 2023]embedding model is a fine-tunning and high-performance embedding model, such as BGE-large-EN developed by the BAAI 3. To cre- ate training data for fine-tuning the BGE model, start by using LLMs like gpt-3.5-turbo to formulate ques- tions based on document chunks, where questions and answers (document chunks) form fine-tuning pairs for the fine-tuning process. \u2022 Dynamic Embedding: Dynamic embedding adjust based on the context in which words appear, differing from static embedding that use a single vector for each word. For instance, in transformer models like BERT, the same word can have varied embeddings depend- ing on surrounding words. Evidence indicates unex- pected high cosine similarity results, especially with text lengths less than 5 tokens, in OpenAI\u2019s text-embedding- ada-002 model4. Ideally, embedding should encompass as much context as possible to ensure \u201chealthy\u201d out- comes.Built upon the principles of large language mod- els like GPT, OpenAI\u2019s embeddings-ada-02 is more so- phisticated than static embedding models, capturing a certain level of context. While it excels in contextual understanding, it may not exhibit the same sensitivity to context as the latest full-size language models like GPT- 4. Post-Retrieval Process After retrieving valuable context from the database, merg- ing it with the query for input into LLM poses challenges. Presenting all relevant documents to the LLM at once may exceed the context window limit. Concatenating numerous documents to form a lengthy retrieval prompt is ineffective, introducing noise and hindering the LLM\u2019s focus on crucial information. Additional processing of the retrieved content is necessary to address these issues. \u2022 ReRank: Re-ranking to relocate the most relevant in- formation to the edges of the prompt is a straightfor- ward idea. This concept has been implemented in frame- works such as LlamaIndex, LangChain, and HayStack [Blagojevi, 2023]. For instance, Diversity Ranker pri- oritizes reordering based on document diversity, while LostInTheMiddleRanker alternates placing the best doc- ument at the beginning and end of the context window. Simultaneously, addressing the challenge of interpreting vector-based simulated searches for semantic similarity, approaches like cohereAI rerank [Cohere, 2023], bge- rerank5, or LongLLMLingua [Jiang et al., 2023a] recal- culate the semantic similarity between relevant text and query. \u2022 Prompt Compression Research indicates that noise in retrieved documents adversely affects RAG perfor- mance. In post-processing, the emphasis lies in com- pressing irrelevant context, highlighting pivotal para- graphs, and reducing the overall context length. Ap- proaches such as Selective Context[Litman et al., 2020] and LLMLingua [Anderson et al., 2022]utilize small 3https://huggingface.co/BAAI/bge-large-en 4https://platform.openai.com/docs/guides/embeddings 5https://huggingface.co/BAAI/bge-reranker-large language models to calculate prompt mutual in- formation or perplexity, estimating element impor- tance. However, these methods may lose key in- formation in RAG or long-context scenarios. Re- comp [Xu et al., 2023a]addresses this by training com- pressors at different granularities. Long Context [Xu et al., 2023b], in dealing with extensive contexts, decomposes and compresses, while \u201cWalking in the Memory Maze\u201d [Chen et al., 2023a]designs a hierarchi- cal summarization tree to enhance LLM\u2019s key informa- tion perception.",
    "raw_text": "Embedding \u2022 Fine-turning Embedding: Fine-tuning embedding models directly impacts the effectiveness of RAG. The purpose of fine-tuning is to enhance the relevance be- tween retrieved content and query. The role of fine- tuning embedding is akin to adjusting ears before gener- ating speech, optimizing the influence of retrieval con- tent on the generated output. Generally, methods for fine-tuning embedding fall into the categories of ad- justing embedding in domain-specific contexts and op- timizing retrieval steps. Especially in professional do- mains dealing with evolving or rare terms, these cus- tomized embedding methods can improve retrieval rel- evance. The BGE[BAAI, 2023]embedding model is a fine-tunning and high-performance embedding model, such as BGE-large-EN developed by the BAAI 3. To cre- ate training data for fine-tuning the BGE model, start by using LLMs like gpt-3.5-turbo to formulate ques- tions based on document chunks, where questions and answers (document chunks) form fine-tuning pairs for the fine-tuning process. \u2022 Dynamic Embedding: Dynamic embedding adjust based on the context in which words appear, differing from static embedding that use a single vector for each word. For instance, in transformer models like BERT, the same word can have varied embeddings depend- ing on surrounding words. Evidence indicates unex- pected high cosine similarity results, especially with text lengths less than 5 tokens, in OpenAI\u2019s text-embedding- ada-002 model4. Ideally, embedding should encompass as much context as possible to ensure \u201chealthy\u201d out- comes.Built upon the principles of large language mod- els like GPT, OpenAI\u2019s embeddings-ada-02 is more so- phisticated than static embedding models, capturing a certain level of context. While it excels in contextual understanding, it may not exhibit the same sensitivity to context as the latest full-size language models like GPT- 4. Post-Retrieval Process After retrieving valuable context from the database, merg- ing it with the query for input into LLM poses challenges. Presenting all relevant documents to the LLM at once may exceed the context window limit. Concatenating numerous documents to form a lengthy retrieval prompt is ineffective, introducing noise and hindering the LLM\u2019s focus on crucial information. Additional processing of the retrieved content is necessary to address these issues. \u2022 ReRank: Re-ranking to relocate the most relevant in- formation to the edges of the prompt is a straightfor- ward idea. This concept has been implemented in frame- works such as LlamaIndex, LangChain, and HayStack [Blagojevi, 2023]. For instance, Diversity Ranker pri- oritizes reordering based on document diversity, while LostInTheMiddleRanker alternates placing the best doc- ument at the beginning and end of the context window. Simultaneously, addressing the challenge of interpreting vector-based simulated searches for semantic similarity, approaches like cohereAI rerank [Cohere, 2023], bge- rerank5, or LongLLMLingua [Jiang et al., 2023a] recal- culate the semantic similarity between relevant text and query. \u2022 Prompt Compression Research indicates that noise in retrieved documents adversely affects RAG perfor- mance. In post-processing, the emphasis lies in com- pressing irrelevant context, highlighting pivotal para- graphs, and reducing the overall context length. Ap- proaches such as Selective Context[Litman et al., 2020] and LLMLingua [Anderson et al., 2022]utilize small 3https://huggingface.co/BAAI/bge-large-en 4https://platform.openai.com/docs/guides/embeddings 5https://huggingface.co/BAAI/bge-reranker-large language models to calculate prompt mutual in- formation or perplexity, estimating element impor- tance. However, these methods may lose key in- formation in RAG or long-context scenarios. Re- comp [Xu et al., 2023a]addresses this by training com- pressors at different granularities. Long Context [Xu et al., 2023b], in dealing with extensive contexts, decomposes and compresses, while \u201cWalking in the Memory Maze\u201d [Chen et al., 2023a]designs a hierarchi- cal summarization tree to enhance LLM\u2019s key informa- tion perception."
  },
  {
    "chunk_id": "a8e90264-ea18-4401-b63e-8341ed132bc1",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 8,
    "retrieval_text": "RAG Pipeline Optimization The optimization of the retrieval process aims to enhance the efficiency and information quality of RAG systems, Current research primarily focuses on intelligently combining various search technologies, optimizing retrieval steps, introducing the concept of cognitive backtracking, flexibly applying di- verse query strategies, and leveraging embedding similarity. These efforts collectively strive to achieve a balance between efficiency and the richness of contextual information in RAG retrieval. \u2022 Exploring Hybrid Search: By intelligently blending various techniques such as keyword-based search, se- mantic search, and vector search, the RAG system can leverage the strengths of each method. This approach enables the RAG system to adapt to different query types and information needs, ensuring consistent retrieval of the most relevant and context-rich information. Hybrid search serves as a robust complement to retrieval strate- gies, enhancing the overall performance of the RAG pipeline. \u2022 Recursive Retrieval and Query Engine:Another pow- erful method to optimize retrieval in the RAG system involves implementing recursive retrieval and a sophis- ticated query engine. Recursive retrieval entails acquir- ing smaller document blocks during the initial retrieval phase to capture key semantic meanings. In the later stages of this process, larger blocks with more contex- tual information are provided to the language model (LM). This two-step retrieval method helps strike a bal- ance between efficiency and contextually rich responses. \u2022 StepBack-prompt: Integrated into the RAG process, the StepBack-prompt approach[Zheng et al., 2023] en- courages LLM to step back from specific instances and engage in reasoning about the underlying general con- cepts or principles. Experimental findings indicate a sig- nificant performance improvement in various challeng- ing, inference-intensive tasks with the incorporation of backward prompts, showcasing its natural adaptability to RAG. The retrieval-enhancing steps can be applied in both the generation of answers to backward prompts and the final question-answering process. \u2022 Subqueries:Various query strategies can be employed in different scenarios, including using query engines pro- vided by frameworks like LlamaIndex, employing tree queries, utilizing vector queries, or employing the most basic sequential querying of chunks. \u2022 HyDE: This approach is grounded on the assumption that the generated answers may be closer in the embed- ding space than a direct query. Utilizing LLM, HyDE generates a hypothetical document (answer) in response to a query, embeds the document, and employs this em- bedding to retrieve real documents similar to the hypo- thetical one. In contrast to seeking embedding similarity based on the query, this method emphasizes the embed- ding similarity from answer to answer. However, it may not consistently yield favorable results, particularly in instances where the language model is unfamiliar with the discussed topic, potentially leading to an increased generation of error-prone instances.",
    "raw_text": "RAG Pipeline Optimization The optimization of the retrieval process aims to enhance the efficiency and information quality of RAG systems, Current research primarily focuses on intelligently combining various search technologies, optimizing retrieval steps, introducing the concept of cognitive backtracking, flexibly applying di- verse query strategies, and leveraging embedding similarity. These efforts collectively strive to achieve a balance between efficiency and the richness of contextual information in RAG retrieval. \u2022 Exploring Hybrid Search: By intelligently blending various techniques such as keyword-based search, se- mantic search, and vector search, the RAG system can leverage the strengths of each method. This approach enables the RAG system to adapt to different query types and information needs, ensuring consistent retrieval of the most relevant and context-rich information. Hybrid search serves as a robust complement to retrieval strate- gies, enhancing the overall performance of the RAG pipeline. \u2022 Recursive Retrieval and Query Engine:Another pow- erful method to optimize retrieval in the RAG system involves implementing recursive retrieval and a sophis- ticated query engine. Recursive retrieval entails acquir- ing smaller document blocks during the initial retrieval phase to capture key semantic meanings. In the later stages of this process, larger blocks with more contex- tual information are provided to the language model (LM). This two-step retrieval method helps strike a bal- ance between efficiency and contextually rich responses. \u2022 StepBack-prompt: Integrated into the RAG process, the StepBack-prompt approach[Zheng et al., 2023] en- courages LLM to step back from specific instances and engage in reasoning about the underlying general con- cepts or principles. Experimental findings indicate a sig- nificant performance improvement in various challeng- ing, inference-intensive tasks with the incorporation of backward prompts, showcasing its natural adaptability to RAG. The retrieval-enhancing steps can be applied in both the generation of answers to backward prompts and the final question-answering process. \u2022 Subqueries:Various query strategies can be employed in different scenarios, including using query engines pro- vided by frameworks like LlamaIndex, employing tree queries, utilizing vector queries, or employing the most basic sequential querying of chunks. \u2022 HyDE: This approach is grounded on the assumption that the generated answers may be closer in the embed- ding space than a direct query. Utilizing LLM, HyDE generates a hypothetical document (answer) in response to a query, embeds the document, and employs this em- bedding to retrieve real documents similar to the hypo- thetical one. In contrast to seeking embedding similarity based on the query, this method emphasizes the embed- ding similarity from answer to answer. However, it may not consistently yield favorable results, particularly in instances where the language model is unfamiliar with the discussed topic, potentially leading to an increased generation of error-prone instances."
  },
  {
    "chunk_id": "d31cb778-8273-4fcb-83c0-c8b3470a9444",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 8,
    "retrieval_text": "Modular RAG The modular RAG structure breaks away from the traditional Naive RAG framework of indexing, retrieval, and genera- tion, offering greater diversity and flexibility in the over- all process. On one hand, it integrates various methods to expand functional modules, such as incorporating a search module in similarity retrieval and applying a fine-tuning ap- proach in the retriever[Lin et al., 2023]. Additionally, spe- cific problems have led to the emergence of restructured RAG modules [Yu et al., 2022] and iterative approaches like [Shao et al., 2023]. The modular RAG paradigm is becom- ing the mainstream in the RAG domain, allowing for ei- ther a serialized pipeline or an end-to-end training approach across multiple modules.The comparison between three RAG paradigms is illustrated in Fig 3. New Modules \u2022 Search Module: Diverging from the similarity re- trieval between queries and corpora in Naive/Advanced RAG, the search module, tailored to specific sce- narios, incorporates direct searches on (additional) corpora in the process using LLM-generated code, query languages (e.g., SQL, Cypher), or other cus- tom tools. The data sources for searching can include search engines, text data, tabular data, or knowledge graphs[Wang et al., 2023c]. \u2022 Memory Module: Leveraging the memory capabili- ties of LLM itself to guide retrieval, the principle in- volves finding memories most similar to the current in- put. Self-mem [Cheng et al., 2023b]iteratively employs a retrieval-enhanced generator to create an unbounded memory pool, combining the \u201coriginal question\u201d and \u201cdual question.\u201d A retrieval-enhanced generative model can use its own outputs to enhance itself, making the text closer to the data distribution in the reasoning pro- cess, with the model\u2019s own outputs rather than training data[Wang et al., 2022a]. \u2022 Extra Generation Module: In retrieved content, re- dundancy and noise are common issues. Instead of di- rectly retrieving from a data source, the Extra Gener- ation Module leverages LLM to generate the required context[Yu et al., 2022]. Content generated by LLM is more likely to contain relevant information compared to direct retrieval. Ea ys Ea js A CAS Ak |CPBS User Query Documents User Query Documents = Document == Document \u00a9) Chunks \u00a9) Chunks <> Vector Bead <> Vector D Database Trochemieval | | @ Database | iS) eons | exrmenneme Puss KR = >it Rerank Filter Prompt Compression Post-Petrieval (B-\u20ac)]) (B-s Prompt LLM Prompt LLM Naive RAG Advanced RAG New Modules Predict Filter New Patterns \u2018On Demand Retrieve Search ~ (Retteve ) Csearch_) S RRR psp ITER-RETGEN Self-RAG Naive RAG \u2014 (wactal 2200} xhattabeta.2no0) shavetal anes} (Asal otal ab) Modular RAG Figure 3: Comparison between the three paradigms of RAG \u2022 Task Adaptable Module: Focused on trans- forming RAG to adapt to various downstream tasks, UPRISE[Cheng et al., 2023a] automati- cally retrieves prompts for given zero-shot task inputs from a pre-constructed data pool, en- hancing universality across tasks and models. PROMPTAGATOR[Dai et al., 2022]utilizes LLM as a few-shot query generator and, based on the gener- ated data, creates task-specific retrievers. Leveraging the generalization capability of LLM, PROMPTAGA- TOR enables the creation of task-specific end-to-end retrievers with just a few examples. \u2022 Alignment Module: The alignment between queries and texts has consistently been a critical issue influenc- ing the effectiveness of RAG. In the era of Modular RAG, researchers have discovered that adding a train- able Adapter module to the retriever can effectively mit- igate alignment issues. PRCA[Yang et al., 2023b] lever- aged reinforcement learning to train a context adapter driven by LLM rewards, positioned between the re- triever and generator. It optimizes the retrieved in- formation by maximizing rewards in the reinforcement learning phase within the labeled autoregressive pol- icy. AAR[Yu et al., 2023b]proposed a universal plu- gin that learns LM preferences from known-source LLMs to assist unknown or non-co-finetuned LLMs. RRR[Ma et al., 2023a]designed a module for rewriting queries based on reinforcement learning to align queries with documents in the corpus. always guaranteed that the retrieved information is reli- able. Retrieving irrelevant data may lead to the occur- rence of illusions in LLM. Therefore, an additional val- idation module can be introduced after retrieving docu- ments to assess the relevance between the retrieved doc- uments and the query. This enhances the robustness of RAG[Yu et al., 2023a].",
    "raw_text": "Modular RAG The modular RAG structure breaks away from the traditional Naive RAG framework of indexing, retrieval, and genera- tion, offering greater diversity and flexibility in the over- all process. On one hand, it integrates various methods to expand functional modules, such as incorporating a search module in similarity retrieval and applying a fine-tuning ap- proach in the retriever[Lin et al., 2023]. Additionally, spe- cific problems have led to the emergence of restructured RAG modules [Yu et al., 2022] and iterative approaches like [Shao et al., 2023]. The modular RAG paradigm is becom- ing the mainstream in the RAG domain, allowing for ei- ther a serialized pipeline or an end-to-end training approach across multiple modules.The comparison between three RAG paradigms is illustrated in Fig 3. New Modules \u2022 Search Module: Diverging from the similarity re- trieval between queries and corpora in Naive/Advanced RAG, the search module, tailored to specific sce- narios, incorporates direct searches on (additional) corpora in the process using LLM-generated code, query languages (e.g., SQL, Cypher), or other cus- tom tools. The data sources for searching can include search engines, text data, tabular data, or knowledge graphs[Wang et al., 2023c]. \u2022 Memory Module: Leveraging the memory capabili- ties of LLM itself to guide retrieval, the principle in- volves finding memories most similar to the current in- put. Self-mem [Cheng et al., 2023b]iteratively employs a retrieval-enhanced generator to create an unbounded memory pool, combining the \u201coriginal question\u201d and \u201cdual question.\u201d A retrieval-enhanced generative model can use its own outputs to enhance itself, making the text closer to the data distribution in the reasoning pro- cess, with the model\u2019s own outputs rather than training data[Wang et al., 2022a]. \u2022 Extra Generation Module: In retrieved content, re- dundancy and noise are common issues. Instead of di- rectly retrieving from a data source, the Extra Gener- ation Module leverages LLM to generate the required context[Yu et al., 2022]. Content generated by LLM is more likely to contain relevant information compared to direct retrieval. Ea ys Ea js A CAS Ak |CPBS User Query Documents User Query Documents = Document == Document \u00a9) Chunks \u00a9) Chunks <> Vector Bead <> Vector D Database Trochemieval | | @ Database | iS) eons | exrmenneme Puss KR = >it Rerank Filter Prompt Compression Post-Petrieval (B-\u20ac)]) (B-s Prompt LLM Prompt LLM Naive RAG Advanced RAG New Modules Predict Filter New Patterns \u2018On Demand Retrieve Search ~ (Retteve ) Csearch_) S RRR psp ITER-RETGEN Self-RAG Naive RAG \u2014 (wactal 2200} xhattabeta.2no0) shavetal anes} (Asal otal ab) Modular RAG Figure 3: Comparison between the three paradigms of RAG \u2022 Task Adaptable Module: Focused on trans- forming RAG to adapt to various downstream tasks, UPRISE[Cheng et al., 2023a] automati- cally retrieves prompts for given zero-shot task inputs from a pre-constructed data pool, en- hancing universality across tasks and models. PROMPTAGATOR[Dai et al., 2022]utilizes LLM as a few-shot query generator and, based on the gener- ated data, creates task-specific retrievers. Leveraging the generalization capability of LLM, PROMPTAGA- TOR enables the creation of task-specific end-to-end retrievers with just a few examples. \u2022 Alignment Module: The alignment between queries and texts has consistently been a critical issue influenc- ing the effectiveness of RAG. In the era of Modular RAG, researchers have discovered that adding a train- able Adapter module to the retriever can effectively mit- igate alignment issues. PRCA[Yang et al., 2023b] lever- aged reinforcement learning to train a context adapter driven by LLM rewards, positioned between the re- triever and generator. It optimizes the retrieved in- formation by maximizing rewards in the reinforcement learning phase within the labeled autoregressive pol- icy. AAR[Yu et al., 2023b]proposed a universal plu- gin that learns LM preferences from known-source LLMs to assist unknown or non-co-finetuned LLMs. RRR[Ma et al., 2023a]designed a module for rewriting queries based on reinforcement learning to align queries with documents in the corpus. always guaranteed that the retrieved information is reli- able. Retrieving irrelevant data may lead to the occur- rence of illusions in LLM. Therefore, an additional val- idation module can be introduced after retrieving docu- ments to assess the relevance between the retrieved doc- uments and the query. This enhances the robustness of RAG[Yu et al., 2023a]."
  },
  {
    "chunk_id": "b3fc8a46-296f-4cde-a057-a25f16b1d55a",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 9,
    "retrieval_text": "New Pattern The organizational approach of Modular RAG is flexible, allowing for the substitution or reconfiguration of modules within the RAG process based on specific problem con- texts. For Naive RAG, which consists of the two modules of retrieval and generation ( referred as read or sythesis in some literature), this framework offers adaptability and abun- dance. Present research primarily explores two organizational paradigms, involving the addition or replacement of modules, as well as the adjustment of the organizational flow between modules. \u2022 Adding or Replacing Modules The strategy of adding or replacing modules entails maintaining the structure of Retrieval-Read while intro- ducing additional modules to enhance specific function- alities. RRR[Ma et al., 2023a] proposes the Rewrite- Retrieve-Read process, utilizing LLM performance as a reward in reinforcement learning for a rewritter module. This allows the rewritter to adjust retrieval queries, im- proving the downstream task performance of the reader. Similarly, modules can be selectively replaced in ap- proaches like Generate-Read[Yu et al., 2022], where the LLM generation module replaces the retrieval module. \u2022 Validation Module: In real-world scenarios, it is not Recite-Read [Sun et al., 2022] transforms external re- trieval into retrieval from model weights, initially hav- ing LLM memorize task-relevant information and gener- ate output for handling knowledge-intensive natural lan- guage processing tasks. \u2022 Adjusting the Flow between Modules In the realm of adjusting the flow between modules, there is an empha- sis on enhancing interaction between language models and retrieval models. DSP[Khattab et al., 2022] intro- duces the Demonstrate-Search-predict framework, treat- ing the context learning system as an explicit program rather than a terminal task prompt to address knowledge- intensive tasks. ITER-RETGEN [Shao et al., 2023] utilizes generated content to guide retrieval, itera- tively performing \u201cretrieval-enhanced generation\u201d and \u201cgeneration-enhanced retrieval\u201d in a Retrieve-Read- Retrieve-Read flow. Self-RAG[Asai et al., 2023b] fol- lows the decide-retrieve-reflect-read process, introduc- ing a module for active judgment. This adaptive and diverse approach allows for the dynamic organization of modules within the Modular RAG framework.",
    "raw_text": "New Pattern The organizational approach of Modular RAG is flexible, allowing for the substitution or reconfiguration of modules within the RAG process based on specific problem con- texts. For Naive RAG, which consists of the two modules of retrieval and generation ( referred as read or sythesis in some literature), this framework offers adaptability and abun- dance. Present research primarily explores two organizational paradigms, involving the addition or replacement of modules, as well as the adjustment of the organizational flow between modules. \u2022 Adding or Replacing Modules The strategy of adding or replacing modules entails maintaining the structure of Retrieval-Read while intro- ducing additional modules to enhance specific function- alities. RRR[Ma et al., 2023a] proposes the Rewrite- Retrieve-Read process, utilizing LLM performance as a reward in reinforcement learning for a rewritter module. This allows the rewritter to adjust retrieval queries, im- proving the downstream task performance of the reader. Similarly, modules can be selectively replaced in ap- proaches like Generate-Read[Yu et al., 2022], where the LLM generation module replaces the retrieval module. \u2022 Validation Module: In real-world scenarios, it is not Recite-Read [Sun et al., 2022] transforms external re- trieval into retrieval from model weights, initially hav- ing LLM memorize task-relevant information and gener- ate output for handling knowledge-intensive natural lan- guage processing tasks. \u2022 Adjusting the Flow between Modules In the realm of adjusting the flow between modules, there is an empha- sis on enhancing interaction between language models and retrieval models. DSP[Khattab et al., 2022] intro- duces the Demonstrate-Search-predict framework, treat- ing the context learning system as an explicit program rather than a terminal task prompt to address knowledge- intensive tasks. ITER-RETGEN [Shao et al., 2023] utilizes generated content to guide retrieval, itera- tively performing \u201cretrieval-enhanced generation\u201d and \u201cgeneration-enhanced retrieval\u201d in a Retrieve-Read- Retrieve-Read flow. Self-RAG[Asai et al., 2023b] fol- lows the decide-retrieve-reflect-read process, introduc- ing a module for active judgment. This adaptive and diverse approach allows for the dynamic organization of modules within the Modular RAG framework."
  },
  {
    "chunk_id": "ec7800d3-840c-431d-8ae7-5aef57ad237d",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 10,
    "retrieval_text": "4 Retriever In the context of RAG, the \u201dR\u201d stands for retrieval, serving the role in the RAG pipeline of retrieving the top-k relevant documents from a vast knowledge base. However, crafting a high-quality retriever is a non-trivial task. In this chapter, we organize our discussions around three key questions: 1) How to acquire accurate semantic representations? 2) How to match the semantic spaces of queries and documents? 3) How to align the output of the retriever with the preferences of the Large Language Model ? 4.1 How to acquire accurate semantic representations? In RAG, semantic space is the multidimensional space where query and Document are mapped. When we perform re- trieval, it is measured within the semantic space. If the se- mantic expression is not accurate, then its effect on RAG is fatal, this section will introduce two methods to help us build a accurate semantic space. Chunk optimization When processing external documents, the first step is chunk- ing to obtain fine-grained features. Then the chunks are Em- bedded. However, Embedding too large or too small text chunks may not achieve good results. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensure the accuracy and relevance of the search results. When choosing a chunking strategy, important considera- tions include: the characteristics of the content being indexed, the embedding model used and its optimal block size, the ex- pected length and complexity of user queries, and how the retrieval results are used in a specific application. For exam- ple, different chunking models should be selected for longer or shorter content. Additionally, different embedding mod- els perform differently at different block sizes; for example, sentence-transformer is more suitable for single sentences, while text-embedding-ada-002 is better for blocks containing 256 or 512 tokens. Furthermore, the length and complexity of the user\u2019s input question text, as well as the specific needs of your application such as semantic search or Q&A, will all affect the choice of chunking strategy. This might directly correlate with the token limits of your chosen LLM, and may require you to adjust the block size. In fact, accurate query results are achieved by adaptively applying several chunking strategies; there is no best, only most suitable. Current research in RAG employs diverse block optimiza- tion methods to improve retrieval efficiency and accuracy. Techniques such as sliding window technology implement layered retrieval by aggregating globally related information through multiple retrievals. The Small2big technique uti- lizes small text blocks during the search process and provides larger affiliated text blocks to the language model for pro- cessing. The Abstract embedding technique performs Top K retrieval on document abstracts, offering full document con- text. The Metadata Filtering technique leverages document metadata for filtering. The Graph Indexing technique con- verts entities and relationships into nodes and connections, significantly enhancing relevance in the context of multi-hop issues. The amalgamation of these methods has resulted in improved retrieval outcomes and enhanced performance for RAG.",
    "raw_text": "4 Retriever In the context of RAG, the \u201dR\u201d stands for retrieval, serving the role in the RAG pipeline of retrieving the top-k relevant documents from a vast knowledge base. However, crafting a high-quality retriever is a non-trivial task. In this chapter, we organize our discussions around three key questions: 1) How to acquire accurate semantic representations? 2) How to match the semantic spaces of queries and documents? 3) How to align the output of the retriever with the preferences of the Large Language Model ? 4.1 How to acquire accurate semantic representations? In RAG, semantic space is the multidimensional space where query and Document are mapped. When we perform re- trieval, it is measured within the semantic space. If the se- mantic expression is not accurate, then its effect on RAG is fatal, this section will introduce two methods to help us build a accurate semantic space. Chunk optimization When processing external documents, the first step is chunk- ing to obtain fine-grained features. Then the chunks are Em- bedded. However, Embedding too large or too small text chunks may not achieve good results. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensure the accuracy and relevance of the search results. When choosing a chunking strategy, important considera- tions include: the characteristics of the content being indexed, the embedding model used and its optimal block size, the ex- pected length and complexity of user queries, and how the retrieval results are used in a specific application. For exam- ple, different chunking models should be selected for longer or shorter content. Additionally, different embedding mod- els perform differently at different block sizes; for example, sentence-transformer is more suitable for single sentences, while text-embedding-ada-002 is better for blocks containing 256 or 512 tokens. Furthermore, the length and complexity of the user\u2019s input question text, as well as the specific needs of your application such as semantic search or Q&A, will all affect the choice of chunking strategy. This might directly correlate with the token limits of your chosen LLM, and may require you to adjust the block size. In fact, accurate query results are achieved by adaptively applying several chunking strategies; there is no best, only most suitable. Current research in RAG employs diverse block optimiza- tion methods to improve retrieval efficiency and accuracy. Techniques such as sliding window technology implement layered retrieval by aggregating globally related information through multiple retrievals. The Small2big technique uti- lizes small text blocks during the search process and provides larger affiliated text blocks to the language model for pro- cessing. The Abstract embedding technique performs Top K retrieval on document abstracts, offering full document con- text. The Metadata Filtering technique leverages document metadata for filtering. The Graph Indexing technique con- verts entities and relationships into nodes and connections, significantly enhancing relevance in the context of multi-hop issues. The amalgamation of these methods has resulted in improved retrieval outcomes and enhanced performance for RAG."
  },
  {
    "chunk_id": "6a4c2bf1-0d6b-49f5-a489-c26a832c7a77",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 10,
    "retrieval_text": "Fine-tuning Embedding Models After getting the proper size of Chunks, we need to Em- bedding the chunks and query in the semantic space by an Embedding model, so it is crucial whether Embedding can represent the corpus effectively. Nowadays, excellent Em- bedding models have appeared, such as [UAE[AngIE, 2023], Voyage[VoyageAI, 2023], BGE[BAAI, 2023], etc.], they have been pre-trained on large-scale corpus, but they may not accurately represent domain-specific corpus information when applied to specific domains. Furthermore, task-specific fine-tuning of Embedding models is critical to ensure that the model understands the user query in relation to the con- tent relevance, whereas an un-fine-tuned model may not be able to fulfill the needs of a specific task. Thus, fine-tuning an Embedding model is essential for downstream applica- tions. There are two basic paradigms in Embedding fine- tuning methods Domain Knowledge Fine-tuning In order for an Embed- ding model to correctly understand domain-specific informa- tion, we need to construct domain-specific datasets to fine- tune the Embedding model. However fine-tuning an Em- bedding model is different from an ordinary language model, mainly in that the datasets used are different. In the current main method of fine-tuning Embedding models, the dataset used consists of three parts, including Queries, Corpus and Relevant Docs. The Embedding model looks up relevant doc- uments in Corpus based on the Query, and then whether the Relevant Docs of the query hit or not is used as a metric for the model. In the construction of datasets, fine-tuning models, and evaluation, numerous challenges may arise in each of these three components. In the LlamaIndex [Liu, 2023], a series of key classes and functions have been introduced specifi- cally for the fine-tuning process of embedding models, signif- icantly streamlining this procedure. By preparing a corpus of domain knowledge and utilizing the methods it provides, we can easily obtain the specialized embedding model tailored to our desired domain. Fine-tuning of downstream tasks It is equally im- portant to adapt Embedding models to downstream tasks. When using RAG in downstream tasks, some works have fine-tuned Embedding models by using the capabilities of LLMs.PROMPTAGATOR[Dai et al., 2022] utilizes the Large Language Model (LLM) as a few-shot query gener- ator and creates task-specific retrievers based on the gen- erated data, and alleviates the problem of supervised fine- tuning, which is difficult in some domains due to data scarcity.LLM-Embedder[Zhang et al., 2023a]uses the Large Language Model to output reward values for data from mul- tiple downstream tasks, fine-tuning the retriever with two dif- ferent supervised signals via hard labeling of the dataset and the soft reward derived from LLM. This somewhat improves the semantic representation through both domain knowledge injection and downstream task fine-tuning. However, the retrievers trained by this ap- proach are not intuitively helpful for large language models, so some work has been done to supervise the fine-tuning of Embedding models directly through feedback signals from the LLM. (This section will be presented in 4.4)",
    "raw_text": "Fine-tuning Embedding Models After getting the proper size of Chunks, we need to Em- bedding the chunks and query in the semantic space by an Embedding model, so it is crucial whether Embedding can represent the corpus effectively. Nowadays, excellent Em- bedding models have appeared, such as [UAE[AngIE, 2023], Voyage[VoyageAI, 2023], BGE[BAAI, 2023], etc.], they have been pre-trained on large-scale corpus, but they may not accurately represent domain-specific corpus information when applied to specific domains. Furthermore, task-specific fine-tuning of Embedding models is critical to ensure that the model understands the user query in relation to the con- tent relevance, whereas an un-fine-tuned model may not be able to fulfill the needs of a specific task. Thus, fine-tuning an Embedding model is essential for downstream applica- tions. There are two basic paradigms in Embedding fine- tuning methods Domain Knowledge Fine-tuning In order for an Embed- ding model to correctly understand domain-specific informa- tion, we need to construct domain-specific datasets to fine- tune the Embedding model. However fine-tuning an Em- bedding model is different from an ordinary language model, mainly in that the datasets used are different. In the current main method of fine-tuning Embedding models, the dataset used consists of three parts, including Queries, Corpus and Relevant Docs. The Embedding model looks up relevant doc- uments in Corpus based on the Query, and then whether the Relevant Docs of the query hit or not is used as a metric for the model. In the construction of datasets, fine-tuning models, and evaluation, numerous challenges may arise in each of these three components. In the LlamaIndex [Liu, 2023], a series of key classes and functions have been introduced specifi- cally for the fine-tuning process of embedding models, signif- icantly streamlining this procedure. By preparing a corpus of domain knowledge and utilizing the methods it provides, we can easily obtain the specialized embedding model tailored to our desired domain. Fine-tuning of downstream tasks It is equally im- portant to adapt Embedding models to downstream tasks. When using RAG in downstream tasks, some works have fine-tuned Embedding models by using the capabilities of LLMs.PROMPTAGATOR[Dai et al., 2022] utilizes the Large Language Model (LLM) as a few-shot query gener- ator and creates task-specific retrievers based on the gen- erated data, and alleviates the problem of supervised fine- tuning, which is difficult in some domains due to data scarcity.LLM-Embedder[Zhang et al., 2023a]uses the Large Language Model to output reward values for data from mul- tiple downstream tasks, fine-tuning the retriever with two dif- ferent supervised signals via hard labeling of the dataset and the soft reward derived from LLM. This somewhat improves the semantic representation through both domain knowledge injection and downstream task fine-tuning. However, the retrievers trained by this ap- proach are not intuitively helpful for large language models, so some work has been done to supervise the fine-tuning of Embedding models directly through feedback signals from the LLM. (This section will be presented in 4.4)"
  },
  {
    "chunk_id": "0e1e67ea-d35f-4fbe-8040-e4cd9561bf8c",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 11,
    "retrieval_text": "4.2 How to Match the Semantic Space of Queries and Documents In the RAG application, some retrievers use the same embed- ding model to encode the query and doc, while others use two models to separately encode the query and doc. Moreover, the original query of the user may have problems of poor expres- sion and lack of semantic information. Therefore, aligning the semantic space of the user\u2019s query and documents is very necessary. This section introduces two key technologies to achieve this goal. Query Rewrite The most intuitive way to align the semantics of query and document is to rewrite the query. As mentioned in Query2Doc[Wang et al., 2023b] and ITER- RETGEN[Shao et al., 2023], the inherent capabilities of large language models are utilized to generate a pseudo- document by guiding it, and then the original query is merged with this pseudo-document to form a new query. In HyDE[Gao et al., 2022], query vectors are established through the use of text indicators, using these indicators to generate a \u2019hypothetical\u2019 document that is relevant, yet may not truly exist, it only needs to capture the relevant pattern. RRR[Ma et al., 2023a]introduced a new framework that in- verts the order of retrieval and reading, focusing on query rewriting. This method generates a query using a large lan- guage model, then uses a web search engine to retrieve con- text, and finally uses a small language model as a train- ing rewriter to serve the frozen large language model. The STEP-BACKPROMPTING[Zheng et al., 2023] method can make large language models carry out abstract reasoning, ex- tract high-level concepts and principles, and conduct retrieval based on this. Lastly, the method in Multi Query Retrieval involves using large language models to generate multiple search queries, these queries can be executed in parallel, and the retrieval results are input together, which is very useful for single problems that rely on multiple sub-problems Embedding Transformation If there is a coarse-grained method like rewriting queries, there should also be a finer-grained implementation spe- cific for embedding operations. In LlamaIndex[Liu, 2023], it is possible to connect an adapter after the query en- coder, and fine-tune the adapter to optimize the represen- tation of query embeddings, mapping it to a latent space that is better suited for specific tasks.When the data struc- ture of a query and an external document are different, such as an unstructured query and a structured external docu- ment, it is very important to enable the query to align with the document.SANTA[Li et al., 2023d] proposes two pre- training methods to make the retriever aware of structured information 1) Using the natural alignment relationship be- tween structured data and unstructured data for contrastive learning for structured-aware pre-training. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities.",
    "raw_text": "4.2 How to Match the Semantic Space of Queries and Documents In the RAG application, some retrievers use the same embed- ding model to encode the query and doc, while others use two models to separately encode the query and doc. Moreover, the original query of the user may have problems of poor expres- sion and lack of semantic information. Therefore, aligning the semantic space of the user\u2019s query and documents is very necessary. This section introduces two key technologies to achieve this goal. Query Rewrite The most intuitive way to align the semantics of query and document is to rewrite the query. As mentioned in Query2Doc[Wang et al., 2023b] and ITER- RETGEN[Shao et al., 2023], the inherent capabilities of large language models are utilized to generate a pseudo- document by guiding it, and then the original query is merged with this pseudo-document to form a new query. In HyDE[Gao et al., 2022], query vectors are established through the use of text indicators, using these indicators to generate a \u2019hypothetical\u2019 document that is relevant, yet may not truly exist, it only needs to capture the relevant pattern. RRR[Ma et al., 2023a]introduced a new framework that in- verts the order of retrieval and reading, focusing on query rewriting. This method generates a query using a large lan- guage model, then uses a web search engine to retrieve con- text, and finally uses a small language model as a train- ing rewriter to serve the frozen large language model. The STEP-BACKPROMPTING[Zheng et al., 2023] method can make large language models carry out abstract reasoning, ex- tract high-level concepts and principles, and conduct retrieval based on this. Lastly, the method in Multi Query Retrieval involves using large language models to generate multiple search queries, these queries can be executed in parallel, and the retrieval results are input together, which is very useful for single problems that rely on multiple sub-problems Embedding Transformation If there is a coarse-grained method like rewriting queries, there should also be a finer-grained implementation spe- cific for embedding operations. In LlamaIndex[Liu, 2023], it is possible to connect an adapter after the query en- coder, and fine-tune the adapter to optimize the represen- tation of query embeddings, mapping it to a latent space that is better suited for specific tasks.When the data struc- ture of a query and an external document are different, such as an unstructured query and a structured external docu- ment, it is very important to enable the query to align with the document.SANTA[Li et al., 2023d] proposes two pre- training methods to make the retriever aware of structured information 1) Using the natural alignment relationship be- tween structured data and unstructured data for contrastive learning for structured-aware pre-training. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities."
  },
  {
    "chunk_id": "2316537c-3f0c-465c-a04b-e5930f46940d",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 11,
    "retrieval_text": "4.3 How to Aligning Retriever\u2019s Output and LLM\u2019s Preference In the RAG pipeline, even if we employ the above techniques to enhance the retrieval hit rate, it may still not improve the final effect of RAG, because the retrieved documents may not be what LLM needs. Thus, this section introduces two meth- ods to align the outputs of the retriever and the preferences of the LLM. LLM supervised training Many works leverage various feedback signals from large language models to fine-tune em- bedding models. AAR[Yu et al., 2023b] provides supervi- sory signals for a pre-trained retriever through an encoder- decoder architecture LM. By determining the LM\u2019s preferred documents through FiD cross-attention scores, the retriever is then fine-tuned with hard negative sampling and standard cross-entropy loss. Ultimately, the fine-tuned retriever can directly be used to enhance unseen target LMs, thereby per- forming better in the target task. The training loss of retriever as: c= YL Mita). faa)) w q@ dtepat d~ED- where Da+ is the documents preferred by the LLM in the retrieved set and Da\u2212 is not preferred. l is the standard cross entropy loss. In the end,it is suggested that LLMs may have a preference for focusing on readable rather than information- rich documents REPLUG[Shi et al., 2023] uses a retriever and an LLM to calculate the probability distributions of the retrieved docu- ments, and then performs supervised training by calculating the KL divergence. This simple and effective training method enhances the performance of the retrieval model by using an LM as the supervisory signal, eliminating the need for any specific cross-attention mechanisms. The training loss of the retriever is as follows: 1 \u00b0 = iDj Yo KL (Pa (de) [IQra(die.y)) 2) \u00abED where D is a set of input contexts, PRis the retrieval like- lihood, QLM is the LM likelihood of each document. UPRISE[Cheng et al., 2023a] also employs frozen large language models to fine-tune the Prompt Retriever. But both the language model and the retriever take Prompt-Input Pairs as inputs, then uses the scores given by the large lan- guage model to supervise the training of the retriever, equiva- lent to using the large language model to label the dataset. Atlas[Izacard et al., 2022] proposes four methods of fine- tuning supervised embedding models, among them, Attention Distillation distills using the cross-attention scores that the language model generates during the output. EMDR2 em- ploys the Expectation-Maximization algorithm to train with the retrieved documents as latent variables. Perplexity Dis- tillation directly trains using the perplexity of the model- generated tokens as an indicator.LOOP introduces a new loss function based on the effect of document deletion on LM prediction, providing an effective training strategy for better adapting the model to specific tasks. Plug in an adapter However, fine-tuning an embed- ding model can be challenging due to factors such as utilizing an API to implement embedding functionality or insufficient local computational resources. There- fore, some works choose to externally attach an adapter for alignment.PRCA[Yang et al., 2023b] trains the Adapter through the Contextual Extraction Stage and the Reward- Driven Stage, and optimizes the output of the re- triever based on a token-based autoregressive strategy. TokenFiltering[Berchansky et al., 2023] method calculates cross-attention scores, selecting the highest scoring input to- kens to effectively filter tokens. RECOMP[Xu et al., 2023a] proposes extractive and generative compressors, which gen- erate summaries by selecting relevant sentences or syn- thesizing document information to achieve multi-document query focus summaries.In addition to that, a novel approach, PKG[Luo et al., 2023], infuses knowledge into a white-box model through directive fine-tuning, and directly replaces the retriever module, used to directly output relevant documents based on the query.",
    "raw_text": "4.3 How to Aligning Retriever\u2019s Output and LLM\u2019s Preference In the RAG pipeline, even if we employ the above techniques to enhance the retrieval hit rate, it may still not improve the final effect of RAG, because the retrieved documents may not be what LLM needs. Thus, this section introduces two meth- ods to align the outputs of the retriever and the preferences of the LLM. LLM supervised training Many works leverage various feedback signals from large language models to fine-tune em- bedding models. AAR[Yu et al., 2023b] provides supervi- sory signals for a pre-trained retriever through an encoder- decoder architecture LM. By determining the LM\u2019s preferred documents through FiD cross-attention scores, the retriever is then fine-tuned with hard negative sampling and standard cross-entropy loss. Ultimately, the fine-tuned retriever can directly be used to enhance unseen target LMs, thereby per- forming better in the target task. The training loss of retriever as: c= YL Mita). faa)) w q@ dtepat d~ED- where Da+ is the documents preferred by the LLM in the retrieved set and Da\u2212 is not preferred. l is the standard cross entropy loss. In the end,it is suggested that LLMs may have a preference for focusing on readable rather than information- rich documents REPLUG[Shi et al., 2023] uses a retriever and an LLM to calculate the probability distributions of the retrieved docu- ments, and then performs supervised training by calculating the KL divergence. This simple and effective training method enhances the performance of the retrieval model by using an LM as the supervisory signal, eliminating the need for any specific cross-attention mechanisms. The training loss of the retriever is as follows: 1 \u00b0 = iDj Yo KL (Pa (de) [IQra(die.y)) 2) \u00abED where D is a set of input contexts, PRis the retrieval like- lihood, QLM is the LM likelihood of each document. UPRISE[Cheng et al., 2023a] also employs frozen large language models to fine-tune the Prompt Retriever. But both the language model and the retriever take Prompt-Input Pairs as inputs, then uses the scores given by the large lan- guage model to supervise the training of the retriever, equiva- lent to using the large language model to label the dataset. Atlas[Izacard et al., 2022] proposes four methods of fine- tuning supervised embedding models, among them, Attention Distillation distills using the cross-attention scores that the language model generates during the output. EMDR2 em- ploys the Expectation-Maximization algorithm to train with the retrieved documents as latent variables. Perplexity Dis- tillation directly trains using the perplexity of the model- generated tokens as an indicator.LOOP introduces a new loss function based on the effect of document deletion on LM prediction, providing an effective training strategy for better adapting the model to specific tasks. Plug in an adapter However, fine-tuning an embed- ding model can be challenging due to factors such as utilizing an API to implement embedding functionality or insufficient local computational resources. There- fore, some works choose to externally attach an adapter for alignment.PRCA[Yang et al., 2023b] trains the Adapter through the Contextual Extraction Stage and the Reward- Driven Stage, and optimizes the output of the re- triever based on a token-based autoregressive strategy. TokenFiltering[Berchansky et al., 2023] method calculates cross-attention scores, selecting the highest scoring input to- kens to effectively filter tokens. RECOMP[Xu et al., 2023a] proposes extractive and generative compressors, which gen- erate summaries by selecting relevant sentences or syn- thesizing document information to achieve multi-document query focus summaries.In addition to that, a novel approach, PKG[Luo et al., 2023], infuses knowledge into a white-box model through directive fine-tuning, and directly replaces the retriever module, used to directly output relevant documents based on the query."
  },
  {
    "chunk_id": "b95c7553-6af9-4cab-a5fc-62e94125a703",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 12,
    "retrieval_text": "5 Generator Another core component in RAG is the generator, responsible for transforming retrieved information into natural and fluent text. Its design is inspired by traditional language models, but in comparison to conventional generative models, RAG\u2019s generator enhances accuracy and relevance by leveraging the retrieved information. In RAG, the generator\u2019s input includes not only traditional contextual information but also relevant text segments obtained through the retriever. This allows the generator to better comprehend the context behind the ques- tion and produce responses that are more information-rich. Furthermore, the generator is guided by the retrieved text to ensure consistency between the generated content and the re- trieved information. It is the diversity of input data that has led to a series of targeted efforts during the generation phase, all aimed at better adapting the large model to the input data from queries and documents. We will delve into the intro- duction of the generator through aspects of post-retrieval pro- cessing and fine-tuning. 5.1 How Can Retrieval Results be Enhanced via Post-retrieval Processing? In terms of untuned large language models, most studies rely on well-recognized large language models like GPT- 4[OpenAI, 2023] to leverage their robust internal knowl- edge for the comprehensive retrieval of document knowledge. However, inherent issues of these large models, such as con- text length restrictions and vulnerability to redundant infor- mation, persist. To mitigate these issues, some research has made efforts in post-retrieval processing. Post-retrieval pro- cessing refers to the process of further treating, filtering, or optimizing the relevant information retrieved by the retriever from a large document database. Its primary purpose is to en- hance the quality of retrieval results to better meet user needs or for subsequent tasks. It can be understood as a process of reprocessing the documents obtained in the retrieval phase. The operations of post-retrieval processing usually involve in- formation compression and result rerank.",
    "raw_text": "5 Generator Another core component in RAG is the generator, responsible for transforming retrieved information into natural and fluent text. Its design is inspired by traditional language models, but in comparison to conventional generative models, RAG\u2019s generator enhances accuracy and relevance by leveraging the retrieved information. In RAG, the generator\u2019s input includes not only traditional contextual information but also relevant text segments obtained through the retriever. This allows the generator to better comprehend the context behind the ques- tion and produce responses that are more information-rich. Furthermore, the generator is guided by the retrieved text to ensure consistency between the generated content and the re- trieved information. It is the diversity of input data that has led to a series of targeted efforts during the generation phase, all aimed at better adapting the large model to the input data from queries and documents. We will delve into the intro- duction of the generator through aspects of post-retrieval pro- cessing and fine-tuning. 5.1 How Can Retrieval Results be Enhanced via Post-retrieval Processing? In terms of untuned large language models, most studies rely on well-recognized large language models like GPT- 4[OpenAI, 2023] to leverage their robust internal knowl- edge for the comprehensive retrieval of document knowledge. However, inherent issues of these large models, such as con- text length restrictions and vulnerability to redundant infor- mation, persist. To mitigate these issues, some research has made efforts in post-retrieval processing. Post-retrieval pro- cessing refers to the process of further treating, filtering, or optimizing the relevant information retrieved by the retriever from a large document database. Its primary purpose is to en- hance the quality of retrieval results to better meet user needs or for subsequent tasks. It can be understood as a process of reprocessing the documents obtained in the retrieval phase. The operations of post-retrieval processing usually involve in- formation compression and result rerank."
  },
  {
    "chunk_id": "02c527ed-cfba-48c1-9bfa-528c7542c62f",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 12,
    "retrieval_text": "Information Compression Even though the retriever can fetch relevant information from a vast knowledge base, we are still confronted with the chal- lenge of dealing with a substantial amount of information in retrieval documents. Some existing research attempts to solve this problem by increasing the context length of large lan- guage models, but current large models still confront context limitations. Thus, in certain situations, information conden- sation is necessary. In short, the importance of information condensation mainly embodies in the following aspects: re- duction of noise, coping with context length restrictions, and enhancing generation effects. PRCA [Yang et al., 2023b] addressed this issue by train- ing an information extractor. In the context extraction stage, given an input text Sinput, it can generate an output sequence Cextracted, which represents the condensed context from the input document. The objective of the training process is to minimize the discrepancy between Cextracted and the actual context Ctruth as much as possible. The loss function they adopted is as follows: N . 1 i AG minL(8) =~ Y) CirunloIL(Sinputi9)) GB) i=1 where f. is the information extractor and \u03b8 is the parameter of the extractor. RECOMP[Xu et al., 2023a] similarly trains an information condenser by leveraging contrastive learning. For each training data point, there exists one positive sample and five negative samples. The encoder is trained using con- trastive loss [Karpukhin et al., 2020] during this process.The specific optimization goals are as follows: esim(2i,P:) 4 sim(ai, pi) + DV njen 2sim(i,pi) lo (4) where xi is the training data, pi is the positive sample, and nj is the negative sample,sim(x,y) is to calculate the simi- larity between x and y. Another study has chosen to further streamline the quantity of documents, aiming to enhance the model\u2019s answer accuracy by reducing the number of retrieved documents. [Ma et al., 2023b] proposed the \u201cFilter-Ranker\u201d paradigm, which integrates the strengths of Large Language Models (LLMs) and Small Language Models (SLMs). In this paradigm, SLMs serve as filters, while LLMs function as re- ordering agents. By prompting LLMs to rearrange portions of difficult samples identified by SLMs, the research results indicate significant improvements across various Information Extraction (IE) tasks.",
    "raw_text": "Information Compression Even though the retriever can fetch relevant information from a vast knowledge base, we are still confronted with the chal- lenge of dealing with a substantial amount of information in retrieval documents. Some existing research attempts to solve this problem by increasing the context length of large lan- guage models, but current large models still confront context limitations. Thus, in certain situations, information conden- sation is necessary. In short, the importance of information condensation mainly embodies in the following aspects: re- duction of noise, coping with context length restrictions, and enhancing generation effects. PRCA [Yang et al., 2023b] addressed this issue by train- ing an information extractor. In the context extraction stage, given an input text Sinput, it can generate an output sequence Cextracted, which represents the condensed context from the input document. The objective of the training process is to minimize the discrepancy between Cextracted and the actual context Ctruth as much as possible. The loss function they adopted is as follows: N . 1 i AG minL(8) =~ Y) CirunloIL(Sinputi9)) GB) i=1 where f. is the information extractor and \u03b8 is the parameter of the extractor. RECOMP[Xu et al., 2023a] similarly trains an information condenser by leveraging contrastive learning. For each training data point, there exists one positive sample and five negative samples. The encoder is trained using con- trastive loss [Karpukhin et al., 2020] during this process.The specific optimization goals are as follows: esim(2i,P:) 4 sim(ai, pi) + DV njen 2sim(i,pi) lo (4) where xi is the training data, pi is the positive sample, and nj is the negative sample,sim(x,y) is to calculate the simi- larity between x and y. Another study has chosen to further streamline the quantity of documents, aiming to enhance the model\u2019s answer accuracy by reducing the number of retrieved documents. [Ma et al., 2023b] proposed the \u201cFilter-Ranker\u201d paradigm, which integrates the strengths of Large Language Models (LLMs) and Small Language Models (SLMs). In this paradigm, SLMs serve as filters, while LLMs function as re- ordering agents. By prompting LLMs to rearrange portions of difficult samples identified by SLMs, the research results indicate significant improvements across various Information Extraction (IE) tasks."
  },
  {
    "chunk_id": "c8cd7c73-7bd1-42c9-b81b-e6ec9fb9258b",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 13,
    "retrieval_text": "Rerank The pivotal role of the reordering model lies in optimizing the set of documents retrieved from retriever. LLMs ex- perience performance degradation with retrospective perfor- mance when additional context is added, and reordering pro- vides an effective solution to address this issue. The core idea involves rearranging document records to place the most rel- evant items at the top, thereby reducing the total number of documents to a fixed quantity. This not only resolves the issue of context window expansion that may be encountered during retrieval but also contributes to improving retrieval efficiency and responsiveness[Zhuang et al., 2023]. Introducing context compression as part of the reordering aims to return relevant information solely based on the given query context. The dual significance of this approach lies in concentrating the display of the most relevant information in the retrieval results by reducing the content of individual doc- uments and filtering entire documents. Thus, the reordering model plays an optimizing and refining role throughout the information retrieval process, providing more effective and accurate inputs for subsequent LLM processing. 5.2 How to Optimize a Generator to Adapt Input Data? In the RAG model, the optimization of the generator is a cru- cial component of the architecture. The generator\u2019s task is to take the retrieved information and generate relevant text, thereby providing the final output of the model. The goal of optimizing the generator is to ensure that the generated text is both natural and effectively utilizes the retrieved documents, in order to better satisfy the user\u2019s query needs. In typical Large Language Model (LLM) generation tasks, the input is usually a query. In RAG, the main difference lies in the fact that the input includes not only a query but also various documents retrieved by the retriever (struc- tured/unstructured). The introduction of additional informa- tion may have a significant impact on the model\u2019s understand- ing, especially for smaller models. In such scenarios, fine- tuning the model to adapt to the input of query + retrieved documents becomes particularly important. Specifically, be- fore providing the input to the fine-tuned model, there is usu- ally post-retrieval processing of the documents retrieved by the retriever. It is essential to note that the method of fine- tuning the generator in RAG is essentially similar to the gen- eral fine-tuning approach for LLMs. Here, we will briefly introduce some representative works, including data (format- ted/unformatted) and optimization functions.",
    "raw_text": "Rerank The pivotal role of the reordering model lies in optimizing the set of documents retrieved from retriever. LLMs ex- perience performance degradation with retrospective perfor- mance when additional context is added, and reordering pro- vides an effective solution to address this issue. The core idea involves rearranging document records to place the most rel- evant items at the top, thereby reducing the total number of documents to a fixed quantity. This not only resolves the issue of context window expansion that may be encountered during retrieval but also contributes to improving retrieval efficiency and responsiveness[Zhuang et al., 2023]. Introducing context compression as part of the reordering aims to return relevant information solely based on the given query context. The dual significance of this approach lies in concentrating the display of the most relevant information in the retrieval results by reducing the content of individual doc- uments and filtering entire documents. Thus, the reordering model plays an optimizing and refining role throughout the information retrieval process, providing more effective and accurate inputs for subsequent LLM processing. 5.2 How to Optimize a Generator to Adapt Input Data? In the RAG model, the optimization of the generator is a cru- cial component of the architecture. The generator\u2019s task is to take the retrieved information and generate relevant text, thereby providing the final output of the model. The goal of optimizing the generator is to ensure that the generated text is both natural and effectively utilizes the retrieved documents, in order to better satisfy the user\u2019s query needs. In typical Large Language Model (LLM) generation tasks, the input is usually a query. In RAG, the main difference lies in the fact that the input includes not only a query but also various documents retrieved by the retriever (struc- tured/unstructured). The introduction of additional informa- tion may have a significant impact on the model\u2019s understand- ing, especially for smaller models. In such scenarios, fine- tuning the model to adapt to the input of query + retrieved documents becomes particularly important. Specifically, be- fore providing the input to the fine-tuned model, there is usu- ally post-retrieval processing of the documents retrieved by the retriever. It is essential to note that the method of fine- tuning the generator in RAG is essentially similar to the gen- eral fine-tuning approach for LLMs. Here, we will briefly introduce some representative works, including data (format- ted/unformatted) and optimization functions."
  },
  {
    "chunk_id": "c743aa9c-dbee-4195-98ee-1c5a07860927",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 13,
    "retrieval_text": "General Optimization Process Refers to the training data containing pairs of (input, output), aiming to train the model\u2019s ability to generate output y given input x. In the work of Self-mem[Cheng et al., 2023b], a relatively classical training process is employed. Given in- put x, relevant documents z are retrieved (selecting Top-1 in the paper), and after integrating (x, z), the model gener- ates output y. The paper utilizes two common paradigms for fine-tuning, namely Joint-Encoder [Arora et al., 2023, Wang et al., 2022b, Lewis et al., 2020] and Dual-Encoder [Xia et al., 2019, Cai et al., 2021, Cheng et al., 2022]. For Joint-Encoder, a standard model based on encoder-decoder is used, where the encoder initially encodes the input, and the decoder, through attention mechanisms, combines the en- coded results to generate tokens in an autoregressive manner: H = Encoder(x[SEP]m) (5) hi = Decoder(CrossAttn(H),y < i) (6) PG\u03be(.|x,y < i) = Softmax(hi) (7) For the Dual-Encoder, the system establishes two indepen- dent encoders, each responsible for encoding the input (query, context) and the document, respectively. The output is then subject to bidirectional cross-attention processing by the de- coder in sequence. The authors choose to use the Transformer [Vaswani et al., 2017] as the building block for both architec- tures and optimize G\u03be Negative Log-Likelihood (NLL) loss. Hx = SourceEncoder(x)Hm = MemoryEncoder(x) (8) hi = Decoder(CrossAttn(Hx,Hm),y < i) (9) lyl Lnu = \u2014 So logPa,(yelz,.my <t) (10) t=1 Utilizing Contrastive Learning In the phase of preparing training data, usually generated are pairs of interactions between inputs and outputs. Un- der this circumstance, the model can only access a unique real output which might induce the \u201dexposure bias\u201d prob- lem [Ranzato et al., 2015]: during the training phase, the model only exposes to a single true feedback without ac- cessing any other generated tokens. This can impair the model\u2019s performance in application as it might excessively fit to specific feedback in the training data without effec- tively generalizing to other scenarios. Therefore, a graph-text contrastive learning method has been proposed by SURGE [Kang et al., 2023]. For any given pair of interactions be- tween inputs and outputs, the objective of this contrastive learning approach can be defined as follows: 1 esim(C(2),\u20ac(h))/t 1 esim((z),\u20ac(h))/t Leont = glog SCOuna blog St, esti CETEMY\" qd) Where \u03b6,\u03be are learnable linear projection layers.z is the av- erage representations of the graph from Encoder,h is the mean of decoder representations.z\u2032,h\u2032 represent the corresponding negative samples respectively. In the given text, \u2019h\u201d and \u2019z\u201d represent negative samples. By introducing a contrastive learning objective, the model can learn to generate diverse and reasonable replies better, rather than just the one seen in the training data. This helps to mitigate the risk of overfitting and improves the model\u2019s generalization ability in real-world scenarios. When dealing with retrieval tasks that involve structured data, the work of SANTA[Li et al., 2023d] utilized a three- stage training process to fully understand the structural and semantic information. Specifically, in the training phase of the retriever, contrastive learning was adopted, with the main goal of optimizing the embedding representations of the queries and documents. The specific optimization objectives are as follows: esim(q.d*) (09 Fae Yep PGT) Lpr (12) where q and d are the query and document encoded by the encoder.d\u2212,d+ represent negative samples and positive sam- ples respectively. In the initial training stage of the gener- ator, we utilize contrastive learning to align structured data and the corresponding document description of unstructured data. The optimization objective is as above. Moreover, in the later training stage of the gener- ator, inspired by references [Sciavolino et al., 2021, Zhang et al., 2019], we recognized the remarkable ef- fectiveness of entity semantics in learning textual data representations in retrieval. Thus, we first perform entity identification in the structured data, subsequently applying a mask to the entities in the input section of the generator\u2019s training data, enabling the generator to predict these masks. The optimization objective hereafter is: k Luep = > \u2014logP(Ya(t;)|XP\", Yalti, vj -1)) j=l (13) where Yd(yj denotes the j-th token in the sequence Yd. And Yd = < mask >1, ent1, ..., < mask >n, entn de- notes the ground truth sequence that contains masked enti- ties. Throughout the training process, we recover the masked entities by acquiring necessary information from the context, understand the structural semantics of textual data, and align the relevant entities in the structured data. We optimize the language model to fill the concealed spans and to better com- prehend the entity semantics[Ye et al., 2020].",
    "raw_text": "General Optimization Process Refers to the training data containing pairs of (input, output), aiming to train the model\u2019s ability to generate output y given input x. In the work of Self-mem[Cheng et al., 2023b], a relatively classical training process is employed. Given in- put x, relevant documents z are retrieved (selecting Top-1 in the paper), and after integrating (x, z), the model gener- ates output y. The paper utilizes two common paradigms for fine-tuning, namely Joint-Encoder [Arora et al., 2023, Wang et al., 2022b, Lewis et al., 2020] and Dual-Encoder [Xia et al., 2019, Cai et al., 2021, Cheng et al., 2022]. For Joint-Encoder, a standard model based on encoder-decoder is used, where the encoder initially encodes the input, and the decoder, through attention mechanisms, combines the en- coded results to generate tokens in an autoregressive manner: H = Encoder(x[SEP]m) (5) hi = Decoder(CrossAttn(H),y < i) (6) PG\u03be(.|x,y < i) = Softmax(hi) (7) For the Dual-Encoder, the system establishes two indepen- dent encoders, each responsible for encoding the input (query, context) and the document, respectively. The output is then subject to bidirectional cross-attention processing by the de- coder in sequence. The authors choose to use the Transformer [Vaswani et al., 2017] as the building block for both architec- tures and optimize G\u03be Negative Log-Likelihood (NLL) loss. Hx = SourceEncoder(x)Hm = MemoryEncoder(x) (8) hi = Decoder(CrossAttn(Hx,Hm),y < i) (9) lyl Lnu = \u2014 So logPa,(yelz,.my <t) (10) t=1 Utilizing Contrastive Learning In the phase of preparing training data, usually generated are pairs of interactions between inputs and outputs. Un- der this circumstance, the model can only access a unique real output which might induce the \u201dexposure bias\u201d prob- lem [Ranzato et al., 2015]: during the training phase, the model only exposes to a single true feedback without ac- cessing any other generated tokens. This can impair the model\u2019s performance in application as it might excessively fit to specific feedback in the training data without effec- tively generalizing to other scenarios. Therefore, a graph-text contrastive learning method has been proposed by SURGE [Kang et al., 2023]. For any given pair of interactions be- tween inputs and outputs, the objective of this contrastive learning approach can be defined as follows: 1 esim(C(2),\u20ac(h))/t 1 esim((z),\u20ac(h))/t Leont = glog SCOuna blog St, esti CETEMY\" qd) Where \u03b6,\u03be are learnable linear projection layers.z is the av- erage representations of the graph from Encoder,h is the mean of decoder representations.z\u2032,h\u2032 represent the corresponding negative samples respectively. In the given text, \u2019h\u201d and \u2019z\u201d represent negative samples. By introducing a contrastive learning objective, the model can learn to generate diverse and reasonable replies better, rather than just the one seen in the training data. This helps to mitigate the risk of overfitting and improves the model\u2019s generalization ability in real-world scenarios. When dealing with retrieval tasks that involve structured data, the work of SANTA[Li et al., 2023d] utilized a three- stage training process to fully understand the structural and semantic information. Specifically, in the training phase of the retriever, contrastive learning was adopted, with the main goal of optimizing the embedding representations of the queries and documents. The specific optimization objectives are as follows: esim(q.d*) (09 Fae Yep PGT) Lpr (12) where q and d are the query and document encoded by the encoder.d\u2212,d+ represent negative samples and positive sam- ples respectively. In the initial training stage of the gener- ator, we utilize contrastive learning to align structured data and the corresponding document description of unstructured data. The optimization objective is as above. Moreover, in the later training stage of the gener- ator, inspired by references [Sciavolino et al., 2021, Zhang et al., 2019], we recognized the remarkable ef- fectiveness of entity semantics in learning textual data representations in retrieval. Thus, we first perform entity identification in the structured data, subsequently applying a mask to the entities in the input section of the generator\u2019s training data, enabling the generator to predict these masks. The optimization objective hereafter is: k Luep = > \u2014logP(Ya(t;)|XP\", Yalti, vj -1)) j=l (13) where Yd(yj denotes the j-th token in the sequence Yd. And Yd = < mask >1, ent1, ..., < mask >n, entn de- notes the ground truth sequence that contains masked enti- ties. Throughout the training process, we recover the masked entities by acquiring necessary information from the context, understand the structural semantics of textual data, and align the relevant entities in the structured data. We optimize the language model to fill the concealed spans and to better com- prehend the entity semantics[Ye et al., 2020]."
  },
  {
    "chunk_id": "ede80e34-fdaa-47ca-8315-32aedfb3b8f8",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 14,
    "retrieval_text": "6 Augmentation in RAG This chapter is primarily organized into three dimensions: the stage of augmentation, augmentation data sources, and the process of augmentation, to elaborate on the key technolo- gies in the development of RAG.Taxonomy of RAG\u2019s Core Components is illustrated in Fig 4. 6.1 RAG in Augmentation Stages As a knowledge-intensive task, RAG employs different tech- nical approaches during the language model training\u2019s pre- training, fine-tuning, and inference stages. Pre-training Stage Since the emergence of pre-trained models, researchers have delved into enhancing the performance of Pre-trained Lan- guage Models (PTMs) in open-domain Question Answering (QA) through retrieval methods at the pre-training stage. Rec- ognizing and expanding implicit knowledge in pre-trained models can be challenging. REALM[Arora et al., 2023] in- troduces a more modular and interpretable knowledge em- bedding approach. Following the Masked Language Model (MLM) paradigm, REALM models both pre-training and fine-tuning as a retrieve-then-predict process, where the lan- guage model pre-trains by predicting masked tokens y based on masked sentences x, modeling P(x|y). RETRO[Borgeaud et al., 2022]leverages retrieval mentation for pre-training a self-regressive language model, enabling large-scale pre-training from scratch by retrieving from a massive set of labeled data and significantly reducing model parameters. RETRO shares the backbone structure with GPT models and introduces an additional RETRO encoder to encode features of neighboring entities retrieved from an external knowledge base. Additionally, RETRO incorporates block-wise cross-attention layers in its decoder transformer structure to effectively integrate retrieval infor- mation from the RETRO encoder. RETRO achieves lower perplexity than standard GPT models. Moreover, it provides flexibility in updating knowledge stored in the language models by updating the retrieval database without the need for retraining the language models [Petroni et al., 2019]. aug- Atla[Izacard et al., 2022]employs a similar approach, in- corporating a retrieval mechanism using the T5 architecture [Raffel et al., 2020]in both the pre-training and fine-tuning stages. Prior to pre-training, it initializes the encoder-decoder LM backbone with a pre-trained T5, and initializes the dense retriever with a pre-trained Contriever. During the pre- training process, it refreshes the asynchronous index every 1000 steps. COG [Vaze et al., 2021]is a text generation model that for- malizes its generation process by gradually copying text frag- ments (such as words or phrases) from an existing collection of text. Unlike traditional text generation models that select words sequentially, COG utilizes efficient vector search tools to calculate meaningful context representations of text frag- ments and index them. Consequently, the text generation task is decomposed into a series of copy and paste operations, where at each time step, relevant text fragments are sought from the text collection instead of selecting from an indepen- dent vocabulary. COG demonstrates superior performance to RETRO in various aspects, including question-answering, domain adaptation, and expanded phrase indexing. On the other hand, following the discovery of the scal- ing law, there has been a rapid increase in model parameters, making autoregressive models the mainstream. Researchers are also exploring whether larger models can be pretrained using the RAG approach. RETRO++[Wang et al., 2023a], an Retriever (4) Chunk Optimization Better Semantic Representation Fine-tuning Embedding Model Query Rewrite/Clarification Align Queries and documents Embedding Transformation Plugin Adapter Align Retriever and LLM LLM Supervised Training \u2018Small2big;Sliding-window:Abstract- Embedding;Metadata Filtering[Liu, 2023) PROMPTAGATORIDai et al, 2022] ; BGE[BAAI, 2023]; LLM-Embedder(Zhang et al, 20252] Query2Doc{Wang et al, 2023d]; RRR[Ma et al, 2023al;, \u2018STEP-BACKPROMPTING[Zheng et al, 2023]; HyDE[Ga0_ et al, 2022];TOCIKim et al, 2023] SANTAILiet al, 20236) PKG[Luo et al, 2023]; RECOMP [Xu et al, 2023]; TokenFiltering[Berchansky et al,2023] \u2018AARIYu et al, 2023]; REPLUGIShi et al, 2023] ; Atlasflzacard et al, 2022] ; UPRISE[Cheng et al, 2023a] Retrieval-Augmented Generation (Augmentation Stage ition Process |} {Set Memicheng et al, 20236), (RETRO [Borgeaud et al, 20221; Atlas [lzacard et al, \\, | 2022]; REALM [Arora et al, 2023]; Toolformer [Schick | | Jet al, 20231; C06 [Vaze ef al, 2021; RAVENTHuang et } RETRO\u201c ! (DPR [Karpukhin et al, 2020] ; UPRISE[Cheng et al, | | 2023a}; FiDllzacard and Grave, 2020}; RA-DITILin et.al, | || 2023]; Self-RAGIAsai et al, 2023]; SUGRE[Kang et al, * | 2023}; SANTA[Li et al, 2023b]; REPLUGISHi et al, | 2023]; AAR[Yu et al, 2023] KNN-LMOchandletwal et al, 2019]; DSPIKhattab et al, | 2022); KAR|Purwar and Sundar, 2023]; PRCA[Yang et \u201cal, 2023a]; IRCOT[Trivedi et al, 2022]; GenRead|Yu et ICRALM[Ram et al, 2023] (UPRISE[Cheng et al, 2023a] 2023a; COG[Vaze et al, 21 >__|PKGILv0 et ab, 2023); LLM-RIWang et al, 202: ' J Atlas [Izacard) et aaa eel oe Soy Iterative Retrieval = al., 2023); Self-RAG[Asai et al, 2023]; J Raventhvang etal, 2025) i Figure 4: Taxonomy of RAG\u2019s Core Components extension of RETRO, increased the model\u2019s parameter scale. Studies have found consistent improvements in text genera- tion quality, factual accuracy, low toxicity, and downstream task accuracy, particularly in knowledge-intensive tasks such as open-domain question answering. These research findings highlight the promising direction of pretraining autoregres- sive language models in conjunction with retrieval for future foundational models. In summary, the advantages and limitations of augmented pre-training are evident. On the positive side, this approach offers a more powerful foundational model, outperforming standard GPT models in perplexity, text generation quality, and downstream task performance. Moreover, it achieves higher efficiency by utilizing fewer parameters compared to purely pre-trained models. It particularly excels in handling knowledge-intensive tasks, allowing the creation of domain- specific models through training on domain-specific corpora. However, there are drawbacks, including the requirement for a substantial amount of pre-training data and larger training resources, as well as the issue of slower update speeds. Espe- cially as model size increases, the cost of retrieval-enhanced training becomes relatively higher. Despite these limitations, this method demonstrates notable characteristics in terms of model robustness. Once trained, retrieval-enhanced models based on pure pre-training eliminate the need for external li-",
    "raw_text": "6 Augmentation in RAG This chapter is primarily organized into three dimensions: the stage of augmentation, augmentation data sources, and the process of augmentation, to elaborate on the key technolo- gies in the development of RAG.Taxonomy of RAG\u2019s Core Components is illustrated in Fig 4. 6.1 RAG in Augmentation Stages As a knowledge-intensive task, RAG employs different tech- nical approaches during the language model training\u2019s pre- training, fine-tuning, and inference stages. Pre-training Stage Since the emergence of pre-trained models, researchers have delved into enhancing the performance of Pre-trained Lan- guage Models (PTMs) in open-domain Question Answering (QA) through retrieval methods at the pre-training stage. Rec- ognizing and expanding implicit knowledge in pre-trained models can be challenging. REALM[Arora et al., 2023] in- troduces a more modular and interpretable knowledge em- bedding approach. Following the Masked Language Model (MLM) paradigm, REALM models both pre-training and fine-tuning as a retrieve-then-predict process, where the lan- guage model pre-trains by predicting masked tokens y based on masked sentences x, modeling P(x|y). RETRO[Borgeaud et al., 2022]leverages retrieval mentation for pre-training a self-regressive language model, enabling large-scale pre-training from scratch by retrieving from a massive set of labeled data and significantly reducing model parameters. RETRO shares the backbone structure with GPT models and introduces an additional RETRO encoder to encode features of neighboring entities retrieved from an external knowledge base. Additionally, RETRO incorporates block-wise cross-attention layers in its decoder transformer structure to effectively integrate retrieval infor- mation from the RETRO encoder. RETRO achieves lower perplexity than standard GPT models. Moreover, it provides flexibility in updating knowledge stored in the language models by updating the retrieval database without the need for retraining the language models [Petroni et al., 2019]. aug- Atla[Izacard et al., 2022]employs a similar approach, in- corporating a retrieval mechanism using the T5 architecture [Raffel et al., 2020]in both the pre-training and fine-tuning stages. Prior to pre-training, it initializes the encoder-decoder LM backbone with a pre-trained T5, and initializes the dense retriever with a pre-trained Contriever. During the pre- training process, it refreshes the asynchronous index every 1000 steps. COG [Vaze et al., 2021]is a text generation model that for- malizes its generation process by gradually copying text frag- ments (such as words or phrases) from an existing collection of text. Unlike traditional text generation models that select words sequentially, COG utilizes efficient vector search tools to calculate meaningful context representations of text frag- ments and index them. Consequently, the text generation task is decomposed into a series of copy and paste operations, where at each time step, relevant text fragments are sought from the text collection instead of selecting from an indepen- dent vocabulary. COG demonstrates superior performance to RETRO in various aspects, including question-answering, domain adaptation, and expanded phrase indexing. On the other hand, following the discovery of the scal- ing law, there has been a rapid increase in model parameters, making autoregressive models the mainstream. Researchers are also exploring whether larger models can be pretrained using the RAG approach. RETRO++[Wang et al., 2023a], an Retriever (4) Chunk Optimization Better Semantic Representation Fine-tuning Embedding Model Query Rewrite/Clarification Align Queries and documents Embedding Transformation Plugin Adapter Align Retriever and LLM LLM Supervised Training \u2018Small2big;Sliding-window:Abstract- Embedding;Metadata Filtering[Liu, 2023) PROMPTAGATORIDai et al, 2022] ; BGE[BAAI, 2023]; LLM-Embedder(Zhang et al, 20252] Query2Doc{Wang et al, 2023d]; RRR[Ma et al, 2023al;, \u2018STEP-BACKPROMPTING[Zheng et al, 2023]; HyDE[Ga0_ et al, 2022];TOCIKim et al, 2023] SANTAILiet al, 20236) PKG[Luo et al, 2023]; RECOMP [Xu et al, 2023]; TokenFiltering[Berchansky et al,2023] \u2018AARIYu et al, 2023]; REPLUGIShi et al, 2023] ; Atlasflzacard et al, 2022] ; UPRISE[Cheng et al, 2023a] Retrieval-Augmented Generation (Augmentation Stage ition Process |} {Set Memicheng et al, 20236), (RETRO [Borgeaud et al, 20221; Atlas [lzacard et al, \\, | 2022]; REALM [Arora et al, 2023]; Toolformer [Schick | | Jet al, 20231; C06 [Vaze ef al, 2021; RAVENTHuang et } RETRO\u201c ! (DPR [Karpukhin et al, 2020] ; UPRISE[Cheng et al, | | 2023a}; FiDllzacard and Grave, 2020}; RA-DITILin et.al, | || 2023]; Self-RAGIAsai et al, 2023]; SUGRE[Kang et al, * | 2023}; SANTA[Li et al, 2023b]; REPLUGISHi et al, | 2023]; AAR[Yu et al, 2023] KNN-LMOchandletwal et al, 2019]; DSPIKhattab et al, | 2022); KAR|Purwar and Sundar, 2023]; PRCA[Yang et \u201cal, 2023a]; IRCOT[Trivedi et al, 2022]; GenRead|Yu et ICRALM[Ram et al, 2023] (UPRISE[Cheng et al, 2023a] 2023a; COG[Vaze et al, 21 >__|PKGILv0 et ab, 2023); LLM-RIWang et al, 202: ' J Atlas [Izacard) et aaa eel oe Soy Iterative Retrieval = al., 2023); Self-RAG[Asai et al, 2023]; J Raventhvang etal, 2025) i Figure 4: Taxonomy of RAG\u2019s Core Components extension of RETRO, increased the model\u2019s parameter scale. Studies have found consistent improvements in text genera- tion quality, factual accuracy, low toxicity, and downstream task accuracy, particularly in knowledge-intensive tasks such as open-domain question answering. These research findings highlight the promising direction of pretraining autoregres- sive language models in conjunction with retrieval for future foundational models. In summary, the advantages and limitations of augmented pre-training are evident. On the positive side, this approach offers a more powerful foundational model, outperforming standard GPT models in perplexity, text generation quality, and downstream task performance. Moreover, it achieves higher efficiency by utilizing fewer parameters compared to purely pre-trained models. It particularly excels in handling knowledge-intensive tasks, allowing the creation of domain- specific models through training on domain-specific corpora. However, there are drawbacks, including the requirement for a substantial amount of pre-training data and larger training resources, as well as the issue of slower update speeds. Espe- cially as model size increases, the cost of retrieval-enhanced training becomes relatively higher. Despite these limitations, this method demonstrates notable characteristics in terms of model robustness. Once trained, retrieval-enhanced models based on pure pre-training eliminate the need for external li-"
  },
  {
    "chunk_id": "e3f610c4-809c-44ef-ac4e-6ab9bfc9fdaf",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 15,
    "retrieval_text": "brary dependencies, enhancing both generation speed and op- erational efficiency. Fine-tuning Stage During the downstream fine-tuning phase, researchers have employed various methods to fine-tune retrievers and gener- ators for improved information retrieval, primarily in open- domain question-answering tasks. Concerning retriever fine- tuning, REPlUG[Shi et al., 2023] treats the language model (LM) as a black box and enhances it through an adjustable re- trieval model. By obtaining feedback from the black-box lan- guage model through supervised signals, REPLUG improves the initial retrieval model. UPRISE[Cheng et al., 2023a], on the other hand, fine-tunes retrievers by creating a lightweight and versatile retriever through fine-tuning on diverse task sets. This retriever can automatically provide retrieval prompts for zero-shot tasks, showcasing its universality and improved performance across tasks and models. Simultaneously, methods for fine-tuning generators in- clude Self-Mem[Cheng et al., 2023b], which fine-tunes the generator through a memory pool of examples, Self-RAG[Asai et al., 2023b], which satisfies active re- trieval needs by generating reflection tokens. The RA- DIT[Lin et al., 2023] method fine-tunes both the generator and retriever by maximizing the probability of correct an- and swers given a retrieval-enhanced directive. It updates the gen- erator and retriever to minimize the semantic similarity be- tween documents and queries, effectively leveraging relevant background knowledge. Additionally, SUGRE[Kang et al., 2023] introduces the concept of contrastive learning. It conducts end-to-end fine- tuning of both retriever and generator, ensuring highly de- tailed text generation and retrieved subgraphs. Using a context-aware subgraph retriever based on Graph Neural Net- works (GNN), SURGE extracts relevant knowledge from a knowledge graph corresponding to an ongoing conversation. This ensures the generated responses faithfully reflect the re- trieved knowledge. SURGE employs an invariant yet efficient graph encoder and a graph-text contrastive learning objective for this purpose. In summary, the enhancement methods during the fine- tuning phase exhibit several characteristics. Firstly, fine- tuning both LLM and retriever allows better adaptation to specific tasks, offering the flexibility to fine-tune ei- ther one or both simultaneously, as seen in methods like RePlug[Shi et al., 2023] and RA-DIT[Lin et al., 2023]. Sec- ondly, the benefits of this fine-tuning extend to adapt- ing to diverse downstream tasks, as demonstrated by UPRISE[Cheng et al., 2023a], making the model more ver- satile. Additionally, fine-tuning enables models to better ac- commodate different data structures in various corpora, par- ticularly advantageous for graph-structured corpora, as high- lighted by the SUGRE method. However, fine-tuning during this phase comes with limita- tions, such as the need for datasets specifically prepared for RAG fine-tuning and the requirement for substantial compu- tational resources compared to the RAG during the inference phase. Overall, during fine-tuning, researchers have the flexi- bility to tailor models according to specific requirements and data formats, reducing the resource consumption compared to the pre-training phase while retaining the ability to adjust the model\u2019s output style.",
    "raw_text": "brary dependencies, enhancing both generation speed and op- erational efficiency. Fine-tuning Stage During the downstream fine-tuning phase, researchers have employed various methods to fine-tune retrievers and gener- ators for improved information retrieval, primarily in open- domain question-answering tasks. Concerning retriever fine- tuning, REPlUG[Shi et al., 2023] treats the language model (LM) as a black box and enhances it through an adjustable re- trieval model. By obtaining feedback from the black-box lan- guage model through supervised signals, REPLUG improves the initial retrieval model. UPRISE[Cheng et al., 2023a], on the other hand, fine-tunes retrievers by creating a lightweight and versatile retriever through fine-tuning on diverse task sets. This retriever can automatically provide retrieval prompts for zero-shot tasks, showcasing its universality and improved performance across tasks and models. Simultaneously, methods for fine-tuning generators in- clude Self-Mem[Cheng et al., 2023b], which fine-tunes the generator through a memory pool of examples, Self-RAG[Asai et al., 2023b], which satisfies active re- trieval needs by generating reflection tokens. The RA- DIT[Lin et al., 2023] method fine-tunes both the generator and retriever by maximizing the probability of correct an- and swers given a retrieval-enhanced directive. It updates the gen- erator and retriever to minimize the semantic similarity be- tween documents and queries, effectively leveraging relevant background knowledge. Additionally, SUGRE[Kang et al., 2023] introduces the concept of contrastive learning. It conducts end-to-end fine- tuning of both retriever and generator, ensuring highly de- tailed text generation and retrieved subgraphs. Using a context-aware subgraph retriever based on Graph Neural Net- works (GNN), SURGE extracts relevant knowledge from a knowledge graph corresponding to an ongoing conversation. This ensures the generated responses faithfully reflect the re- trieved knowledge. SURGE employs an invariant yet efficient graph encoder and a graph-text contrastive learning objective for this purpose. In summary, the enhancement methods during the fine- tuning phase exhibit several characteristics. Firstly, fine- tuning both LLM and retriever allows better adaptation to specific tasks, offering the flexibility to fine-tune ei- ther one or both simultaneously, as seen in methods like RePlug[Shi et al., 2023] and RA-DIT[Lin et al., 2023]. Sec- ondly, the benefits of this fine-tuning extend to adapt- ing to diverse downstream tasks, as demonstrated by UPRISE[Cheng et al., 2023a], making the model more ver- satile. Additionally, fine-tuning enables models to better ac- commodate different data structures in various corpora, par- ticularly advantageous for graph-structured corpora, as high- lighted by the SUGRE method. However, fine-tuning during this phase comes with limita- tions, such as the need for datasets specifically prepared for RAG fine-tuning and the requirement for substantial compu- tational resources compared to the RAG during the inference phase. Overall, during fine-tuning, researchers have the flexi- bility to tailor models according to specific requirements and data formats, reducing the resource consumption compared to the pre-training phase while retaining the ability to adjust the model\u2019s output style."
  },
  {
    "chunk_id": "d3acf465-6c4d-4eff-b424-aeedbfff3753",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 16,
    "retrieval_text": "Inference Stage The integration of RAG methods with LLM has become a prevalent research direction in the inference phase. Notably, the research paradigm of Naive RAG relies on incorporating retrieval content during the inference stage. To overcome the limitations of Naive RAG, researchers have introduced richer context in the RAG during the in- ference phase. The DSP[Khattab et al., 2022] framework re- lies on a complex pipeline that involves passing natural lan- guage text between a frozen Language Model (LM) and a Re- trieval Model (RM), providing the model with more informa- tive context to enhance generation quality. PKG equips LLMs with a knowledge-guided module that allows access to rele- vant knowledge without altering the parameters of LLMs, en- abling the model to perform more sophisticated tasks. Addi- tionally, CREA-ICL[Li et al., 2023b] leverages synchronous retrieval of cross-lingual knowledge to assist in acquiring ad- ditional information, while RECITE forms context by sam- pling one or more paragraphs from LLMs. During the inference phase, optimizing the process of RAG can benefit adaptation to more challenging tasks. For ex- ample, ITRG[Feng et al., 2023a] enhances adaptability for tasks requiring multiple-step reasoning by iteratively retriev- ing and searching for the correct reasoning path. ITER- RETGEN[Shao et al., 2023] employs an iterative approach to coalesce retrieval and generation, achieving an alternating process of \u201dretrieval-enhanced generation\u201d and \u201dgeneration- enhanced retrieval.\u201d On the other hand, IRCOT[Trivedi et al., 2022] merges the concepts of RAG and CoT[Wei et al., 2022], employing al- ternate CoT-guided retrievals and using retrieval results to improve CoT. This method significantly improves the perfor- mance of GPT-3 across various QA tasks, highlighting the potential advantages of integrating retrieval and generation. In summary, inference-stage enhancement methods offer the advantages of being lightweight, cost-effective, requir- ing no additional training, and utilizing powerful pre-trained models. The main strength lies in freezing the parameters of the LLMs during fine-tuning, focusing on providing con- text that better suits the requirements, with the characteristics of being fast and low-cost. However, this approach also has some limitations, including the need for additional data pro- cessing and process optimization, while being constrained by the foundation model\u2019s capabilities. Typically, this method is often combined with process optimization techniques such as step-wise reasoning , iterative reasoning, and adaptive re- trieval to better meet the requirements of different tasks.",
    "raw_text": "Inference Stage The integration of RAG methods with LLM has become a prevalent research direction in the inference phase. Notably, the research paradigm of Naive RAG relies on incorporating retrieval content during the inference stage. To overcome the limitations of Naive RAG, researchers have introduced richer context in the RAG during the in- ference phase. The DSP[Khattab et al., 2022] framework re- lies on a complex pipeline that involves passing natural lan- guage text between a frozen Language Model (LM) and a Re- trieval Model (RM), providing the model with more informa- tive context to enhance generation quality. PKG equips LLMs with a knowledge-guided module that allows access to rele- vant knowledge without altering the parameters of LLMs, en- abling the model to perform more sophisticated tasks. Addi- tionally, CREA-ICL[Li et al., 2023b] leverages synchronous retrieval of cross-lingual knowledge to assist in acquiring ad- ditional information, while RECITE forms context by sam- pling one or more paragraphs from LLMs. During the inference phase, optimizing the process of RAG can benefit adaptation to more challenging tasks. For ex- ample, ITRG[Feng et al., 2023a] enhances adaptability for tasks requiring multiple-step reasoning by iteratively retriev- ing and searching for the correct reasoning path. ITER- RETGEN[Shao et al., 2023] employs an iterative approach to coalesce retrieval and generation, achieving an alternating process of \u201dretrieval-enhanced generation\u201d and \u201dgeneration- enhanced retrieval.\u201d On the other hand, IRCOT[Trivedi et al., 2022] merges the concepts of RAG and CoT[Wei et al., 2022], employing al- ternate CoT-guided retrievals and using retrieval results to improve CoT. This method significantly improves the perfor- mance of GPT-3 across various QA tasks, highlighting the potential advantages of integrating retrieval and generation. In summary, inference-stage enhancement methods offer the advantages of being lightweight, cost-effective, requir- ing no additional training, and utilizing powerful pre-trained models. The main strength lies in freezing the parameters of the LLMs during fine-tuning, focusing on providing con- text that better suits the requirements, with the characteristics of being fast and low-cost. However, this approach also has some limitations, including the need for additional data pro- cessing and process optimization, while being constrained by the foundation model\u2019s capabilities. Typically, this method is often combined with process optimization techniques such as step-wise reasoning , iterative reasoning, and adaptive re- trieval to better meet the requirements of different tasks."
  },
  {
    "chunk_id": "ef63f054-91da-4e0f-b57f-40726f261658",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 16,
    "retrieval_text": "6.2 Augmentation Data Source Data source is crucial factors for RAG effectiveness. Vari- ous data sources offer distinct granularities and dimensions of knowledge, requiring different processing methods. They primarily fall into three categories: unstructured data, struc- tured data, and content generated by LLMs. Augmented with Unstructured Data Unstructured data mainly encompasses textual data , typi- cally derived from pure text corpora. Additionally, other text data can serve as retrieval sources, such as Prompt data used for large model fine-tuning[Cheng et al., 2023a] and cross- language data[Li et al., 2023b]. In terms of text granularity, beyond the common chunks (including sentences), the retrieval unit can be to- kens (e.g., kNN-LM[Khandelwal et al., 2019]), phrases (e.g., NPM[Lee et al., 2020], COG[Vaze et al., 2021]), and docu- ment paragraphs. Finer-grained retrieval units can often bet- ter handle rare patterns and out-of-domain scenarios but come with an increase in retrieval costs. At the word level, FLARE employs an active retrieval strat- egy, conducting retrieval only when the LM generates low- probability words. The method involves generating a tempo- rary next sentence for retrieval of relevant documents, then re-generating the next sentence under the condition of the re- trieved documents to predict subsequent sentences. At the chunk level, RETRO uses the previous chunk to re- trieve the nearest neighboring chunk and integrates this infor- mation with the contextual information of the previous chunk to guide the generation of the next chunk. RETRO achieves this by retrieving the nearest neighboring block N(Ci\u22121) from the retrieval database, then fusing the contextual in- formation of the preceding blocks (C1,...,Ci\u22121) and the retrieval information of N(Ci\u22121) through cross-attention to guide the generation of the next block Ci. To maintain causal- ity, the autoregressive generation of the i-th block Ci can only use the nearest neighbor of the previous block N(Ci\u22121) and not N(Ci).",
    "raw_text": "6.2 Augmentation Data Source Data source is crucial factors for RAG effectiveness. Vari- ous data sources offer distinct granularities and dimensions of knowledge, requiring different processing methods. They primarily fall into three categories: unstructured data, struc- tured data, and content generated by LLMs. Augmented with Unstructured Data Unstructured data mainly encompasses textual data , typi- cally derived from pure text corpora. Additionally, other text data can serve as retrieval sources, such as Prompt data used for large model fine-tuning[Cheng et al., 2023a] and cross- language data[Li et al., 2023b]. In terms of text granularity, beyond the common chunks (including sentences), the retrieval unit can be to- kens (e.g., kNN-LM[Khandelwal et al., 2019]), phrases (e.g., NPM[Lee et al., 2020], COG[Vaze et al., 2021]), and docu- ment paragraphs. Finer-grained retrieval units can often bet- ter handle rare patterns and out-of-domain scenarios but come with an increase in retrieval costs. At the word level, FLARE employs an active retrieval strat- egy, conducting retrieval only when the LM generates low- probability words. The method involves generating a tempo- rary next sentence for retrieval of relevant documents, then re-generating the next sentence under the condition of the re- trieved documents to predict subsequent sentences. At the chunk level, RETRO uses the previous chunk to re- trieve the nearest neighboring chunk and integrates this infor- mation with the contextual information of the previous chunk to guide the generation of the next chunk. RETRO achieves this by retrieving the nearest neighboring block N(Ci\u22121) from the retrieval database, then fusing the contextual in- formation of the preceding blocks (C1,...,Ci\u22121) and the retrieval information of N(Ci\u22121) through cross-attention to guide the generation of the next block Ci. To maintain causal- ity, the autoregressive generation of the i-th block Ci can only use the nearest neighbor of the previous block N(Ci\u22121) and not N(Ci)."
  },
  {
    "chunk_id": "d1d38253-9379-42dd-a5c5-a90095d5e3b5",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 17,
    "retrieval_text": "Augmented with Structured Data Structured data sources like Knowledge Graphs (KG) are gradually integrated into the paradigm of RAG. Verified KGs can offer higher-quality context, reducing the likelihood of model hallucinations. RET-LLM [Modarressi et al., 2023] constructs a per- sonalized knowledge graph memory by extracting relation triples from past dialogues for future use. SUGRE[Kang et al., 2023] embeds relevant subgraphs retrieved from the knowledge graph using Graph Neural Networks (GNN) to prevent the model from generating contextually irrelevant replies. SUGRE[Kang et al., 2023] employs a graph encoding method that reflects the graph structure into PTMs\u2019 representation space and utilizes a multi-modal contrastive learning objective between graph- text modes to ensure consistency between retrieved facts and generated text. KnowledgeGPT[Wang et al., 2023c] generates search queries for Knowledge Bases (KB) in code format and includes predefined KB operation functions. Apart from retrieval, KnowledgeGPT also offers the ca- pability to store knowledge in a personalized knowledge base to meet individual user needs. These structured data sources provide RAG with richer knowledge and context, contributing to improved model performance. LLM Generated Content RAG Observing that the auxiliary information recalled by RAG is not always effective and may even have negative effects, some studies have expanded the paradigm of RAG by delving deeper into the internal knowledge of LLM. This approach utilizes the content generated by LLM itself for retrieval, aim- ing to enhance performance in downstream tasks. The follow- ing outlines notable studies within this category: SKR[Wang et al., 2023d] employs a labeled training set, categorizing questions that the model can directly answer as known and those requiring retrieval enhancement as un- known. The model is trained to discern whether a question is known, applying retrieval enhancement only to inputs identi- fied as unknown, while directly answering the rest. GenRead[Yu et al., 2022] substitutes the LLM generator for the retriever. Experimental results indicate that situations where the generated context document contains correct an- swers are more prevalent than those retrieved by Naive RAG. The generated answers also demonstrate superior quality. The authors attribute this to the alignment between the task of gen- erating document-level context and the pre-training objective of causal language modeling, allowing for better utilization of world knowledge stored in the model parameters. Selfmem[Cheng et al., 2023b] iteratively uses a retrieval- enhanced generator to create an unbounded memory pool. A memory selector is employed to choose an output as the mem- ory for subsequent generations. This output serves as the dual problem to the original question. By combining the original and dual problems, a retrieval-enhanced generative model can leverage its own output to enhance itself. These diverse approaches showcase innovative strategies in RAG retrieval enhancement, aiming to elevate model perfor- mance and effectiveness.",
    "raw_text": "Augmented with Structured Data Structured data sources like Knowledge Graphs (KG) are gradually integrated into the paradigm of RAG. Verified KGs can offer higher-quality context, reducing the likelihood of model hallucinations. RET-LLM [Modarressi et al., 2023] constructs a per- sonalized knowledge graph memory by extracting relation triples from past dialogues for future use. SUGRE[Kang et al., 2023] embeds relevant subgraphs retrieved from the knowledge graph using Graph Neural Networks (GNN) to prevent the model from generating contextually irrelevant replies. SUGRE[Kang et al., 2023] employs a graph encoding method that reflects the graph structure into PTMs\u2019 representation space and utilizes a multi-modal contrastive learning objective between graph- text modes to ensure consistency between retrieved facts and generated text. KnowledgeGPT[Wang et al., 2023c] generates search queries for Knowledge Bases (KB) in code format and includes predefined KB operation functions. Apart from retrieval, KnowledgeGPT also offers the ca- pability to store knowledge in a personalized knowledge base to meet individual user needs. These structured data sources provide RAG with richer knowledge and context, contributing to improved model performance. LLM Generated Content RAG Observing that the auxiliary information recalled by RAG is not always effective and may even have negative effects, some studies have expanded the paradigm of RAG by delving deeper into the internal knowledge of LLM. This approach utilizes the content generated by LLM itself for retrieval, aim- ing to enhance performance in downstream tasks. The follow- ing outlines notable studies within this category: SKR[Wang et al., 2023d] employs a labeled training set, categorizing questions that the model can directly answer as known and those requiring retrieval enhancement as un- known. The model is trained to discern whether a question is known, applying retrieval enhancement only to inputs identi- fied as unknown, while directly answering the rest. GenRead[Yu et al., 2022] substitutes the LLM generator for the retriever. Experimental results indicate that situations where the generated context document contains correct an- swers are more prevalent than those retrieved by Naive RAG. The generated answers also demonstrate superior quality. The authors attribute this to the alignment between the task of gen- erating document-level context and the pre-training objective of causal language modeling, allowing for better utilization of world knowledge stored in the model parameters. Selfmem[Cheng et al., 2023b] iteratively uses a retrieval- enhanced generator to create an unbounded memory pool. A memory selector is employed to choose an output as the mem- ory for subsequent generations. This output serves as the dual problem to the original question. By combining the original and dual problems, a retrieval-enhanced generative model can leverage its own output to enhance itself. These diverse approaches showcase innovative strategies in RAG retrieval enhancement, aiming to elevate model perfor- mance and effectiveness."
  },
  {
    "chunk_id": "75f6aeb1-7ebe-4049-b55d-0c3530b7bb50",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 17,
    "retrieval_text": "6.3 Augmentation Process Most RAG research typically only performs a single retrieval and generation process. However, single retrievals may con- tain redundant information, leading to a \u201dlost in the mid- dle\u201d phenomenon[Liu et al., 2023]. This redundant informa- tion can obscure key information or contain information con- trary to the real answer, negatively impacting the generation effect[Yoran et al., 2023]. Additionally, the information ob- tained from a single retrieval is limited in problems requiring multi-step reasoning. Current methods to optimize the retrieval process mainly include iterative retrieval and adaptive retrieval. These allow the model to iterate multiple times during the retrieval process or adaptively adjust the retrieval process to better accommo- date different tasks and scenarios. Iterative Retrieval Regularly collecting documents based on the original query and generated text can provide additional materials for LLMs[Borgeaud et al., 2022, Arora et al., 2023]. Providing additional references in multiple iterative retrievals has im- proved the robustness of subsequent answer generation. However, this method may be semantically discontinuous and potentially lead to the collection of noisy and useless infor- mation, as it primarily relies on a sequence of n tokens to separate the generated and retrieved documents. Recursive retrieval and multi-hop retrieval are used for spe- cific data scenarios. Recursive retrieval can first process data through a structured index, then retrieve it level by level. When retrieving hierarchically rich documents, a summary can be made for each section in an entire document or long PDF. A retrieval is then performed based on the summary. After determining the document, a second retrieval is con- ducted for the internal chunks, thus realizing recursive re- trieval. Multi-hop retrieval is often used to further mine in- formation in graph-structured data sources[Li et al., 2023c]. Some methods iterate the steps of retrieval and generation. ITER-RETGEN [Shao et al., 2023] collaboratively utilizes \u201dretrieval-enhanced generation\u201d and \u201dgeneration-enhanced retrieval\u201d for tasks requiring reproduction of information. That is, the model uses the content needed to complete the task to respond to the input task, and these target contents serve as the information context for retrieving more relevant knowledge. This helps to generate better responses in another iteration. IRCoT[Trivedi et al., 2022] also explores retrieving docu- ments for each generated sentence, introducing retrieval at every step of the thought chain. It uses CoT to guide the re- trieval and uses the retrieval results to improve CoT, ensuring semantic completeness.",
    "raw_text": "6.3 Augmentation Process Most RAG research typically only performs a single retrieval and generation process. However, single retrievals may con- tain redundant information, leading to a \u201dlost in the mid- dle\u201d phenomenon[Liu et al., 2023]. This redundant informa- tion can obscure key information or contain information con- trary to the real answer, negatively impacting the generation effect[Yoran et al., 2023]. Additionally, the information ob- tained from a single retrieval is limited in problems requiring multi-step reasoning. Current methods to optimize the retrieval process mainly include iterative retrieval and adaptive retrieval. These allow the model to iterate multiple times during the retrieval process or adaptively adjust the retrieval process to better accommo- date different tasks and scenarios. Iterative Retrieval Regularly collecting documents based on the original query and generated text can provide additional materials for LLMs[Borgeaud et al., 2022, Arora et al., 2023]. Providing additional references in multiple iterative retrievals has im- proved the robustness of subsequent answer generation. However, this method may be semantically discontinuous and potentially lead to the collection of noisy and useless infor- mation, as it primarily relies on a sequence of n tokens to separate the generated and retrieved documents. Recursive retrieval and multi-hop retrieval are used for spe- cific data scenarios. Recursive retrieval can first process data through a structured index, then retrieve it level by level. When retrieving hierarchically rich documents, a summary can be made for each section in an entire document or long PDF. A retrieval is then performed based on the summary. After determining the document, a second retrieval is con- ducted for the internal chunks, thus realizing recursive re- trieval. Multi-hop retrieval is often used to further mine in- formation in graph-structured data sources[Li et al., 2023c]. Some methods iterate the steps of retrieval and generation. ITER-RETGEN [Shao et al., 2023] collaboratively utilizes \u201dretrieval-enhanced generation\u201d and \u201dgeneration-enhanced retrieval\u201d for tasks requiring reproduction of information. That is, the model uses the content needed to complete the task to respond to the input task, and these target contents serve as the information context for retrieving more relevant knowledge. This helps to generate better responses in another iteration. IRCoT[Trivedi et al., 2022] also explores retrieving docu- ments for each generated sentence, introducing retrieval at every step of the thought chain. It uses CoT to guide the re- trieval and uses the retrieval results to improve CoT, ensuring semantic completeness."
  },
  {
    "chunk_id": "b894873c-ab85-4c1c-94d3-78ea8fac6bae",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 17,
    "retrieval_text": "Adaptive Retrieval Indeed, the RAG methods described in the previous two sections follow a passive approach where retrieval is prior- itized. This method, which involves querying related doc- uments and inputting into a LLM based on context, may lead to efficiency issues. Adaptive retrieval methods such as those introduced by Flare[Jiang et al., 2023b] and Self- RAG[Asai et al., 2023b], optimize the RAG retrieval process, enabling the LLM to actively judge the timing and content of retrieval. This helps to improve the efficiency and relevance of the information retrieved. In fact, the way in which LLM actively uses tools and makes judgments is not originated from RAG but has been widely used in the agents of large models[Yang et al., 2023c, Schick et al., 2023, Zhang, 2023]. The retrieval steps of Graph-Toolformer[Zhang, 2023] are roughly divided into: LLMs actively use the retriever, Self-Ask and DSP[Khattab et al., 2022] try to use few-shot prompts to trig- ger LLM search queries. When LLMs think it is necessary, they can decide to search for a relevant query to collect the necessary materials, similar to the tool call of the agent. WebGPT[Nakano et al., 2021] employs a reinforcement learning framework to automatically train the GPT-3 model to use a search engine for text generation. It uses special to- kens to perform actions, including querying on a search en- gine, scrolling rankings, and citing references. This allows GPT-3 to leverage a search engine for text generation. Flare[Jiang et al., 2023b], on the other hand, automates the timing of retrieval and addresses the cost of periodic docu- ment retrieval based on the probability of the generated text. It uses probability as an indicator of LLMs\u2019 confidence during the generation process. When the probability of a term falls below a predefined threshold, the information retrieval sys- tem would retrieve references and removes terms with lower probabilities. This approach is designed to handle situations where LLMs might need additional knowledge. Self-RAG[Asai et al., 2023b] introduces an important in- novation called Reflection tokens. These special tokens are generated to review the output and come in two types: Re- trieve and Critic. The model can autonomously decide when to retrieve paragraphs or use a set threshold to trigger re- trieval. When retrieval is needed, the generator processes multiple paragraphs simultaneously, performing fragment- level beam search to obtain the best sequence. The scores for each subdivision are updated using Critic scores, and these weights can be adjusted during the inference process to cus- tomize the model\u2019s behavior. The Self-RAG framework also allows the LLM to autonomously determine whether recall is necessary, avoiding training additional classifiers or rely- ing on NLI models. This enhances the model\u2019s ability to au- tonomously judge inputs and generate accurate answers.",
    "raw_text": "Adaptive Retrieval Indeed, the RAG methods described in the previous two sections follow a passive approach where retrieval is prior- itized. This method, which involves querying related doc- uments and inputting into a LLM based on context, may lead to efficiency issues. Adaptive retrieval methods such as those introduced by Flare[Jiang et al., 2023b] and Self- RAG[Asai et al., 2023b], optimize the RAG retrieval process, enabling the LLM to actively judge the timing and content of retrieval. This helps to improve the efficiency and relevance of the information retrieved. In fact, the way in which LLM actively uses tools and makes judgments is not originated from RAG but has been widely used in the agents of large models[Yang et al., 2023c, Schick et al., 2023, Zhang, 2023]. The retrieval steps of Graph-Toolformer[Zhang, 2023] are roughly divided into: LLMs actively use the retriever, Self-Ask and DSP[Khattab et al., 2022] try to use few-shot prompts to trig- ger LLM search queries. When LLMs think it is necessary, they can decide to search for a relevant query to collect the necessary materials, similar to the tool call of the agent. WebGPT[Nakano et al., 2021] employs a reinforcement learning framework to automatically train the GPT-3 model to use a search engine for text generation. It uses special to- kens to perform actions, including querying on a search en- gine, scrolling rankings, and citing references. This allows GPT-3 to leverage a search engine for text generation. Flare[Jiang et al., 2023b], on the other hand, automates the timing of retrieval and addresses the cost of periodic docu- ment retrieval based on the probability of the generated text. It uses probability as an indicator of LLMs\u2019 confidence during the generation process. When the probability of a term falls below a predefined threshold, the information retrieval sys- tem would retrieve references and removes terms with lower probabilities. This approach is designed to handle situations where LLMs might need additional knowledge. Self-RAG[Asai et al., 2023b] introduces an important in- novation called Reflection tokens. These special tokens are generated to review the output and come in two types: Re- trieve and Critic. The model can autonomously decide when to retrieve paragraphs or use a set threshold to trigger re- trieval. When retrieval is needed, the generator processes multiple paragraphs simultaneously, performing fragment- level beam search to obtain the best sequence. The scores for each subdivision are updated using Critic scores, and these weights can be adjusted during the inference process to cus- tomize the model\u2019s behavior. The Self-RAG framework also allows the LLM to autonomously determine whether recall is necessary, avoiding training additional classifiers or rely- ing on NLI models. This enhances the model\u2019s ability to au- tonomously judge inputs and generate accurate answers."
  },
  {
    "chunk_id": "ed64b9f6-a08c-4408-81b3-3044ecdb207b",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 18,
    "retrieval_text": "7 RAG Evaluation In exploring the development and optimization of RAG, ef- fectively evaluating its performance has emerged as a central issue. This chapter primarily discusses the methods of eval- uation, key metrics for RAG, the abilities it should possess, and some mainstream evaluation frameworks. 7.1 Evaluation Methods There are primarily two approaches to evaluating the ef- fectiveness of RAG: independent evaluation and end-to-end evaluation[Liu, 2023]. Independent Evaluation Independent evaluation includes assessing the retrieval mod- ule and the generation (read/synthesis) module. 1. Retrieval Module A suite of metrics that measure the effectiveness of sys- tems (like search engines, recommendation systems, or information retrieval systems) in ranking items accord- ing to queries or tasks are commonly used to evaluate the performance of the RAG retrieval module. Exam- ples include Hit Rate, MRR, NDCG, Precision, etc. 2. Generation Module The generation module here refers to the enhanced or synthesized input formed by supplementing the retrieved documents into the query, distinct from the final an- swer/response generation, which is typically evaluated end-to-end. The evaluation metrics for the generation module mainly focus on context relevance, measuring the relatedness of retrieved documents to the query ques- tion. End-to-End Evaluation End-to-end evaluation assesses the final response gener- ated by the RAG model for a given input, involving the relevance and alignment of the model-generated answers with the input query. From the perspective of content generation goals, evaluation can be divided into unlabeled and labeled content. Unlabeled content evaluation met- rics include answer fidelity, answer relevance, harmless- ness, etc., while labeled content evaluation metrics in- clude Accuracy and EM. Additionally, from the perspec- tive of evaluation methods, end-to-end evaluation can be di- vided into manual evaluation and automated evaluation us- ing LLMs. The above summarizes the general case of end- to-end evaluation for RAG. Furthermore, specific evalua- tion metrics are adopted based on the application of RAG in particular domains, such as EM for question-answering tasks[Borgeaud et al., 2022, Izacard et al., 2022], UniEval and E-F1 for summarization tasks[Jiang et al., 2023b], and BLEU for machine translation[Zhong et al., 2022]. These metrics help in understanding the performance of RAG in var- ious specific application scenarios.",
    "raw_text": "7 RAG Evaluation In exploring the development and optimization of RAG, ef- fectively evaluating its performance has emerged as a central issue. This chapter primarily discusses the methods of eval- uation, key metrics for RAG, the abilities it should possess, and some mainstream evaluation frameworks. 7.1 Evaluation Methods There are primarily two approaches to evaluating the ef- fectiveness of RAG: independent evaluation and end-to-end evaluation[Liu, 2023]. Independent Evaluation Independent evaluation includes assessing the retrieval mod- ule and the generation (read/synthesis) module. 1. Retrieval Module A suite of metrics that measure the effectiveness of sys- tems (like search engines, recommendation systems, or information retrieval systems) in ranking items accord- ing to queries or tasks are commonly used to evaluate the performance of the RAG retrieval module. Exam- ples include Hit Rate, MRR, NDCG, Precision, etc. 2. Generation Module The generation module here refers to the enhanced or synthesized input formed by supplementing the retrieved documents into the query, distinct from the final an- swer/response generation, which is typically evaluated end-to-end. The evaluation metrics for the generation module mainly focus on context relevance, measuring the relatedness of retrieved documents to the query ques- tion. End-to-End Evaluation End-to-end evaluation assesses the final response gener- ated by the RAG model for a given input, involving the relevance and alignment of the model-generated answers with the input query. From the perspective of content generation goals, evaluation can be divided into unlabeled and labeled content. Unlabeled content evaluation met- rics include answer fidelity, answer relevance, harmless- ness, etc., while labeled content evaluation metrics in- clude Accuracy and EM. Additionally, from the perspec- tive of evaluation methods, end-to-end evaluation can be di- vided into manual evaluation and automated evaluation us- ing LLMs. The above summarizes the general case of end- to-end evaluation for RAG. Furthermore, specific evalua- tion metrics are adopted based on the application of RAG in particular domains, such as EM for question-answering tasks[Borgeaud et al., 2022, Izacard et al., 2022], UniEval and E-F1 for summarization tasks[Jiang et al., 2023b], and BLEU for machine translation[Zhong et al., 2022]. These metrics help in understanding the performance of RAG in var- ious specific application scenarios."
  },
  {
    "chunk_id": "f8037a86-4f58-4b9e-9775-88d082da87eb",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 18,
    "retrieval_text": "7.2 Key Metrics and Abilities Existing research often lacks rigorous evaluation of the im- pact of retrieval-augmented generation on different LLMs. In most cases, the evaluaion of RAG\u2019s application in vari- ous downstream tasks and with different retrievers may yield divergent results. However, some academic and engineering practices have focused on general evaluation metrics for RAG and the abilities required for its effective use. This section primarily introduces key metrics for evaluating RAG\u2019s effec- tiveness and essential abilities for assessing its performance. Key Metrics Recent OpenAI report[Jarvis and Allard, 2023] have mentioned various techniques for optimizing large language models (LLMs), including RAG and its evaluation metrics. Additionally, the latest evalu- ation frameworks like RAGAS[Es et al., 2023] and ARES[Saad-Falcon et al., 2023] also involve RAG eval- uation metrics. Summarizing these works, three core metrics are primarily focused on: Faithfulness of the answer, Answer Relevance, and Context Relevance. 1. Faithfulness This metric emphasizes that the answers generated by the model must remain true to the given context, ensur- ing that the answers are consistent with the context infor- mation and do not deviate or contradict it. This aspect of evaluation is vital for addressing illusions in large mod- els. 2. Answer Relevance This metric stresses that the generated answers need to be directly related to the posed question. 3. Context Relevance This metric demands that the retrieved contextual infor- mation be as accurate and targeted as possible, avoid- ing irrelevant content. After all, processing long texts is costly for LLMs, and too much irrelevant information can reduce the efficiency of LLMs in utilizing context. The OpenAI report also mentioned \u201dContext Recall\u201d as a supplementary metric, measuring the model\u2019s abil- ity to retrieve all relevant information needed to an- swer a question. This metric reflects the search opti- mization level of the RAG retrieval module. A low re- call rate indicates a potential need for optimization of the search functionality, such as introducing re-ranking mechanisms or fine-tuning embeddings, to ensure more relevant content retrieval.",
    "raw_text": "7.2 Key Metrics and Abilities Existing research often lacks rigorous evaluation of the im- pact of retrieval-augmented generation on different LLMs. In most cases, the evaluaion of RAG\u2019s application in vari- ous downstream tasks and with different retrievers may yield divergent results. However, some academic and engineering practices have focused on general evaluation metrics for RAG and the abilities required for its effective use. This section primarily introduces key metrics for evaluating RAG\u2019s effec- tiveness and essential abilities for assessing its performance. Key Metrics Recent OpenAI report[Jarvis and Allard, 2023] have mentioned various techniques for optimizing large language models (LLMs), including RAG and its evaluation metrics. Additionally, the latest evalu- ation frameworks like RAGAS[Es et al., 2023] and ARES[Saad-Falcon et al., 2023] also involve RAG eval- uation metrics. Summarizing these works, three core metrics are primarily focused on: Faithfulness of the answer, Answer Relevance, and Context Relevance. 1. Faithfulness This metric emphasizes that the answers generated by the model must remain true to the given context, ensur- ing that the answers are consistent with the context infor- mation and do not deviate or contradict it. This aspect of evaluation is vital for addressing illusions in large mod- els. 2. Answer Relevance This metric stresses that the generated answers need to be directly related to the posed question. 3. Context Relevance This metric demands that the retrieved contextual infor- mation be as accurate and targeted as possible, avoid- ing irrelevant content. After all, processing long texts is costly for LLMs, and too much irrelevant information can reduce the efficiency of LLMs in utilizing context. The OpenAI report also mentioned \u201dContext Recall\u201d as a supplementary metric, measuring the model\u2019s abil- ity to retrieve all relevant information needed to an- swer a question. This metric reflects the search opti- mization level of the RAG retrieval module. A low re- call rate indicates a potential need for optimization of the search functionality, such as introducing re-ranking mechanisms or fine-tuning embeddings, to ensure more relevant content retrieval."
  },
  {
    "chunk_id": "71bafd85-d171-4492-86ed-9528d13928ab",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 19,
    "retrieval_text": "Key abilities The work of RGB[Chen et al., 2023b] analyzed the perfor- mance of different large language models in terms of four basic abilities required for RAG, including Noise Robust- ness, Negative Rejection, Information Integration, and Coun- terfactual Robustness, establishing a benchmark for retrieval- augmented generation.RGB focuses on the following four abilities: 1. Noise Robustness This capability measures the model\u2019s efficiency in han- dling noisy documents, which are those related to the question but do not contain useful information. 2. Negative Rejection When documents retrieved by the model lack the knowl- edge required to answer a question, the model should correctly refuse to respond. In the test setting for neg- ative rejection, external documents contain only noise. Ideally, the LLM should issue a \u201dlack of information\u201d or similar refusal signal. 3. Information Integration This ability assesses whether the model can integrate information from multiple documents to answer more complex questions. 4. Counterfactual Robustness This test aims to evaluate whether the model can iden- tify and deal with known erroneous information in doc- uments when receiving instructions about potential risks in retrieved information. Counterfactual robustness tests include questions that the LLM can answer directly, but the related external documents contain factual errors. 7.3 Evaluation Frameworks Recently, the LLM community has been exploring the use of \u201dLLMs as judge\u201d for automatic assessment, with many utilizing powerful LLMs (such as GPT-4) to evaluate their own LLM applications outputs. Practices by Databricks us- ing GPT-3.5 and GPT-4 as LLM judges to assess their chatbot applications suggest that using LLMs as automatic evaluation tools is effective[Leng et al., 2023]. They believe this method can also efficiently and cost-effectively evaluate RAG-based applications. In the field of RAG evaluation frameworks, RAGAS and ARES are relatively new. The core focus of these evaluations is on three main metrics: Faithfulness of the answer, answer relevance, and context relevance. Additionally, TruLens, an open-source library proposed by the industry, also offers a similar evaluation mode. These frameworks all use LLMs as judges for evaluation. As TruLens is similar to RAGAS, this chapter will specifically introduce RAGAS and ARES.",
    "raw_text": "Key abilities The work of RGB[Chen et al., 2023b] analyzed the perfor- mance of different large language models in terms of four basic abilities required for RAG, including Noise Robust- ness, Negative Rejection, Information Integration, and Coun- terfactual Robustness, establishing a benchmark for retrieval- augmented generation.RGB focuses on the following four abilities: 1. Noise Robustness This capability measures the model\u2019s efficiency in han- dling noisy documents, which are those related to the question but do not contain useful information. 2. Negative Rejection When documents retrieved by the model lack the knowl- edge required to answer a question, the model should correctly refuse to respond. In the test setting for neg- ative rejection, external documents contain only noise. Ideally, the LLM should issue a \u201dlack of information\u201d or similar refusal signal. 3. Information Integration This ability assesses whether the model can integrate information from multiple documents to answer more complex questions. 4. Counterfactual Robustness This test aims to evaluate whether the model can iden- tify and deal with known erroneous information in doc- uments when receiving instructions about potential risks in retrieved information. Counterfactual robustness tests include questions that the LLM can answer directly, but the related external documents contain factual errors. 7.3 Evaluation Frameworks Recently, the LLM community has been exploring the use of \u201dLLMs as judge\u201d for automatic assessment, with many utilizing powerful LLMs (such as GPT-4) to evaluate their own LLM applications outputs. Practices by Databricks us- ing GPT-3.5 and GPT-4 as LLM judges to assess their chatbot applications suggest that using LLMs as automatic evaluation tools is effective[Leng et al., 2023]. They believe this method can also efficiently and cost-effectively evaluate RAG-based applications. In the field of RAG evaluation frameworks, RAGAS and ARES are relatively new. The core focus of these evaluations is on three main metrics: Faithfulness of the answer, answer relevance, and context relevance. Additionally, TruLens, an open-source library proposed by the industry, also offers a similar evaluation mode. These frameworks all use LLMs as judges for evaluation. As TruLens is similar to RAGAS, this chapter will specifically introduce RAGAS and ARES."
  },
  {
    "chunk_id": "95e5f81a-242e-43aa-8506-fad6208086ef",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 19,
    "retrieval_text": "RAGAS This framework considers the retrieval system\u2019s ability to identify relevant and key context paragraphs, the LLM\u2019s abil- ity to use these paragraphs faithfully, and the quality of the generation itself. RAGAS is an evaluation framework based on simple handwritten prompts, using these prompts to measure the three aspects of quality - answer faithfulness, answer relevance, and context relevance - in a fully auto- mated manner. In the implementation and experimentation of this framework, all prompts are evaluated using the gpt- 3.5-turbo-16k model, which is available through the OpenAI API[Es et al., 2023]. Algorithm Principles 1. Assessing Answer Faithfulness: Decompose the answer into individual statements using an LLM and verify whether each statement is consistent with the context. Ultimately, a \u201dFaithfulness Score\u201d is calculated by com- paring the number of supported statements to the total number of statements. 2. Assessing Answer Relevance: Generate potential ques- tions using an LLM and calculate the similarity between these questions and the original question. The Answer Relevance Score is derived by calculating the average similarity of all generated questions to the original ques- tion. 3. Assessing Context Relevance: Extract sentences directly relevant to the question using an LLM, and use the ratio of these sentences to the total number of sentences in the context as the Context Relevance Score. ARES ARES aims to automatically evaluate the performance of RAG systems in three aspects: Context Relevance, Answer Faithfulness, and Answer Relevance. These evaluation met- rics are similar to those in RAGAS. However, RAGAS, being a newer evaluation framework based on simple handwritten prompts, has limited adaptability to new RAG evaluation set- tings, which is one of the significances of the ARES work. Furthermore, as demonstrated in its assessments, ARES per- forms significantly lower than RAGAS. ARES reduces the cost of evaluation by using a small amount of manually annotated data and synthetic data, and utilizes Predictive-Driven Reasoning (PDR) to provide statistical confidence intervals, enhancing the accuracy of evaluation[Saad-Falcon et al., 2023].",
    "raw_text": "RAGAS This framework considers the retrieval system\u2019s ability to identify relevant and key context paragraphs, the LLM\u2019s abil- ity to use these paragraphs faithfully, and the quality of the generation itself. RAGAS is an evaluation framework based on simple handwritten prompts, using these prompts to measure the three aspects of quality - answer faithfulness, answer relevance, and context relevance - in a fully auto- mated manner. In the implementation and experimentation of this framework, all prompts are evaluated using the gpt- 3.5-turbo-16k model, which is available through the OpenAI API[Es et al., 2023]. Algorithm Principles 1. Assessing Answer Faithfulness: Decompose the answer into individual statements using an LLM and verify whether each statement is consistent with the context. Ultimately, a \u201dFaithfulness Score\u201d is calculated by com- paring the number of supported statements to the total number of statements. 2. Assessing Answer Relevance: Generate potential ques- tions using an LLM and calculate the similarity between these questions and the original question. The Answer Relevance Score is derived by calculating the average similarity of all generated questions to the original ques- tion. 3. Assessing Context Relevance: Extract sentences directly relevant to the question using an LLM, and use the ratio of these sentences to the total number of sentences in the context as the Context Relevance Score. ARES ARES aims to automatically evaluate the performance of RAG systems in three aspects: Context Relevance, Answer Faithfulness, and Answer Relevance. These evaluation met- rics are similar to those in RAGAS. However, RAGAS, being a newer evaluation framework based on simple handwritten prompts, has limited adaptability to new RAG evaluation set- tings, which is one of the significances of the ARES work. Furthermore, as demonstrated in its assessments, ARES per- forms significantly lower than RAGAS. ARES reduces the cost of evaluation by using a small amount of manually annotated data and synthetic data, and utilizes Predictive-Driven Reasoning (PDR) to provide statistical confidence intervals, enhancing the accuracy of evaluation[Saad-Falcon et al., 2023]."
  },
  {
    "chunk_id": "b8e9de0b-4dcb-464d-9219-ea6aaec0c26b",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 20,
    "retrieval_text": "Algorithm Principles 1. Generating Synthetic Dataset: ARES initially generates synthetic questions and answers from documents in the target corpus using a language model to create positive and negative samples. 2. Preparing LLM Judges: Next, ARES fine-tunes lightweight language models using the synthetic dataset to train them to evaluate Context Relevance, Answer Faithfulness, and Answer Relevance. 3. Ranking RAG Systems Using Confidence Intervals: Fi- nally, ARES applies these judge models to score RAG systems and combines them with a manually annotated validation set using the PPI method to generate confi- dence intervals, reliably estimating the performance of RAG systems. 8 Future Prospects In this chapter, we delve into three future prospects for RAG, namely vertical optimization, horizontal expansion and ecosystem of RAG. 8.1 Vertical Optimization of RAG Despite the rapid advancements in RAG technology over the past year, there are still several areas in its vertical domain that require further investigation. Firstly, the issue of long context in RAG is a significant challenge. As mentioned in the literature [Xu et al., 2023c], RAG\u2019s generation phase is constrained by the context win- dow of LLMs. If the window is too short, it may not contain enough relevant information; if it\u2019s too long, it might lead to information loss. Currently, expanding the context window of LLMs, even to the extent of limitless context, is a critical direction in LLM development. However, once the context window constraint is removed, how RAG should adapt re- mains a noteworthy question. Secondly, the robustness of RAG is another important re- search focus. If irrelevant noise appears during retrieval, or if the retrieved content contradicts facts, it can significantly impact RAG\u2019s effectiveness. This situation is figuratively referred to as \u201dopening a book to a poisonous mushroom\u201d. Therefore, enhancing the robustness of RAG has increasingly gained researchers\u2019 attention, as represented in studies such as [Yu et al., 2023a, Glass et al., 2021, Baek et al., 2023]. Thirdly, the issue of RAG and Fine-tuning\u2019s synergy is also a primary research point. Hybrid has gradually become one of the mainstream methods in RAG, exemplified by RA- DIT [Lin et al., 2023]. How to coordinate the relationship between the two to simultaneously obtain the advantages of parameterization and non-parameterization is a problem that needs addressing. Lastly, the engineering practice of RAG is a significant area of interest. The ease of implementation and align- ment with corporate engineering needs have contributed to RAG\u2019s rise. However, in engineering practice, questions like how to improve retrieval efficiency and document re- call rate in large-scale knowledge base scenarios, and how to ensure enterprise data security, such as preventing LLMs from being induced to disclose the source, metadata, or other information of documents, are crucial issues that need resolution[Alon et al., 2022].",
    "raw_text": "Algorithm Principles 1. Generating Synthetic Dataset: ARES initially generates synthetic questions and answers from documents in the target corpus using a language model to create positive and negative samples. 2. Preparing LLM Judges: Next, ARES fine-tunes lightweight language models using the synthetic dataset to train them to evaluate Context Relevance, Answer Faithfulness, and Answer Relevance. 3. Ranking RAG Systems Using Confidence Intervals: Fi- nally, ARES applies these judge models to score RAG systems and combines them with a manually annotated validation set using the PPI method to generate confi- dence intervals, reliably estimating the performance of RAG systems. 8 Future Prospects In this chapter, we delve into three future prospects for RAG, namely vertical optimization, horizontal expansion and ecosystem of RAG. 8.1 Vertical Optimization of RAG Despite the rapid advancements in RAG technology over the past year, there are still several areas in its vertical domain that require further investigation. Firstly, the issue of long context in RAG is a significant challenge. As mentioned in the literature [Xu et al., 2023c], RAG\u2019s generation phase is constrained by the context win- dow of LLMs. If the window is too short, it may not contain enough relevant information; if it\u2019s too long, it might lead to information loss. Currently, expanding the context window of LLMs, even to the extent of limitless context, is a critical direction in LLM development. However, once the context window constraint is removed, how RAG should adapt re- mains a noteworthy question. Secondly, the robustness of RAG is another important re- search focus. If irrelevant noise appears during retrieval, or if the retrieved content contradicts facts, it can significantly impact RAG\u2019s effectiveness. This situation is figuratively referred to as \u201dopening a book to a poisonous mushroom\u201d. Therefore, enhancing the robustness of RAG has increasingly gained researchers\u2019 attention, as represented in studies such as [Yu et al., 2023a, Glass et al., 2021, Baek et al., 2023]. Thirdly, the issue of RAG and Fine-tuning\u2019s synergy is also a primary research point. Hybrid has gradually become one of the mainstream methods in RAG, exemplified by RA- DIT [Lin et al., 2023]. How to coordinate the relationship between the two to simultaneously obtain the advantages of parameterization and non-parameterization is a problem that needs addressing. Lastly, the engineering practice of RAG is a significant area of interest. The ease of implementation and align- ment with corporate engineering needs have contributed to RAG\u2019s rise. However, in engineering practice, questions like how to improve retrieval efficiency and document re- call rate in large-scale knowledge base scenarios, and how to ensure enterprise data security, such as preventing LLMs from being induced to disclose the source, metadata, or other information of documents, are crucial issues that need resolution[Alon et al., 2022]."
  },
  {
    "chunk_id": "76b5d9e5-e37e-4c3b-b352-9849f566b850",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 20,
    "retrieval_text": "Horizontal expansion of RAG Research on RAG has rapidly expanded in the horizontal field. Starting from the initial text question answering do- main, RAG\u2019s ideas have gradually been applied to more modal data, such as images, code, structured knowledge, au- dio and video, and so on. There are already many works in this regard. In the image field, the propozhiyosal of BLIP- 2[Li et al., 2023a], which uses frozen image encoders and large-scale language models for visual language pre-training, has lowered the cost of model training. Addi- tionally, the model can generate image-to-text conversions from zero samples. In the field of text generation, the VBR[Zhu et al., 2022] method is used to generate images to guide the text generation of the language model, which has significant effects in open text generation tasks. In the code field, RBPS[Nashid et al., 2023] is used for small-scale learning related to code. By encoding or fre- quency analysis, similar code examples to the developers\u2019 tasks are automatically retrieved. This technique has proven its effectiveness in test assertion generation and program re- pair tasks. In the field of structured knowledge, methods like CoK[Li et al., 2023c] hints first retrieve facts related to the input question from the knowledge graph and then add these facts to the input in the form of hints. This method has per- formed well in knowledge graph question answering tasks. For the field of audio and video, the GSS[Zhao et al., 2022] method retrieves and concatenates audio clips from the spoken vocabulary bank, immediately transforming MT data into ST data. UEOP[Chan et al., 2023] introduces a new breakthrough in end-to-end automatic speech recognition by introducing external offline strate- gies for voice-to-text mapping. Audio embeddings and semantic text embeddings generated by text-to-speech methods can bias ASR through KNN-based attention fu- sion, effectively shortening domain adaptation time. The Vid2Seq[Yang et al., 2023a] architecture enhances the lan- guage model by introducing special time markings, enabling it to seamlessly predict event boundaries and text descriptions within the same output sequence.",
    "raw_text": "Horizontal expansion of RAG Research on RAG has rapidly expanded in the horizontal field. Starting from the initial text question answering do- main, RAG\u2019s ideas have gradually been applied to more modal data, such as images, code, structured knowledge, au- dio and video, and so on. There are already many works in this regard. In the image field, the propozhiyosal of BLIP- 2[Li et al., 2023a], which uses frozen image encoders and large-scale language models for visual language pre-training, has lowered the cost of model training. Addi- tionally, the model can generate image-to-text conversions from zero samples. In the field of text generation, the VBR[Zhu et al., 2022] method is used to generate images to guide the text generation of the language model, which has significant effects in open text generation tasks. In the code field, RBPS[Nashid et al., 2023] is used for small-scale learning related to code. By encoding or fre- quency analysis, similar code examples to the developers\u2019 tasks are automatically retrieved. This technique has proven its effectiveness in test assertion generation and program re- pair tasks. In the field of structured knowledge, methods like CoK[Li et al., 2023c] hints first retrieve facts related to the input question from the knowledge graph and then add these facts to the input in the form of hints. This method has per- formed well in knowledge graph question answering tasks. For the field of audio and video, the GSS[Zhao et al., 2022] method retrieves and concatenates audio clips from the spoken vocabulary bank, immediately transforming MT data into ST data. UEOP[Chan et al., 2023] introduces a new breakthrough in end-to-end automatic speech recognition by introducing external offline strate- gies for voice-to-text mapping. Audio embeddings and semantic text embeddings generated by text-to-speech methods can bias ASR through KNN-based attention fu- sion, effectively shortening domain adaptation time. The Vid2Seq[Yang et al., 2023a] architecture enhances the lan- guage model by introducing special time markings, enabling it to seamlessly predict event boundaries and text descriptions within the same output sequence."
  },
  {
    "chunk_id": "d2a0a99b-d61f-49b2-907a-ccd07135c38a",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 21,
    "retrieval_text": "8.2 Ecosystem of RAG Downstream Tasks and Evaluation By integrating relevant information from a broad knowledge base, RAG has demonstrated significant potential in enhanc- ing language models\u2019 ability to process complex queries and generate information-rich responses. Numerous studies have shown that RAG performs well in various downstream tasks, such as open-ended question answering and fact verification. RAG models not only improve the accuracy and relevance of information in downstream applications but also increase the diversity and depth of responses. Given the success of RAG, exploring the model\u2019s adapt- ability and universality in multi-domain applications will be part of future work. This includes its use in professional do- main knowledge question-answering, such as in medicine, law, and education. In the application of downstream tasks such as professional domain knowledge question-answering, RAG might offer lower training costs and better performance benefits than fine-tuning. Simultaneously, improving the evaluation system of RAG for assessing and optimizing its application in different down- stream tasks is crucial for the model\u2019s efficiency and bene- fits in specific tasks. This includes developing more accurate evaluation metrics and frameworks for different downstream tasks, such as context relevance, content creativity, and harm- lessness, among others. Furthermore, enhancing the interpretability of models through RAG, allowing users to better understand how and why the model makes specific responses, is also a meaning- ful task. Technical Stack In the ecosystem of RAG, the development of the related technical stack has played a driving role. For instance, LangChain and LLamaIndex have become widely known quickly with the popularity of ChatGPT. They both offer a rich set of RAG-related APIs, gradually becoming one of the indispensable technologies in the era of large models. Meanwhile, new types of technical stacks are constantly be- ing developed. Although they do not offer as many features as LangChain and LLamaIndex, they focus more on their unique characteristics. For example, Flowise AI6 emphasizes low-code, allowing users to implement various AI applica- tions represented by RAG without writing code, simply by dragging and dropping. Other emerging technologies include HayStack, Meltno, and Cohere Coral. In addition to AI-native frameworks, traditional software or cloud service providers have also expanded their service range. For instance, Verba7, provided by the vector database company Weaviate, focuses on personal assistants. Amazon offers its users the intelligent enterprise search service tool Kendra, based on RAG thinking. Users can search in different content repositories through built-in connectors. The development of the technical stack and RAG are mu- tually reinforcing. New technologies pose higher demands 6https://flowiseai.com 7https://github.com/weaviate/Verba on the existing technical stack, while the optimization of the technical stack\u2019s functions further promotes the development of RAG technology. Overall, the technical stack of RAG\u2019s toolchain has initially formed, and many enterprise-level ap- plications have gradually emerged, but an all-in-one platform still needs to be refined.",
    "raw_text": "8.2 Ecosystem of RAG Downstream Tasks and Evaluation By integrating relevant information from a broad knowledge base, RAG has demonstrated significant potential in enhanc- ing language models\u2019 ability to process complex queries and generate information-rich responses. Numerous studies have shown that RAG performs well in various downstream tasks, such as open-ended question answering and fact verification. RAG models not only improve the accuracy and relevance of information in downstream applications but also increase the diversity and depth of responses. Given the success of RAG, exploring the model\u2019s adapt- ability and universality in multi-domain applications will be part of future work. This includes its use in professional do- main knowledge question-answering, such as in medicine, law, and education. In the application of downstream tasks such as professional domain knowledge question-answering, RAG might offer lower training costs and better performance benefits than fine-tuning. Simultaneously, improving the evaluation system of RAG for assessing and optimizing its application in different down- stream tasks is crucial for the model\u2019s efficiency and bene- fits in specific tasks. This includes developing more accurate evaluation metrics and frameworks for different downstream tasks, such as context relevance, content creativity, and harm- lessness, among others. Furthermore, enhancing the interpretability of models through RAG, allowing users to better understand how and why the model makes specific responses, is also a meaning- ful task. Technical Stack In the ecosystem of RAG, the development of the related technical stack has played a driving role. For instance, LangChain and LLamaIndex have become widely known quickly with the popularity of ChatGPT. They both offer a rich set of RAG-related APIs, gradually becoming one of the indispensable technologies in the era of large models. Meanwhile, new types of technical stacks are constantly be- ing developed. Although they do not offer as many features as LangChain and LLamaIndex, they focus more on their unique characteristics. For example, Flowise AI6 emphasizes low-code, allowing users to implement various AI applica- tions represented by RAG without writing code, simply by dragging and dropping. Other emerging technologies include HayStack, Meltno, and Cohere Coral. In addition to AI-native frameworks, traditional software or cloud service providers have also expanded their service range. For instance, Verba7, provided by the vector database company Weaviate, focuses on personal assistants. Amazon offers its users the intelligent enterprise search service tool Kendra, based on RAG thinking. Users can search in different content repositories through built-in connectors. The development of the technical stack and RAG are mu- tually reinforcing. New technologies pose higher demands 6https://flowiseai.com 7https://github.com/weaviate/Verba on the existing technical stack, while the optimization of the technical stack\u2019s functions further promotes the development of RAG technology. Overall, the technical stack of RAG\u2019s toolchain has initially formed, and many enterprise-level ap- plications have gradually emerged, but an all-in-one platform still needs to be refined."
  },
  {
    "chunk_id": "c1bb30e2-04f7-4379-9de3-b2f3a99134ef",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 21,
    "retrieval_text": "9 Conclusion This paper thoroughly explores Retrieval-Augmented Gener- ation (RAG), a technique that uses an external knowledge base to supplement the context of Large Language Models (LLMs) and generate responses. Notably, RAG combines pa- rameterized knowledge from LLMs and non-parameterized external knowledge, alleviates hallucination issues, identifies timely information via retrieval technology, and enhances re- sponse accuracy. Additionally, by citing sources, RAG in- creases transparency and user trust in model outputs. RAG can also be customized based on specific domains by index- ing relevant text corpora. RAG\u2019s development and charac- teristics are summarized into three paradigms: Naive RAG, Advanced RAG, and Modular RAG, each with its models, methods, and shortcomings. Naive RAG primarily involves the \u2019retrieval-reading\u2019 process. Advanced RAG uses more refined data processing, optimizes the knowledge base in- dexing, and introduces multiple or iterative retrievals. As exploration deepens, RAG integrates other techniques like fine-tuning, leading to the emergence of the Modular RAG paradigm, which enriches the RAG process with new mod- ules and offers more flexibility. In the subsequent chapters, we further analyze three key parts of RAG in detail. Chapter 4 introduces the retriever of RAG, how to process corpora to obtain better semantic repre- sentations, how to mitigate the semantic gap between Query and documents, and how to adjust the retriever to fit the gen- erator. Chapter 5 explains how the generator obtains better generation results by post-processing retrieved documents, avoiding the \u201dLost in the middle\u201d issue, as well as methods to adjust the generator to fit the retriever. Subsequently, in Chap- ter 6, we review the current retrieval enhancement methods from the aspects of the retrieval stage, retrieval data sources, and retrieval process. Chapter 7 explains how to evaluate current RAG methods, including evaluation, key indicators, and current evaluation frameworks Finally, we provided an outlook on the poten- tial future research directions for RAG. As a method that combines retrieval and generation, RAG has numerous po- tential development directions in future research. By contin- uously improving the technology and expanding its applica- tions, the performance and practicality of RAG can be further enhanced.",
    "raw_text": "9 Conclusion This paper thoroughly explores Retrieval-Augmented Gener- ation (RAG), a technique that uses an external knowledge base to supplement the context of Large Language Models (LLMs) and generate responses. Notably, RAG combines pa- rameterized knowledge from LLMs and non-parameterized external knowledge, alleviates hallucination issues, identifies timely information via retrieval technology, and enhances re- sponse accuracy. Additionally, by citing sources, RAG in- creases transparency and user trust in model outputs. RAG can also be customized based on specific domains by index- ing relevant text corpora. RAG\u2019s development and charac- teristics are summarized into three paradigms: Naive RAG, Advanced RAG, and Modular RAG, each with its models, methods, and shortcomings. Naive RAG primarily involves the \u2019retrieval-reading\u2019 process. Advanced RAG uses more refined data processing, optimizes the knowledge base in- dexing, and introduces multiple or iterative retrievals. As exploration deepens, RAG integrates other techniques like fine-tuning, leading to the emergence of the Modular RAG paradigm, which enriches the RAG process with new mod- ules and offers more flexibility. In the subsequent chapters, we further analyze three key parts of RAG in detail. Chapter 4 introduces the retriever of RAG, how to process corpora to obtain better semantic repre- sentations, how to mitigate the semantic gap between Query and documents, and how to adjust the retriever to fit the gen- erator. Chapter 5 explains how the generator obtains better generation results by post-processing retrieved documents, avoiding the \u201dLost in the middle\u201d issue, as well as methods to adjust the generator to fit the retriever. Subsequently, in Chap- ter 6, we review the current retrieval enhancement methods from the aspects of the retrieval stage, retrieval data sources, and retrieval process. Chapter 7 explains how to evaluate current RAG methods, including evaluation, key indicators, and current evaluation frameworks Finally, we provided an outlook on the poten- tial future research directions for RAG. As a method that combines retrieval and generation, RAG has numerous po- tential development directions in future research. By contin- uously improving the technology and expanding its applica- tions, the performance and practicality of RAG can be further enhanced."
  },
  {
    "chunk_id": "1b3c58a7-4516-45a2-adba-88ede5e374f0",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 21,
    "retrieval_text": "References [Alon et al., 2022] Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. Neuro- symbolic language modeling with automaton-augmented retrieval. In International Conference on Machine Learn- ing, pages 468\u2013485. PMLR, 2022. [Anderson et al., 2022] Nathan Anderson, Caleb Wilson, and Stephen D. Richardson. Lingua: Addressing scenar- ios for live interpretation and automatic dubbing. In Jan- ice Campbell, Stephen Larocca, Jay Marciano, Konstantin Savenkov, and Alex Yanishevsky, editors, Proceedings of the 15th Biennial Conference of the Association for Ma- chine Translation in the Americas (Volume 2: Users and Providers Track and Government Track), pages 202\u2013209, Orlando, USA, September 2022. Association for Machine Translation in the Americas. [AngIE, 2023] AngIE. Angle-optimized text embeddings. https://github.com/SeanLee97/AnglE, 2023. [Arora et al., 2023] Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gaurav Sinha, and Amit Sharma. Gar-meets-rag paradigm for zero-shot infor- mation retrieval. arXiv preprint arXiv:2310.20158, 2023. [Asai et al., 2023a] Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pages 41\u201346, 2023. [Asai et al., 2023b] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023. [BAAI, 2023] BAAI. Flagembedding. https://github.com/ FlagOpen/FlagEmbedding, 2023. [Baek et al., 2023] Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C Park, and Sung Ju Hwang. Knowledge- augmented language model verification. arXiv preprint arXiv:2310.12836, 2023. [Bai et al., 2022] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [Bang et al., 2023] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023. [Berchansky et al., 2023] Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. Opti- mizing retrieval-augmented reader models via token elim- ination. arXiv preprint arXiv:2310.13682, 2023. [Bisk et al., 2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physi- cal commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020. [Blagojevi, 2023] Vladimir Blagojevi. Enhancing rag pipelines in haystack: Introducing diversityranker and lostinthemiddleranker. https://towardsdatascience.com/ enhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023. [Borgeaud et al., 2022] Sebastian Borgeaud, Arthur Men- sch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206\u20132240. PMLR, 2022. [Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry- der, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing sys- tems, 33:1877\u20131901, 2020. [Cai et al., 2021] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. arXiv preprint arXiv:2105.11269, 2021. [Chan et al., 2023] David M Chan, Shalini Ghosh, Ariya Rastrow, and Bj\u00a8orn Hoffmeister. Using external off- policy speech-to-text mappings in contextual end-to- end automated speech recognition. arXiv preprint arXiv:2301.02736, 2023. [Chen and Yih, 2020] Danqi Chen and Wen-tau Yih. Open- domain question answering. In Proceedings of the 58th annual meeting of the association for computational lin- guistics: tutorial abstracts, pages 34\u201337, 2020. [Chen et al., 2023a] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading. arXiv preprint arXiv:2310.05029, 2023. [Chen et al., 2023b] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language mod- els in retrieval-augmented generation. arXiv preprint arXiv:2309.01431, 2023. [Cheng et al., 2022] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine transla- tion with contrastive translation memories. arXiv preprint arXiv:2212.03140, 2022. [Cheng et al., 2023a] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, and Qi Zhang. Uprise: Uni- versal prompt retrieval for improving zero-shot evaluation. arXiv preprint arXiv:2303.08518, 2023. [Cheng et al., 2023b] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself up: Retrieval-augmented text generation with self mem- ory. arXiv preprint arXiv:2305.02437, 2023. [Clark et al., 2019] Christopher Clark, Kenton Lee, Ming- Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [Cohere, 2023] Cohere. Say goodbye to irrelevant search results: Cohere rerank is here. https://txt.cohere.com/ rerank/, 2023. [Dai et al., 2022] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022.",
    "raw_text": "References [Alon et al., 2022] Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. Neuro- symbolic language modeling with automaton-augmented retrieval. In International Conference on Machine Learn- ing, pages 468\u2013485. PMLR, 2022. [Anderson et al., 2022] Nathan Anderson, Caleb Wilson, and Stephen D. Richardson. Lingua: Addressing scenar- ios for live interpretation and automatic dubbing. In Jan- ice Campbell, Stephen Larocca, Jay Marciano, Konstantin Savenkov, and Alex Yanishevsky, editors, Proceedings of the 15th Biennial Conference of the Association for Ma- chine Translation in the Americas (Volume 2: Users and Providers Track and Government Track), pages 202\u2013209, Orlando, USA, September 2022. Association for Machine Translation in the Americas. [AngIE, 2023] AngIE. Angle-optimized text embeddings. https://github.com/SeanLee97/AnglE, 2023. [Arora et al., 2023] Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gaurav Sinha, and Amit Sharma. Gar-meets-rag paradigm for zero-shot infor- mation retrieval. arXiv preprint arXiv:2310.20158, 2023. [Asai et al., 2023a] Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pages 41\u201346, 2023. [Asai et al., 2023b] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023. [BAAI, 2023] BAAI. Flagembedding. https://github.com/ FlagOpen/FlagEmbedding, 2023. [Baek et al., 2023] Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C Park, and Sung Ju Hwang. Knowledge- augmented language model verification. arXiv preprint arXiv:2310.12836, 2023. [Bai et al., 2022] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [Bang et al., 2023] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023. [Berchansky et al., 2023] Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. Opti- mizing retrieval-augmented reader models via token elim- ination. arXiv preprint arXiv:2310.13682, 2023. [Bisk et al., 2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physi- cal commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020. [Blagojevi, 2023] Vladimir Blagojevi. Enhancing rag pipelines in haystack: Introducing diversityranker and lostinthemiddleranker. https://towardsdatascience.com/ enhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023. [Borgeaud et al., 2022] Sebastian Borgeaud, Arthur Men- sch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206\u20132240. PMLR, 2022. [Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry- der, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing sys- tems, 33:1877\u20131901, 2020. [Cai et al., 2021] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. arXiv preprint arXiv:2105.11269, 2021. [Chan et al., 2023] David M Chan, Shalini Ghosh, Ariya Rastrow, and Bj\u00a8orn Hoffmeister. Using external off- policy speech-to-text mappings in contextual end-to- end automated speech recognition. arXiv preprint arXiv:2301.02736, 2023. [Chen and Yih, 2020] Danqi Chen and Wen-tau Yih. Open- domain question answering. In Proceedings of the 58th annual meeting of the association for computational lin- guistics: tutorial abstracts, pages 34\u201337, 2020. [Chen et al., 2023a] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading. arXiv preprint arXiv:2310.05029, 2023. [Chen et al., 2023b] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language mod- els in retrieval-augmented generation. arXiv preprint arXiv:2309.01431, 2023. [Cheng et al., 2022] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine transla- tion with contrastive translation memories. arXiv preprint arXiv:2212.03140, 2022. [Cheng et al., 2023a] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, and Qi Zhang. Uprise: Uni- versal prompt retrieval for improving zero-shot evaluation. arXiv preprint arXiv:2303.08518, 2023. [Cheng et al., 2023b] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself up: Retrieval-augmented text generation with self mem- ory. arXiv preprint arXiv:2305.02437, 2023. [Clark et al., 2019] Christopher Clark, Kenton Lee, Ming- Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [Cohere, 2023] Cohere. Say goodbye to irrelevant search results: Cohere rerank is here. https://txt.cohere.com/ rerank/, 2023. [Dai et al., 2022] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022."
  },
  {
    "chunk_id": "b040dc75-bae3-4659-849b-11ac9c8952f5",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 23,
    "retrieval_text": "[Es et al., 2023] Shahul Es, Jithin James, Luis Espinosa- Anke, and Steven Schockaert. Ragas: Automated eval- uation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023. [Feng et al., 2023a] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. Retrieval-generation synergy augmented large language models. arXiv preprint arXiv:2310.05149, 2023. [Feng et al., 2023b] Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. Trends in integra- tion of knowledge and large language models: A survey and taxonomy of methods, benchmarks, and applications. arXiv preprint arXiv:2311.05876, 2023. [Gao et al., 2022] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496, 2022. [Glass et al., 2021] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, and Alfio Gliozzo. Robust retrieval augmented generation for zero-shot slot filling. arXiv preprint arXiv:2108.13934, 2021. [Google, 2023] Google. Gemini: A family of highly capable multimodal models. https://goo.gle/GeminiPaper, 2023. [Hendrycks et al., 2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask lan- guage understanding. arXiv preprint arXiv:2009.03300, 2020. [Izacard et al., 2022] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re- trieval augmented language models. arXiv preprint arXiv:2208.03299, 2022. [Jarvis and Allard, 2023] Colin Jarvis and John Al- lard. A survey of techniques for maximizing llm performance. https://community.openai.com/ t/openai-dev-day-2023-breakout-sessions/505213# a-survey-of-techniques-for-maximizing-llm-performance-2, 2023. [Jiang et al., 2023a] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language mod- els. arXiv preprint arXiv:2310.05736, 2023. [Jiang et al., 2023b] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023. [Kandpal et al., 2023] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pages 15696\u201315707. PMLR, 2023. [Kang et al., 2023] Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. Knowledge graph-augmented language models for knowledge-grounded dialogue gener- ation. arXiv preprint arXiv:2305.18846, 2023. [Karpukhin et al., 2020] Vladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. [Khandelwal et al., 2019] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Gen- eralization through memorization: Nearest neighbor lan- guage models. arXiv preprint arXiv:1911.00172, 2019. [Khattab and Zaharia, 2020] Omar Khattab and Matei Za- haria. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39\u201348, 2020. [Khattab et al., 2022] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Compos- ing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022. [Kwiatkowski et al., 2019] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Asso- ciation for Computational Linguistics, 7:453\u2013466, 2019. [Lee et al., 2020] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of phrases at scale. arXiv preprint arXiv:2012.12624, 2020. [Leng et al., 2023] Quinn Leng, Kasey Uhlenhuth, and Alkis Polyzotis. Best practices for llm evaluation of rag applications. https://www.databricks.com/blog/ LLM-auto-eval-best-practices-RAG, 2023. [Lewis et al., 2020] Patrick Lewis, Ethan Perez, Aleksan- dra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Infor- mation Processing Systems, 33:9459\u20139474, 2020. [Li et al., 2023a] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre- training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. [Li et al., 2023b] Xiaoqian Li, Ercong Nie, and Sheng Liang. From classification to generation: Insights into crosslingual retrieval augmented icl. arXiv preprint arXiv:2311.06595, 2023. [Li et al., 2023c] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq Joty, and Sou- janya Poria. Chain of knowledge: A framework for grounding large language models with structured knowl- edge bases. arXiv preprint arXiv:2305.13269, 2023. [Li et al., 2023d] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu. Structure-aware language model pretraining improves dense retrieval on structured data. arXiv preprint arXiv:2305.19912, 2023. [Lin et al., 2023] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Ro- driguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint arXiv:2310.01352, 2023.",
    "raw_text": "[Es et al., 2023] Shahul Es, Jithin James, Luis Espinosa- Anke, and Steven Schockaert. Ragas: Automated eval- uation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023. [Feng et al., 2023a] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. Retrieval-generation synergy augmented large language models. arXiv preprint arXiv:2310.05149, 2023. [Feng et al., 2023b] Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. Trends in integra- tion of knowledge and large language models: A survey and taxonomy of methods, benchmarks, and applications. arXiv preprint arXiv:2311.05876, 2023. [Gao et al., 2022] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496, 2022. [Glass et al., 2021] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, and Alfio Gliozzo. Robust retrieval augmented generation for zero-shot slot filling. arXiv preprint arXiv:2108.13934, 2021. [Google, 2023] Google. Gemini: A family of highly capable multimodal models. https://goo.gle/GeminiPaper, 2023. [Hendrycks et al., 2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask lan- guage understanding. arXiv preprint arXiv:2009.03300, 2020. [Izacard et al., 2022] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re- trieval augmented language models. arXiv preprint arXiv:2208.03299, 2022. [Jarvis and Allard, 2023] Colin Jarvis and John Al- lard. A survey of techniques for maximizing llm performance. https://community.openai.com/ t/openai-dev-day-2023-breakout-sessions/505213# a-survey-of-techniques-for-maximizing-llm-performance-2, 2023. [Jiang et al., 2023a] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language mod- els. arXiv preprint arXiv:2310.05736, 2023. [Jiang et al., 2023b] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023. [Kandpal et al., 2023] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pages 15696\u201315707. PMLR, 2023. [Kang et al., 2023] Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. Knowledge graph-augmented language models for knowledge-grounded dialogue gener- ation. arXiv preprint arXiv:2305.18846, 2023. [Karpukhin et al., 2020] Vladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. [Khandelwal et al., 2019] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Gen- eralization through memorization: Nearest neighbor lan- guage models. arXiv preprint arXiv:1911.00172, 2019. [Khattab and Zaharia, 2020] Omar Khattab and Matei Za- haria. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39\u201348, 2020. [Khattab et al., 2022] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Compos- ing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022. [Kwiatkowski et al., 2019] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Asso- ciation for Computational Linguistics, 7:453\u2013466, 2019. [Lee et al., 2020] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of phrases at scale. arXiv preprint arXiv:2012.12624, 2020. [Leng et al., 2023] Quinn Leng, Kasey Uhlenhuth, and Alkis Polyzotis. Best practices for llm evaluation of rag applications. https://www.databricks.com/blog/ LLM-auto-eval-best-practices-RAG, 2023. [Lewis et al., 2020] Patrick Lewis, Ethan Perez, Aleksan- dra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Infor- mation Processing Systems, 33:9459\u20139474, 2020. [Li et al., 2023a] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre- training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. [Li et al., 2023b] Xiaoqian Li, Ercong Nie, and Sheng Liang. From classification to generation: Insights into crosslingual retrieval augmented icl. arXiv preprint arXiv:2311.06595, 2023. [Li et al., 2023c] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq Joty, and Sou- janya Poria. Chain of knowledge: A framework for grounding large language models with structured knowl- edge bases. arXiv preprint arXiv:2305.13269, 2023. [Li et al., 2023d] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu. Structure-aware language model pretraining improves dense retrieval on structured data. arXiv preprint arXiv:2305.19912, 2023. [Lin et al., 2023] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Ro- driguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint arXiv:2310.01352, 2023."
  },
  {
    "chunk_id": "9fdcdc0e-8015-4b74-87ba-4f31b4340c0b",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 24,
    "retrieval_text": "[Litman et al., 2020] Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman, Shai Mazor, and R Manmatha. Scat- ter: selective context attentional scene text recognizer. In proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 11962\u201311972, 2020. [Liu et al., 2023] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language mod- els use long contexts. arXiv preprint arXiv:2307.03172, 2023. [Liu, 2023] Jerry Liu. Building production-ready rag applications. https://www.ai.engineer/summit/schedule/ building-production-ready-rag-applications, 2023. [Luo et al., 2023] Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Augmented large language models with paramet- ric knowledge guiding. arXiv preprint arXiv:2305.04757, 2023. [Ma et al., 2023a] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting for retrieval-augmented large language models. arXiv preprint arXiv:2305.14283, 2023. [Ma et al., 2023b] Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. Large language model is not a good few- shot information extractor, but a good reranker for hard samples! ArXiv, abs/2303.08559, 2023. [Modarressi et al., 2023] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Sch\u00a8utze. Ret-llm: Towards a general read-write memory for large language models. arXiv preprint arXiv:2305.14322, 2023. [Nakano et al., 2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. [Nashid et al., 2023] Noor Nashid, Mifta Sintaha, and Ali Mesbah. Retrieval-based prompt selection for code-related few-shot learning. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pages 2450\u2013 2462, 2023. [OpenAI, 2023] OpenAI. Gpt-4 technical report. https://cdn. openai.com/papers/gpt-4.pdf, 2023. [Petroni et al., 2019] Fabio Petroni, Tim Rockt\u00a8aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019. [Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u2013 5551, 2020. [Ranzato et al., 2015] Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015. [Reddy et al., 2019] Siva Reddy, Danqi Chen, and Christo- pher D Manning. Coqa: A conversational question an- swering challenge. Transactions of the Association for Computational Linguistics, 7:249\u2013266, 2019. [Robertson et al., 2009] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends\u00ae in Information Re- trieval, 3(4):333\u2013389, 2009. [Saad-Falcon et al., 2023] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: An automated evaluation framework for retrieval-augmented generation systems. arXiv preprint arXiv:2311.09476, 2023. [Schick et al., 2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. [Sciavolino et al., 2021] Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. Simple entity- centric questions challenge dense retrievers. arXiv preprint arXiv:2109.08535, 2021. [Shao et al., 2023] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. En- hancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294, 2023. [Shi et al., 2023] Weijia Shi, Sewon Min, Michihiro Ya- sunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval- augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023. [Shuster et al., 2021] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval aug- mentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021. [Srivastava et al., 2022] Aarohi Srivastava, Abhinav Ras- togi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. [Sun et al., 2022] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yim- ing Yang, and Denny Zhou. Recitation-augmented lan- guage models. arXiv preprint arXiv:2210.01296, 2022. [Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [Trivedi et al., 2022] Harsh Trivedi, Niranjan Balasubrama- nian, Tushar Khot, and Ashish Sabharwal. Inter- leaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509, 2022. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [Vaze et al., 2021] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need? arXiv preprint arXiv:2110.06207, 2021. [VoyageAI, 2023] VoyageAI. Voyage\u2019s embedding models. https://docs.voyageai.com/embeddings/, 2023.",
    "raw_text": "[Litman et al., 2020] Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman, Shai Mazor, and R Manmatha. Scat- ter: selective context attentional scene text recognizer. In proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 11962\u201311972, 2020. [Liu et al., 2023] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language mod- els use long contexts. arXiv preprint arXiv:2307.03172, 2023. [Liu, 2023] Jerry Liu. Building production-ready rag applications. https://www.ai.engineer/summit/schedule/ building-production-ready-rag-applications, 2023. [Luo et al., 2023] Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Augmented large language models with paramet- ric knowledge guiding. arXiv preprint arXiv:2305.04757, 2023. [Ma et al., 2023a] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting for retrieval-augmented large language models. arXiv preprint arXiv:2305.14283, 2023. [Ma et al., 2023b] Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. Large language model is not a good few- shot information extractor, but a good reranker for hard samples! ArXiv, abs/2303.08559, 2023. [Modarressi et al., 2023] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Sch\u00a8utze. Ret-llm: Towards a general read-write memory for large language models. arXiv preprint arXiv:2305.14322, 2023. [Nakano et al., 2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. [Nashid et al., 2023] Noor Nashid, Mifta Sintaha, and Ali Mesbah. Retrieval-based prompt selection for code-related few-shot learning. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pages 2450\u2013 2462, 2023. [OpenAI, 2023] OpenAI. Gpt-4 technical report. https://cdn. openai.com/papers/gpt-4.pdf, 2023. [Petroni et al., 2019] Fabio Petroni, Tim Rockt\u00a8aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019. [Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u2013 5551, 2020. [Ranzato et al., 2015] Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015. [Reddy et al., 2019] Siva Reddy, Danqi Chen, and Christo- pher D Manning. Coqa: A conversational question an- swering challenge. Transactions of the Association for Computational Linguistics, 7:249\u2013266, 2019. [Robertson et al., 2009] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends\u00ae in Information Re- trieval, 3(4):333\u2013389, 2009. [Saad-Falcon et al., 2023] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: An automated evaluation framework for retrieval-augmented generation systems. arXiv preprint arXiv:2311.09476, 2023. [Schick et al., 2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. [Sciavolino et al., 2021] Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. Simple entity- centric questions challenge dense retrievers. arXiv preprint arXiv:2109.08535, 2021. [Shao et al., 2023] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. En- hancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294, 2023. [Shi et al., 2023] Weijia Shi, Sewon Min, Michihiro Ya- sunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval- augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023. [Shuster et al., 2021] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval aug- mentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021. [Srivastava et al., 2022] Aarohi Srivastava, Abhinav Ras- togi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. [Sun et al., 2022] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yim- ing Yang, and Denny Zhou. Recitation-augmented lan- guage models. arXiv preprint arXiv:2210.01296, 2022. [Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [Trivedi et al., 2022] Harsh Trivedi, Niranjan Balasubrama- nian, Tushar Khot, and Ashish Sabharwal. Inter- leaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509, 2022. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [Vaze et al., 2021] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need? arXiv preprint arXiv:2110.06207, 2021. [VoyageAI, 2023] VoyageAI. Voyage\u2019s embedding models. https://docs.voyageai.com/embeddings/, 2023."
  },
  {
    "chunk_id": "8496963f-e8c1-495c-9437-8c4757f1699c",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 25,
    "retrieval_text": "[Wang et al., 2019] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stick- ier benchmark for general-purpose language understand- ing systems. Advances in neural information processing systems, 32, 2019. [Wang et al., 2022a] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. arXiv preprint arXiv:2203.08773, 2022. [Wang et al., 2022b] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retriev- ing from training data. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 3170\u2013 3179, Dublin, Ireland, May 2022. Association for Compu- tational Linguistics. [Wang et al., 2023a] Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models with retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. [Wang et al., 2023b] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language models. arXiv preprint arXiv:2303.07678, 2023. [Wang et al., 2023c] Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang. Knowledgpt: Enhancing large lan- guage models with retrieval and storage access on knowl- edge bases. arXiv preprint arXiv:2308.11761, 2023. [Wang et al., 2023d] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. Self-knowledge guided retrieval aug- mentation for large language models. arXiv preprint arXiv:2310.05002, 2023. [Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuur- mans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022. [Xia et al., 2019] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. Graph based translation mem- ory for neural machine translation. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7297\u20137304, 2019. [Xu et al., 2023a] Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. arXiv preprint arXiv:2310.04408, 2023. [Xu et al., 2023b] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Sub- ramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large lan- guage models. arXiv preprint arXiv:2310.03025, 2023. [Xu et al., 2023c] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Sub- ramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large lan- guage models. arXiv preprint arXiv:2310.03025, 2023. [Yang et al., 2023a] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10714\u201310726, 2023. [Yang et al., 2023b] Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. Prca: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contex- tual adapter. arXiv preprint arXiv:2310.18347, 2023. [Yang et al., 2023c] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and ad- ditional opinions. arXiv preprint arXiv:2306.02224, 2023. [Yao et al., 2023] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469, 2023. [Yasunaga et al., 2022] Michihiro Yasunaga, Armen Agha- janyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561, 2022. [Ye et al., 2020] Deming Ye, Yankai Lin, Jiaju Du, Zheng- hao Liu, Peng Li, Maosong Sun, and Zhiyuan Liu. Coref- erential reasoning learning for language representation. arXiv preprint arXiv:2004.06870, 2020. [Yoran et al., 2023] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented lan- guage models robust to irrelevant context. arXiv preprint arXiv:2310.01558, 2023. [Yu et al., 2022] Wenhao Yu, Dan Iter, Shuohang Wang, Yi- chong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than re- trieve: Large language models are strong context genera- tors. arXiv preprint arXiv:2209.10063, 2022. [Yu et al., 2023a] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chain- of-note: Enhancing robustness in retrieval-augmented lan- guage models. arXiv preprint arXiv:2311.09210, 2023. [Yu et al., 2023b] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. Augmentation-adapted retriever improves generalization of language models as generic plug-in. arXiv preprint arXiv:2305.17331, 2023. [Zhang et al., 2019] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced language representation with informative entities. arXiv preprint arXiv:1905.07129, 2019. [Zhang et al., 2023a] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve any- thing to augment large language models. arXiv preprint arXiv:2310.07554, 2023. [Zhang et al., 2023b] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.",
    "raw_text": "[Wang et al., 2019] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stick- ier benchmark for general-purpose language understand- ing systems. Advances in neural information processing systems, 32, 2019. [Wang et al., 2022a] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. arXiv preprint arXiv:2203.08773, 2022. [Wang et al., 2022b] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retriev- ing from training data. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 3170\u2013 3179, Dublin, Ireland, May 2022. Association for Compu- tational Linguistics. [Wang et al., 2023a] Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models with retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. [Wang et al., 2023b] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language models. arXiv preprint arXiv:2303.07678, 2023. [Wang et al., 2023c] Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang. Knowledgpt: Enhancing large lan- guage models with retrieval and storage access on knowl- edge bases. arXiv preprint arXiv:2308.11761, 2023. [Wang et al., 2023d] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. Self-knowledge guided retrieval aug- mentation for large language models. arXiv preprint arXiv:2310.05002, 2023. [Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuur- mans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022. [Xia et al., 2019] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. Graph based translation mem- ory for neural machine translation. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7297\u20137304, 2019. [Xu et al., 2023a] Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. arXiv preprint arXiv:2310.04408, 2023. [Xu et al., 2023b] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Sub- ramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large lan- guage models. arXiv preprint arXiv:2310.03025, 2023. [Xu et al., 2023c] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Sub- ramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large lan- guage models. arXiv preprint arXiv:2310.03025, 2023. [Yang et al., 2023a] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10714\u201310726, 2023. [Yang et al., 2023b] Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. Prca: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contex- tual adapter. arXiv preprint arXiv:2310.18347, 2023. [Yang et al., 2023c] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and ad- ditional opinions. arXiv preprint arXiv:2306.02224, 2023. [Yao et al., 2023] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469, 2023. [Yasunaga et al., 2022] Michihiro Yasunaga, Armen Agha- janyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561, 2022. [Ye et al., 2020] Deming Ye, Yankai Lin, Jiaju Du, Zheng- hao Liu, Peng Li, Maosong Sun, and Zhiyuan Liu. Coref- erential reasoning learning for language representation. arXiv preprint arXiv:2004.06870, 2020. [Yoran et al., 2023] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented lan- guage models robust to irrelevant context. arXiv preprint arXiv:2310.01558, 2023. [Yu et al., 2022] Wenhao Yu, Dan Iter, Shuohang Wang, Yi- chong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than re- trieve: Large language models are strong context genera- tors. arXiv preprint arXiv:2209.10063, 2022. [Yu et al., 2023a] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chain- of-note: Enhancing robustness in retrieval-augmented lan- guage models. arXiv preprint arXiv:2311.09210, 2023. [Yu et al., 2023b] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. Augmentation-adapted retriever improves generalization of language models as generic plug-in. arXiv preprint arXiv:2305.17331, 2023. [Zhang et al., 2019] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced language representation with informative entities. arXiv preprint arXiv:1905.07129, 2019. [Zhang et al., 2023a] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve any- thing to augment large language models. arXiv preprint arXiv:2310.07554, 2023. [Zhang et al., 2023b] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023."
  },
  {
    "chunk_id": "8613d0c9-51c2-40a1-9eeb-a0c9c95d0019",
    "modality": "text",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": 26,
    "retrieval_text": "[Zhang, 2023] Jiawei Zhang. Graph-toolformer: To em- power llms with graph reasoning ability via prompt aug- mented by chatgpt. arXiv preprint arXiv:2304.11116, 2023. [Zhao et al., 2022] Jinming Zhao, Gholamreza Haffar, and Ehsan Shareghi. Generating synthetic speech from spokenvocab for speech translation. arXiv preprint arXiv:2210.08174, 2022. [Zheng et al., 2023] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny Zhou. Take a step back: Evoking reasoning via abstraction in large language models. arXiv preprint arXiv:2310.06117, 2023. [Zhong et al., 2022] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmen- tation. arXiv preprint arXiv:2205.12674, 2022. [Zhu et al., 2022] Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, and William Yang Wang. Visualize before you write: Imagination- guided open-ended text generation. arXiv preprint arXiv:2210.03765, 2022. [Zhu et al., 2023] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107, 2023. [Zhuang et al., 2023] Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon. Open-source large language models are strong zero-shot query likeli- hood models for document ranking. arXiv preprint arXiv:2310.13243, 2023.",
    "raw_text": "[Zhang, 2023] Jiawei Zhang. Graph-toolformer: To em- power llms with graph reasoning ability via prompt aug- mented by chatgpt. arXiv preprint arXiv:2304.11116, 2023. [Zhao et al., 2022] Jinming Zhao, Gholamreza Haffar, and Ehsan Shareghi. Generating synthetic speech from spokenvocab for speech translation. arXiv preprint arXiv:2210.08174, 2022. [Zheng et al., 2023] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny Zhou. Take a step back: Evoking reasoning via abstraction in large language models. arXiv preprint arXiv:2310.06117, 2023. [Zhong et al., 2022] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmen- tation. arXiv preprint arXiv:2205.12674, 2022. [Zhu et al., 2022] Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, and William Yang Wang. Visualize before you write: Imagination- guided open-ended text generation. arXiv preprint arXiv:2210.03765, 2022. [Zhu et al., 2023] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107, 2023. [Zhuang et al., 2023] Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon. Open-source large language models are strong zero-shot query likeli- hood models for document ranking. arXiv preprint arXiv:2310.13243, 2023."
  },
  {
    "chunk_id": "b615ea81-fc1d-4e39-bd1d-d1aadf66bea3",
    "modality": "image",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital graphic that appears to be a flowchart or concept map with a tree-like structure at its center. At the top of the tree, there are three branches labeled \"2019,\" \"2020,\" and \"2021.\" Each branch has sub-branches representing different topics or areas of focus.\n\nThe 2019 branch includes a sub-branch labeled \"Retail\" with two further sub-branches: \"Influence\" and \"Training.\" The \"Influence\" sub-branch has three more branches: \"Fire,\" \"Water,\" and \"Air.\" Each of these branches is connected to a specific node on the tree, suggesting a relationship or connection between them.\n\nThe 2020 branch has two sub-branches: \"Retail\" and \"Influence.\" The \"Retail\" sub-branch has three more branches: \"Fire,\" \"Water,\" and \"Air.\" The \"Influence\" sub-branch also has three more branches: \"Fire,\" \"Water,\" and \"Air.\"\n\nThe 2021 branch has two sub-branches: \"Retail\" and \"Influence.\" The \"Retail\" sub-branch has three more branches: \"Fire,\" \"Water,\" and \"Air.\" The \"Influence\" sub-branch also has three more branches: \"Fire,\" \"Water,\" and \"Air.\"\n\nThe tree trunk is labeled with the text \"2021,\" and there are two additional labels on the tree: \"Retail\" at the bottom of the trunk, and \"Retail\" at the top of the 2021 branch. The background of the image is white, and the text and branches are colored in various shades of blue, green, and orange.\n\nAt the bottom of the image, there's a horizontal bar with a timeline that spans from 2019 to 2021, indicating the time period associated with each branch of the tree. The timeline is marked with years and has a color gradient that matches the colors used in the tree.\n\nThe style of the image suggests it could be a visual representation of a strategic plan or organizational structure for a company or organization over three years, with different areas of focus or initiatives being highlighted for each year. The use of branches and nodes implies a hierarchical or networked approach to planning or organizing information. ",
    "raw_text": null
  },
  {
    "chunk_id": "93ff9e3e-47c8-4a3f-9149-4bdb809ca443",
    "modality": "image",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital photograph of a whiteboard with handwritten text and diagrams. At the top left corner, there's a title that reads \"EXTENSION OF KNOWLEDGE REQUIRED.\" Below this, there are two main sections: \"Normal knowledge\" and \"Advanced knowledge.\"\n\nUnder \"Normal knowledge,\" there is a list of items including \"RAG,\" \"FINE TUNING,\" \"COMPARISON WITH EXISTING TECHNIQUE,\" and \"ADAPTATION TO THE SITUATION.\" Each item is connected to the next by arrows, indicating a flow or process.\n\nThe \"Advanced knowledge\" section contains two items: \"DEVELOPING NEW TECHNIQUES\" and \"INTEGRATING WITH EXISTING SYSTEMS.\" These are also linked with arrows, suggesting a sequence or progression from normal to advanced knowledge.\n\nIn the center of the whiteboard, there is a diagram that includes three main components: \"RAG,\" \"FINE TUNING,\" and \"ADAPTATION TO THE SITUATION.\" Each component has a sub-component connected to it with arrows, indicating a more detailed breakdown or expansion of each concept.\n\nThe text \"ALL OF ABOVE\" is written in the center of the whiteboard, connecting all the components together.\n\nAt the bottom left corner, there's a note that says \"LOW,\" and at the bottom right corner, there's another note that reads \"HIGH.\" These notes are connected by an arrow, suggesting a scale or range from low to high.\n\nThe whiteboard also includes a flowchart with various boxes and arrows connecting them, illustrating a process or workflow. The text \"PROCESSING\" is written in the center of this flowchart, indicating that it represents a sequence of steps or actions within the system being described.\n\nThe overall style of the image is informational and appears to be used for educational or instructional purposes, possibly related to engineering or technical knowledge. ",
    "raw_text": null
  },
  {
    "chunk_id": "afcd5dd7-c78e-4004-8421-74c04073b0f2",
    "modality": "image",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital screenshot of a flowchart with a complex structure, containing various nodes connected by arrows indicating the flow of information or processes. The flowchart appears to be a representation of a software architecture or system design, possibly for an AI-related application given the presence of terms like \"Neural Network\" and \"Machine Learning.\"\n\nAt the top left corner, there is a node labeled \"Neural Network,\" which seems to be the starting point of the flow. From this node, arrows lead to other nodes that represent different components or modules within the system. These include \"Data Preprocessing,\" \"Feature Selection,\" \"Model Training,\" and \"Evaluation.\"\n\nThe \"Data Preprocessing\" module is connected to a \"Database\" node, suggesting that data is sourced from a database before being processed further. The \"Feature Selection\" module is linked to both the \"Neural Network\" and \"Data Preprocessing\" nodes, indicating its role in selecting relevant features for the neural network model.\n\nThe \"Model Training\" module is connected to the \"Feature Selection\" module, suggesting that after feature selection, the selected features are used for training the model. This module is also linked to a \"Machine Learning\" node, which is further connected to a \"Deployment\" node, indicating that once the model is trained, it is deployed for use.\n\nThe \"Evaluation\" module is connected to both the \"Model Training\" and \"Deployment\" modules, suggesting that evaluation occurs after the model has been trained and deployed. This could involve assessing the performance of the model or the accuracy of its predictions.\n\nThe flowchart also includes a \"Data Ingestion\" node, which is connected to the \"Database\" node, indicating that data is ingested into the system from an external source. Additionally, there are nodes labeled \"AI/ML,\" \"Advanced Analytics,\" and \"Business Intelligence,\" which could represent different areas of application for the AI model within the organization or industry.\n\nThe flowchart uses a combination of rectangles and circles to represent the various components and modules, with arrows indicating the direction of data flow or process execution. The text labels are clear and legible, providing a detailed overview of the system's architecture. ",
    "raw_text": null
  },
  {
    "chunk_id": "888f7b78-fc18-468f-b6b7-c246c0f5d841",
    "modality": "image",
    "source_pdf": "A Survey on Retrieval-Augmented Generation.pdf",
    "page_number": null,
    "retrieval_text": " The image displays a flowchart with a title at the top that reads \"DEVELOPMENT OF A COMPREHENSIVE STRATEGY FOR THE MANAGEMENT AND OPERATION OF THE LIBRARY\". The chart is structured in a hierarchical manner, with the main branches stemming from the title.\n\nThe first branch, labeled \"Objectives\", has two sub-branches: \"Improve accessibility\" and \"Enhance user experience\". Under \"Improve accessibility\", there are three sub-branches: \"Increase visibility of library resources\", \"Enhance the library's physical environment\", and \"Develop a comprehensive online presence\". The second branch, \"Enhance user experience\", has two sub-branches: \"Provide a welcoming atmosphere\" and \"Offer personalized services\".\n\nThe next main branch is \"Strategies for implementation\", which includes three sub-branches: \"Implement new technologies\", \"Train staff on customer service\", and \"Create partnerships with community organizations\".\n\nBelow these branches, there are two additional branches labeled \"Resources\" and \"Evaluation\". The \"Resources\" branch has two sub-branches: \"Financial resources\" and \"Human resources\". The \"Evaluation\" branch also has two sub-branches: \"Performance metrics\" and \"Feedback mechanisms\".\n\nThe chart uses a combination of colors (red, blue, and orange) to differentiate between the main branches and their sub-branches. The text is in English, and the style of the image is informational and organizational, typical of a strategic planning document or presentation slide. ",
    "raw_text": null
  },
  {
    "chunk_id": "56edd5ed-9bec-4fb2-babe-159fe6b2c82b",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 1,
    "retrieval_text": "8 2018 1 0 2 r p A 9 1 ] G L . s c [ 2 v 1 7 2 1 0 . 3 0 8 1 : v arXiv i X r a An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Shaojie Bai 1 J. Zico Kolter 2 Vladlen Koltun 3 Abstract For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional ar- chitectures can outperform recurrent networks on tasks such as audio synthesis and machine trans- lation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convo- lutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our re- sults indicate that a simple convolutional archi- tecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common associ- ation between sequence modeling and recurrent networks should be reconsidered, and convolu- tional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN. On the other hand, recent research indicates that certain con- volutional architectures can reach state-of-the-art accuracy in audio synthesis, word-level language modeling, and ma- chine translation (van den Oord et al., 2016; Kalchbrenner et al., 2016; Dauphin et al., 2017; Gehring et al., 2017a;b). This raises the question of whether these successes of con- volutional sequence modeling are con\ufb01ned to speci\ufb01c ap- plication domains or whether a broader reconsideration of the association between sequence processing and recurrent networks is in order. We address this question by conducting a systematic empiri- cal evaluation of convolutional and recurrent architectures on a broad range of sequence modeling tasks. We specif- ically target a comprehensive set of tasks that have been repeatedly used to compare the effectiveness of different recurrent network architectures. These tasks include poly- phonic music modeling, word- and character-level language modeling, as well as synthetic stress tests that had been de- liberately designed and frequently used to benchmark RNNs. Our evaluation is thus set up to compare convolutional and recurrent approaches to sequence modeling on the recurrent networks\u2019 \u201chome turf\u201d.",
    "raw_text": "8 2018 1 0 2 r p A 9 1 ] G L . s c [ 2 v 1 7 2 1 0 . 3 0 8 1 : v arXiv i X r a An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Shaojie Bai 1 J. Zico Kolter 2 Vladlen Koltun 3 Abstract For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional ar- chitectures can outperform recurrent networks on tasks such as audio synthesis and machine trans- lation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convo- lutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our re- sults indicate that a simple convolutional archi- tecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common associ- ation between sequence modeling and recurrent networks should be reconsidered, and convolu- tional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN. On the other hand, recent research indicates that certain con- volutional architectures can reach state-of-the-art accuracy in audio synthesis, word-level language modeling, and ma- chine translation (van den Oord et al., 2016; Kalchbrenner et al., 2016; Dauphin et al., 2017; Gehring et al., 2017a;b). This raises the question of whether these successes of con- volutional sequence modeling are con\ufb01ned to speci\ufb01c ap- plication domains or whether a broader reconsideration of the association between sequence processing and recurrent networks is in order. We address this question by conducting a systematic empiri- cal evaluation of convolutional and recurrent architectures on a broad range of sequence modeling tasks. We specif- ically target a comprehensive set of tasks that have been repeatedly used to compare the effectiveness of different recurrent network architectures. These tasks include poly- phonic music modeling, word- and character-level language modeling, as well as synthetic stress tests that had been de- liberately designed and frequently used to benchmark RNNs. Our evaluation is thus set up to compare convolutional and recurrent approaches to sequence modeling on the recurrent networks\u2019 \u201chome turf\u201d."
  },
  {
    "chunk_id": "7e67ee49-c132-4f6e-96d0-6532eb8be33a",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 1,
    "retrieval_text": "1. Introduction Deep learning practitioners commonly regard recurrent ar- chitectures as the default starting point for sequence model- ing tasks. The sequence modeling chapter in the canonical textbook on deep learning is titled \u201cSequence Modeling: Recurrent and Recursive Nets\u201d (Goodfellow et al., 2016), capturing the common association of sequence modeling and recurrent architectures. A well-regarded recent online course on \u201cSequence Models\u201d focuses exclusively on recur- rent architectures (Ng, 2018). 1Machine Learning Department, Carnegie Mellon Univer- sity, Pittsburgh, PA, USA 2Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA 3Intel Labs, Santa Clara, CA, USA. Correspondence to: Shaojie Bai <shaojieb@cs.cmu.edu>, J. Zico Kolter <zkolter@cs.cmu.edu>, Vladlen Koltun <vkoltun@gmail.edu>. To represent convolutional networks, we describe a generic temporal convolutional network (TCN) architecture that is applied across all tasks. This architecture is informed by recent research, but is deliberately kept simple, combining some of the best practices of modern convolutional archi- tectures. It is compared to canonical recurrent architectures such as LSTMs and GRUs. The results suggest that TCNs convincingly outperform baseline recurrent architectures across a broad range of se- quence modeling tasks. This is particularly notable because the tasks include diverse benchmarks that have commonly been used to evaluate recurrent network designs (Chung et al., 2014; Pascanu et al., 2014; Jozefowicz et al., 2015; Zhang et al., 2016). This indicates that the recent successes of convolutional architectures in applications such as audio processing are not con\ufb01ned to these domains. To further understand these results, we analyze more deeply the memory retention characteristics of recurrent networks. We show that despite the theoretical ability of recurrent architectures to capture in\ufb01nitely long history, TCNs exhibit An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling substantially longer memory, and are thus more suitable for domains where a long history is required. To our knowledge, the presented study is the most extensive systematic comparison of convolutional and recurrent archi- tectures on sequence modeling tasks. The results suggest that the common association between sequence modeling and recurrent networks should be reconsidered. The TCN architecture appears not only more accurate than canoni- cal recurrent networks such as LSTMs and GRUs, but also simpler and clearer. It may therefore be a more appropri- ate starting point in the application of deep networks to sequences.",
    "raw_text": "1. Introduction Deep learning practitioners commonly regard recurrent ar- chitectures as the default starting point for sequence model- ing tasks. The sequence modeling chapter in the canonical textbook on deep learning is titled \u201cSequence Modeling: Recurrent and Recursive Nets\u201d (Goodfellow et al., 2016), capturing the common association of sequence modeling and recurrent architectures. A well-regarded recent online course on \u201cSequence Models\u201d focuses exclusively on recur- rent architectures (Ng, 2018). 1Machine Learning Department, Carnegie Mellon Univer- sity, Pittsburgh, PA, USA 2Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA 3Intel Labs, Santa Clara, CA, USA. Correspondence to: Shaojie Bai <shaojieb@cs.cmu.edu>, J. Zico Kolter <zkolter@cs.cmu.edu>, Vladlen Koltun <vkoltun@gmail.edu>. To represent convolutional networks, we describe a generic temporal convolutional network (TCN) architecture that is applied across all tasks. This architecture is informed by recent research, but is deliberately kept simple, combining some of the best practices of modern convolutional archi- tectures. It is compared to canonical recurrent architectures such as LSTMs and GRUs. The results suggest that TCNs convincingly outperform baseline recurrent architectures across a broad range of se- quence modeling tasks. This is particularly notable because the tasks include diverse benchmarks that have commonly been used to evaluate recurrent network designs (Chung et al., 2014; Pascanu et al., 2014; Jozefowicz et al., 2015; Zhang et al., 2016). This indicates that the recent successes of convolutional architectures in applications such as audio processing are not con\ufb01ned to these domains. To further understand these results, we analyze more deeply the memory retention characteristics of recurrent networks. We show that despite the theoretical ability of recurrent architectures to capture in\ufb01nitely long history, TCNs exhibit An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling substantially longer memory, and are thus more suitable for domains where a long history is required. To our knowledge, the presented study is the most extensive systematic comparison of convolutional and recurrent archi- tectures on sequence modeling tasks. The results suggest that the common association between sequence modeling and recurrent networks should be reconsidered. The TCN architecture appears not only more accurate than canoni- cal recurrent networks such as LSTMs and GRUs, but also simpler and clearer. It may therefore be a more appropri- ate starting point in the application of deep networks to sequences."
  },
  {
    "chunk_id": "4d83e7c2-aee3-4c60-bef2-ad9337d487e4",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 2,
    "retrieval_text": "2. Background Convolutional networks (LeCun et al., 1989) have been applied to sequences for decades (Sejnowski & Rosen- berg, 1987; Hinton, 1989). They were used prominently for speech recognition in the 80s and 90s (Waibel et al., 1989; Bottou et al., 1990). ConvNets were subsequently applied to NLP tasks such as part-of-speech tagging and semantic role labelling (Collobert & Weston, 2008; Col- lobert et al., 2011; dos Santos & Zadrozny, 2014). More recently, convolutional networks were applied to sentence classi\ufb01cation (Kalchbrenner et al., 2014; Kim, 2014) and document classi\ufb01cation (Zhang et al., 2015; Conneau et al., 2017; Johnson & Zhang, 2015; 2017). Particularly inspiring for our work are the recent applications of convolutional architectures to machine translation (Kalchbrenner et al., 2016; Gehring et al., 2017a;b), audio synthesis (van den Oord et al., 2016), and language modeling (Dauphin et al., 2017). Recurrent networks are dedicated sequence models that maintain a vector of hidden activations that are propagated through time (Elman, 1990; Werbos, 1990; Graves, 2012). This family of architectures has gained tremendous pop- ularity due to prominent applications to language mod- eling (Sutskever et al., 2011; Graves, 2013; Hermans & Schrauwen, 2013) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015). The intuitive appeal of re- current modeling is that the hidden state can act as a rep- resentation of everything that has been seen so far in the sequence. Basic RNN architectures are notoriously dif\ufb01cult to train (Bengio et al., 1994; Pascanu et al., 2013) and more elaborate architectures are commonly used instead, such as the LSTM (Hochreiter & Schmidhuber, 1997) and the GRU (Cho et al., 2014). Many other architectural innova- tions and training techniques for recurrent networks have been introduced and continue to be actively explored (El Hihi & Bengio, 1995; Schuster & Paliwal, 1997; Gers et al., 2002; Koutnik et al., 2014; Le et al., 2015; Ba et al., 2016; Wu et al., 2016; Krueger et al., 2017; Merity et al., 2017; Campos et al., 2018). Multiple empirical studies have been conducted to evaluate the effectiveness of different recurrent architectures. These studies have been motivated in part by the many degrees of freedom in the design of such architectures. Chung et al. (2014) compared different types of recurrent units (LSTM vs. GRU) on the task of polyphonic music modeling. Pas- canu et al. (2014) explored different ways to construct deep RNNs and evaluated the performance of different architec- tures on polyphonic music modeling, character-level lan- guage modeling, and word-level language modeling. Joze- fowicz et al. (2015) searched through more than ten thou- sand different RNN architectures and evaluated their perfor- mance on various tasks. They concluded that if there were \u201carchitectures much better than the LSTM\u201d, then they were \u201cnot trivial to \ufb01nd\u201d. Greff et al. (2017) benchmarked the performance of eight LSTM variants on speech recognition, handwriting recognition, and polyphonic music modeling. They also found that \u201cnone of the variants can improve upon the standard LSTM architecture signi\ufb01cantly\u201d. Zhang et al. (2016) systematically analyzed the connecting architectures of RNNs and evaluated different architectures on character- level language modeling and on synthetic stress tests. Melis et al. (2018) benchmarked LSTM-based architectures on word-level and character-level language modeling, and con- cluded that \u201cLSTMs outperform the more recent models\u201d. Other recent works have aimed to combine aspects of RNN and CNN architectures. This includes the Convolutional LSTM (Shi et al., 2015), which replaces the fully-connected layers in an LSTM with convolutional layers to allow for additional structure in the recurrent layers; the Quasi-RNN model (Bradbury et al., 2017) that interleaves convolutional layers with simple recurrent layers; and the dilated RNN (Chang et al., 2017), which adds dilations to recurrent ar- chitectures. While these combinations show promise in combining the desirable aspects of both types of architec- tures, our study here focuses on a comparison of generic convolutional and recurrent architectures. While there have been multiple thorough evaluations of RNN architectures on representative sequence modeling tasks, we are not aware of a similarly thorough compari- son of convolutional and recurrent approaches to sequence modeling. (Yin et al. (2017) have reported a comparison of convolutional and recurrent networks for sentence-level and document-level classi\ufb01cation tasks. In contrast, se- quence modeling calls for architectures that can synthesize whole sequences, element by element.) Such comparison is particularly intriguing in light of the aforementioned re- cent success of convolutional architectures in this domain. Our work aims to compare generic convolutional and re- current architectures on typical sequence modeling tasks that are commonly used to benchmark RNN variants them- selves (Hermans & Schrauwen, 2013; Le et al., 2015; Joze- fowicz et al., 2015; Zhang et al., 2016). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
    "raw_text": "2. Background Convolutional networks (LeCun et al., 1989) have been applied to sequences for decades (Sejnowski & Rosen- berg, 1987; Hinton, 1989). They were used prominently for speech recognition in the 80s and 90s (Waibel et al., 1989; Bottou et al., 1990). ConvNets were subsequently applied to NLP tasks such as part-of-speech tagging and semantic role labelling (Collobert & Weston, 2008; Col- lobert et al., 2011; dos Santos & Zadrozny, 2014). More recently, convolutional networks were applied to sentence classi\ufb01cation (Kalchbrenner et al., 2014; Kim, 2014) and document classi\ufb01cation (Zhang et al., 2015; Conneau et al., 2017; Johnson & Zhang, 2015; 2017). Particularly inspiring for our work are the recent applications of convolutional architectures to machine translation (Kalchbrenner et al., 2016; Gehring et al., 2017a;b), audio synthesis (van den Oord et al., 2016), and language modeling (Dauphin et al., 2017). Recurrent networks are dedicated sequence models that maintain a vector of hidden activations that are propagated through time (Elman, 1990; Werbos, 1990; Graves, 2012). This family of architectures has gained tremendous pop- ularity due to prominent applications to language mod- eling (Sutskever et al., 2011; Graves, 2013; Hermans & Schrauwen, 2013) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015). The intuitive appeal of re- current modeling is that the hidden state can act as a rep- resentation of everything that has been seen so far in the sequence. Basic RNN architectures are notoriously dif\ufb01cult to train (Bengio et al., 1994; Pascanu et al., 2013) and more elaborate architectures are commonly used instead, such as the LSTM (Hochreiter & Schmidhuber, 1997) and the GRU (Cho et al., 2014). Many other architectural innova- tions and training techniques for recurrent networks have been introduced and continue to be actively explored (El Hihi & Bengio, 1995; Schuster & Paliwal, 1997; Gers et al., 2002; Koutnik et al., 2014; Le et al., 2015; Ba et al., 2016; Wu et al., 2016; Krueger et al., 2017; Merity et al., 2017; Campos et al., 2018). Multiple empirical studies have been conducted to evaluate the effectiveness of different recurrent architectures. These studies have been motivated in part by the many degrees of freedom in the design of such architectures. Chung et al. (2014) compared different types of recurrent units (LSTM vs. GRU) on the task of polyphonic music modeling. Pas- canu et al. (2014) explored different ways to construct deep RNNs and evaluated the performance of different architec- tures on polyphonic music modeling, character-level lan- guage modeling, and word-level language modeling. Joze- fowicz et al. (2015) searched through more than ten thou- sand different RNN architectures and evaluated their perfor- mance on various tasks. They concluded that if there were \u201carchitectures much better than the LSTM\u201d, then they were \u201cnot trivial to \ufb01nd\u201d. Greff et al. (2017) benchmarked the performance of eight LSTM variants on speech recognition, handwriting recognition, and polyphonic music modeling. They also found that \u201cnone of the variants can improve upon the standard LSTM architecture signi\ufb01cantly\u201d. Zhang et al. (2016) systematically analyzed the connecting architectures of RNNs and evaluated different architectures on character- level language modeling and on synthetic stress tests. Melis et al. (2018) benchmarked LSTM-based architectures on word-level and character-level language modeling, and con- cluded that \u201cLSTMs outperform the more recent models\u201d. Other recent works have aimed to combine aspects of RNN and CNN architectures. This includes the Convolutional LSTM (Shi et al., 2015), which replaces the fully-connected layers in an LSTM with convolutional layers to allow for additional structure in the recurrent layers; the Quasi-RNN model (Bradbury et al., 2017) that interleaves convolutional layers with simple recurrent layers; and the dilated RNN (Chang et al., 2017), which adds dilations to recurrent ar- chitectures. While these combinations show promise in combining the desirable aspects of both types of architec- tures, our study here focuses on a comparison of generic convolutional and recurrent architectures. While there have been multiple thorough evaluations of RNN architectures on representative sequence modeling tasks, we are not aware of a similarly thorough compari- son of convolutional and recurrent approaches to sequence modeling. (Yin et al. (2017) have reported a comparison of convolutional and recurrent networks for sentence-level and document-level classi\ufb01cation tasks. In contrast, se- quence modeling calls for architectures that can synthesize whole sequences, element by element.) Such comparison is particularly intriguing in light of the aforementioned re- cent success of convolutional architectures in this domain. Our work aims to compare generic convolutional and re- current architectures on typical sequence modeling tasks that are commonly used to benchmark RNN variants them- selves (Hermans & Schrauwen, 2013; Le et al., 2015; Joze- fowicz et al., 2015; Zhang et al., 2016). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling"
  },
  {
    "chunk_id": "95d289eb-839d-4584-8bc0-8951beabcb20",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 3,
    "retrieval_text": "3. Temporal Convolutional Networks We begin by describing a generic architecture for convo- lutional sequence prediction. Our aim is to distill the best practices in convolutional network design into a simple architecture that can serve as a convenient but powerful starting point. We refer to the presented architecture as a temporal convolutional network (TCN), emphasizing that we adopt this term not as a label for a truly new architecture, but as a simple descriptive term for a family of architec- tures. (Note that the term has been used before (Lea et al., 2017).) The distinguishing characteristics of TCNs are: 1) the convolutions in the architecture are causal, meaning that there is no information \u201cleakage\u201d from future to past; 2) the architecture can take a sequence of any length and map it to an output sequence of the same length, just as with an RNN. Beyond this, we emphasize how to build very long effective history sizes (i.e., the ability for the networks to look very far into the past to make a prediction) using a combination of very deep networks (augmented with residual layers) and dilated convolutions. Our architecture is informed by recent convolutional ar- chitectures for sequential data (van den Oord et al., 2016; Kalchbrenner et al., 2016; Dauphin et al., 2017; Gehring et al., 2017a;b), but is distinct from all of them and was designed from \ufb01rst principles to combine simplicity, autore- gressive prediction, and very long memory. For example, the TCN is much simpler than WaveNet (van den Oord et al., 2016) (no skip connections across layers, conditioning, con- text stacking, or gated activations). This formalism encompasses many settings such as auto- regressive prediction (where we try to predict some signal given its past) by setting the target output to be simply the input shifted by one time step. It does not, however, directly capture domains such as machine translation, or sequence- to-sequence prediction in general, since in these cases the entire input sequence (including \u201cfuture\u201d states) can be used to predict each output (though the techniques can naturally be extended to work in such settings).",
    "raw_text": "3. Temporal Convolutional Networks We begin by describing a generic architecture for convo- lutional sequence prediction. Our aim is to distill the best practices in convolutional network design into a simple architecture that can serve as a convenient but powerful starting point. We refer to the presented architecture as a temporal convolutional network (TCN), emphasizing that we adopt this term not as a label for a truly new architecture, but as a simple descriptive term for a family of architec- tures. (Note that the term has been used before (Lea et al., 2017).) The distinguishing characteristics of TCNs are: 1) the convolutions in the architecture are causal, meaning that there is no information \u201cleakage\u201d from future to past; 2) the architecture can take a sequence of any length and map it to an output sequence of the same length, just as with an RNN. Beyond this, we emphasize how to build very long effective history sizes (i.e., the ability for the networks to look very far into the past to make a prediction) using a combination of very deep networks (augmented with residual layers) and dilated convolutions. Our architecture is informed by recent convolutional ar- chitectures for sequential data (van den Oord et al., 2016; Kalchbrenner et al., 2016; Dauphin et al., 2017; Gehring et al., 2017a;b), but is distinct from all of them and was designed from \ufb01rst principles to combine simplicity, autore- gressive prediction, and very long memory. For example, the TCN is much simpler than WaveNet (van den Oord et al., 2016) (no skip connections across layers, conditioning, con- text stacking, or gated activations). This formalism encompasses many settings such as auto- regressive prediction (where we try to predict some signal given its past) by setting the target output to be simply the input shifted by one time step. It does not, however, directly capture domains such as machine translation, or sequence- to-sequence prediction in general, since in these cases the entire input sequence (including \u201cfuture\u201d states) can be used to predict each output (though the techniques can naturally be extended to work in such settings)."
  },
  {
    "chunk_id": "8c2cf928-115a-455a-8bdd-5477ed3092e8",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 3,
    "retrieval_text": "3.2. Causal Convolutions As mentioned above, the TCN is based upon two principles: the fact that the network produces an output of the same length as the input, and the fact that there can be no leakage from the future into the past. To accomplish the \ufb01rst point, the TCN uses a 1D fully-convolutional network (FCN) ar- chitecture (Long et al., 2015), where each hidden layer is the same length as the input layer, and zero padding of length (kernel size \u2212 1) is added to keep subsequent layers the same length as previous ones. To achieve the second point, the TCN uses causal convolutions, convolutions where an output at time t is convolved only with elements from time t and earlier in the previous layer. To put it simply: TCN = 1D FCN + causal convolutions. Note that this is essentially the same architecture as the time delay neural network proposed nearly 30 years ago by Waibel et al. (1989), with the sole tweak of zero padding to ensure equal sizes of all layers. Compared to the language modeling architecture of Dauphin et al. (2017), TCNs do not use gating mechanisms and have much longer memory. 3.1. Sequence Modeling Before de\ufb01ning the network structure, we highlight the na- ture of the sequence modeling task. Suppose that we are given an input sequence x0,...,xT, and wish to predict some corresponding outputs y0,...,yT at each time. The key constraint is that to predict the output yt for some time t, we are constrained to only use those inputs that have been previously observed: x0,...,xt. Formally, a sequence modeling network is any function f : X T+1 \u2192 YT+1 that produces the mapping \u02c6y0,..., \u02c6yT = f(x0,...,xT) (1) if it satis\ufb01es the causal constraint that yt depends only on x0,...,xt and not on any \u201cfuture\u201d inputs xt+1,...,xT. The goal of learning in the sequence modeling setting is to \ufb01nd a network f that minimizes some expected loss between the actual outputs and the predictions, L(y0,...,yT,f(x0,...,xT)), where the sequences and outputs are drawn according to some distribution. A major disadvantage of this basic design is that in order to achieve a long effective history size, we need an extremely deep network or very large \ufb01lters, neither of which were particularly feasible when the methods were \ufb01rst introduced. Thus, in the following sections, we describe how techniques from modern convolutional architectures can be integrated into a TCN to allow for both very deep networks and very long effective history.",
    "raw_text": "3.2. Causal Convolutions As mentioned above, the TCN is based upon two principles: the fact that the network produces an output of the same length as the input, and the fact that there can be no leakage from the future into the past. To accomplish the \ufb01rst point, the TCN uses a 1D fully-convolutional network (FCN) ar- chitecture (Long et al., 2015), where each hidden layer is the same length as the input layer, and zero padding of length (kernel size \u2212 1) is added to keep subsequent layers the same length as previous ones. To achieve the second point, the TCN uses causal convolutions, convolutions where an output at time t is convolved only with elements from time t and earlier in the previous layer. To put it simply: TCN = 1D FCN + causal convolutions. Note that this is essentially the same architecture as the time delay neural network proposed nearly 30 years ago by Waibel et al. (1989), with the sole tweak of zero padding to ensure equal sizes of all layers. Compared to the language modeling architecture of Dauphin et al. (2017), TCNs do not use gating mechanisms and have much longer memory. 3.1. Sequence Modeling Before de\ufb01ning the network structure, we highlight the na- ture of the sequence modeling task. Suppose that we are given an input sequence x0,...,xT, and wish to predict some corresponding outputs y0,...,yT at each time. The key constraint is that to predict the output yt for some time t, we are constrained to only use those inputs that have been previously observed: x0,...,xt. Formally, a sequence modeling network is any function f : X T+1 \u2192 YT+1 that produces the mapping \u02c6y0,..., \u02c6yT = f(x0,...,xT) (1) if it satis\ufb01es the causal constraint that yt depends only on x0,...,xt and not on any \u201cfuture\u201d inputs xt+1,...,xT. The goal of learning in the sequence modeling setting is to \ufb01nd a network f that minimizes some expected loss between the actual outputs and the predictions, L(y0,...,yT,f(x0,...,xT)), where the sequences and outputs are drawn according to some distribution. A major disadvantage of this basic design is that in order to achieve a long effective history size, we need an extremely deep network or very large \ufb01lters, neither of which were particularly feasible when the methods were \ufb01rst introduced. Thus, in the following sections, we describe how techniques from modern convolutional architectures can be integrated into a TCN to allow for both very deep networks and very long effective history."
  },
  {
    "chunk_id": "9b9aaf78-6b20-4447-9134-f39d33075790",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 3,
    "retrieval_text": "3.3. Dilated Convolutions A simple causal convolution is only able to look back at a history with size linear in the depth of the network. This makes it challenging to apply the aforementioned causal con- volution on sequence tasks, especially those requiring longer history. Our solution here, following the work of van den Oord et al. (2016), is to employ dilated convolutions that enable an exponentially large receptive \ufb01eld (Yu & Koltun, 2016). More formally, for a 1-D sequence input x \u2208 Rn and a \ufb01lter f : {0,...,k \u22121} \u2192 R, the dilated convolution operation F on element s of the sequence is de\ufb01ned as > I F(s) = (x*a f)(s) =) f(t) Xs\u2014ai (2) I \u00b0 An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (a) (b) (c) Figure 1. Architectural elements in a TCN. (a) A dilated causal convolution with dilation factors d = 1,2,4 and \ufb01lter size k = 3. The receptive \ufb01eld is able to cover all values from the input sequence. (b) TCN residual block. An 1x1 convolution is added when residual input and output have different dimensions. (c) An example of residual connection in a TCN. The blue lines are \ufb01lters in the residual function, and the green lines are identity mappings. where d is the dilation factor, k is the \ufb01lter size, and s\u2212d\u00b7i accounts for the direction of the past. Dilation is thus equiv- alent to introducing a \ufb01xed step between every two adjacent \ufb01lter taps. When d = 1, a dilated convolution reduces to a regular convolution. Using larger dilation enables an output at the top level to represent a wider range of inputs, thus effectively expanding the receptive \ufb01eld of a ConvNet. This gives us two ways to increase the receptive \ufb01eld of the TCN: choosing larger \ufb01lter sizes k and increasing the dila- tion factor d, where the effective history of one such layer is (k \u2212 1)d. As is common when using dilated convolutions, we increase d exponentially with the depth of the network (i.e., d = O(2i) at level i of the network). This ensures that there is some \ufb01lter that hits each input within the effective history, while also allowing for an extremely large effective history using deep networks. We provide an illustration in Figure 1(a). The residual block for our baseline TCN is shown in Fig- ure 1(b). Within a residual block, the TCN has two layers of dilated causal convolution and non-linearity, for which we used the recti\ufb01ed linear unit (ReLU) (Nair & Hinton, 2010). For normalization, we applied weight normaliza- tion (Salimans & Kingma, 2016) to the convolutional \ufb01lters. In addition, a spatial dropout (Srivastava et al., 2014) was added after each dilated convolution for regularization: at each training step, a whole channel is zeroed out. However, whereas in standard ResNet the input is added directly to the output of the residual function, in TCN (and ConvNets in general) the input and output could have differ- ent widths. To account for discrepant input-output widths, we use an additional 1x1 convolution to ensure that element- wise addition \u2295 receives tensors of the same shape (see Figure 1(b,c)).",
    "raw_text": "3.3. Dilated Convolutions A simple causal convolution is only able to look back at a history with size linear in the depth of the network. This makes it challenging to apply the aforementioned causal con- volution on sequence tasks, especially those requiring longer history. Our solution here, following the work of van den Oord et al. (2016), is to employ dilated convolutions that enable an exponentially large receptive \ufb01eld (Yu & Koltun, 2016). More formally, for a 1-D sequence input x \u2208 Rn and a \ufb01lter f : {0,...,k \u22121} \u2192 R, the dilated convolution operation F on element s of the sequence is de\ufb01ned as > I F(s) = (x*a f)(s) =) f(t) Xs\u2014ai (2) I \u00b0 An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (a) (b) (c) Figure 1. Architectural elements in a TCN. (a) A dilated causal convolution with dilation factors d = 1,2,4 and \ufb01lter size k = 3. The receptive \ufb01eld is able to cover all values from the input sequence. (b) TCN residual block. An 1x1 convolution is added when residual input and output have different dimensions. (c) An example of residual connection in a TCN. The blue lines are \ufb01lters in the residual function, and the green lines are identity mappings. where d is the dilation factor, k is the \ufb01lter size, and s\u2212d\u00b7i accounts for the direction of the past. Dilation is thus equiv- alent to introducing a \ufb01xed step between every two adjacent \ufb01lter taps. When d = 1, a dilated convolution reduces to a regular convolution. Using larger dilation enables an output at the top level to represent a wider range of inputs, thus effectively expanding the receptive \ufb01eld of a ConvNet. This gives us two ways to increase the receptive \ufb01eld of the TCN: choosing larger \ufb01lter sizes k and increasing the dila- tion factor d, where the effective history of one such layer is (k \u2212 1)d. As is common when using dilated convolutions, we increase d exponentially with the depth of the network (i.e., d = O(2i) at level i of the network). This ensures that there is some \ufb01lter that hits each input within the effective history, while also allowing for an extremely large effective history using deep networks. We provide an illustration in Figure 1(a). The residual block for our baseline TCN is shown in Fig- ure 1(b). Within a residual block, the TCN has two layers of dilated causal convolution and non-linearity, for which we used the recti\ufb01ed linear unit (ReLU) (Nair & Hinton, 2010). For normalization, we applied weight normaliza- tion (Salimans & Kingma, 2016) to the convolutional \ufb01lters. In addition, a spatial dropout (Srivastava et al., 2014) was added after each dilated convolution for regularization: at each training step, a whole channel is zeroed out. However, whereas in standard ResNet the input is added directly to the output of the residual function, in TCN (and ConvNets in general) the input and output could have differ- ent widths. To account for discrepant input-output widths, we use an additional 1x1 convolution to ensure that element- wise addition \u2295 receives tensors of the same shape (see Figure 1(b,c))."
  },
  {
    "chunk_id": "c32e041a-6337-4d6f-932b-42b3c58654de",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 4,
    "retrieval_text": "3.5. Discussion 3.4. Residual Connections A residual block (He et al., 2016) contains a branch leading out to a series of transformations F, whose outputs are added to the input x of the block: o = Activation(x + F(x)) (3) This effectively allows layers to learn modi\ufb01cations to the identity mapping rather than the entire transformation, which has repeatedly been shown to bene\ufb01t very deep net- works. Since a TCN\u2019s receptive \ufb01eld depends on the network depth n as well as \ufb01lter size k and dilation factor d, stabilization of deeper and larger TCNs becomes important. For example, in a case where the prediction could depend on a history of size 212 and a high-dimensional input sequence, a network of up to 12 layers could be needed. Each layer, more speci\ufb01cally, consists of multiple \ufb01lters for feature extraction. In our design of the generic TCN model, we therefore employ a generic residual module in place of a convolutional layer. We conclude this section by listing several advantages and disadvantages of using TCNs for sequence modeling. \u2022 Parallelism. Unlike in RNNs where the predictions for later timesteps must wait for their predecessors to com- plete, convolutions can be done in parallel since the same \ufb01lter is used in each layer. Therefore, in both training and evaluation, a long input sequence can be processed as a whole in TCN, instead of sequentially as in RNN. \u2022 Flexible receptive \ufb01eld size. A TCN can change its re- ceptive \ufb01eld size in multiple ways. For instance, stacking more dilated (causal) convolutional layers, using larger dilation factors, or increasing the \ufb01lter size are all viable options (with possibly different interpretations). TCNs thus afford better control of the model\u2019s memory size, and are easy to adapt to different domains. \u2022 Stable gradients. Unlike recurrent architectures, TCN has a backpropagation path different from the temporal direction of the sequence. TCN thus avoids the problem An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling of exploding/vanishing gradients, which is a major issue for RNNs (and which led to the development of LSTM, GRU, HF-RNN (Martens & Sutskever, 2011), etc.). \u2022 Low memory requirement for training. Especially in the case of a long input sequence, LSTMs and GRUs can easily use up a lot of memory to store the partial results for their multiple cell gates. However, in a TCN the \ufb01lters are shared across a layer, with the backpropagation path depending only on network depth. Therefore in practice, we found gated RNNs likely to use up to a multiplicative factor more memory than TCNs. \u2022 Variable length inputs. Just like RNNs, which model inputs with variable lengths in a recurrent way, TCNs can also take in inputs of arbitrary lengths by sliding the 1D convolutional kernels. This means that TCNs can be adopted as drop-in replacements for RNNs for sequential data of arbitrary length. are marked by 1. Simply predicting the sum to be 1 should give an MSE of about 0.1767. First introduced by Hochreiter & Schmidhuber (1997), the adding problem has been used repeatedly as a stress test for sequence models (Martens & Sutskever, 2011; Pascanu et al., 2013; Le et al., 2015; Arjovsky et al., 2016; Zhang et al., 2016). Sequential MNIST and P-MNIST. Sequential MNIST is frequently used to test a recurrent network\u2019s ability to retain information from the distant past (Le et al., 2015; Zhang et al., 2016; Wisdom et al., 2016; Cooijmans et al., 2016; Krueger et al., 2017; Jing et al., 2017). In this task, MNIST images (LeCun et al., 1998) are presented to the model as a 784\u00d71 sequence for digit classi\ufb01cation. In the more challenging P-MNIST setting, the order of the sequence is permuted at random (Le et al., 2015; Arjovsky et al., 2016; Wisdom et al., 2016; Krueger et al., 2017). There are also two notable disadvantages to using TCNs. \u2022 Data storage during evaluation. In evaluation/testing, RNNs only need to maintain a hidden state and take in a current input xt in order to generate a prediction. In other words, a \u201csummary\u201d of the entire history is provided by the \ufb01xed-length set of vectors ht, and the actual observed sequence can be discarded. In contrast, TCNs need to take in the raw sequence up to the effective history length, thus possibly requiring more memory during evaluation. \u2022 Potential parameter change for a transfer of domain. Different domains can have different requirements on the amount of history the model needs in order to predict. Therefore, when transferring a model from a domain where only little memory is needed (i.e., small k and d) to a domain where much longer memory is required (i.e., much larger k and d), TCN may perform poorly for not having a suf\ufb01ciently large receptive \ufb01eld.",
    "raw_text": "3.5. Discussion 3.4. Residual Connections A residual block (He et al., 2016) contains a branch leading out to a series of transformations F, whose outputs are added to the input x of the block: o = Activation(x + F(x)) (3) This effectively allows layers to learn modi\ufb01cations to the identity mapping rather than the entire transformation, which has repeatedly been shown to bene\ufb01t very deep net- works. Since a TCN\u2019s receptive \ufb01eld depends on the network depth n as well as \ufb01lter size k and dilation factor d, stabilization of deeper and larger TCNs becomes important. For example, in a case where the prediction could depend on a history of size 212 and a high-dimensional input sequence, a network of up to 12 layers could be needed. Each layer, more speci\ufb01cally, consists of multiple \ufb01lters for feature extraction. In our design of the generic TCN model, we therefore employ a generic residual module in place of a convolutional layer. We conclude this section by listing several advantages and disadvantages of using TCNs for sequence modeling. \u2022 Parallelism. Unlike in RNNs where the predictions for later timesteps must wait for their predecessors to com- plete, convolutions can be done in parallel since the same \ufb01lter is used in each layer. Therefore, in both training and evaluation, a long input sequence can be processed as a whole in TCN, instead of sequentially as in RNN. \u2022 Flexible receptive \ufb01eld size. A TCN can change its re- ceptive \ufb01eld size in multiple ways. For instance, stacking more dilated (causal) convolutional layers, using larger dilation factors, or increasing the \ufb01lter size are all viable options (with possibly different interpretations). TCNs thus afford better control of the model\u2019s memory size, and are easy to adapt to different domains. \u2022 Stable gradients. Unlike recurrent architectures, TCN has a backpropagation path different from the temporal direction of the sequence. TCN thus avoids the problem An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling of exploding/vanishing gradients, which is a major issue for RNNs (and which led to the development of LSTM, GRU, HF-RNN (Martens & Sutskever, 2011), etc.). \u2022 Low memory requirement for training. Especially in the case of a long input sequence, LSTMs and GRUs can easily use up a lot of memory to store the partial results for their multiple cell gates. However, in a TCN the \ufb01lters are shared across a layer, with the backpropagation path depending only on network depth. Therefore in practice, we found gated RNNs likely to use up to a multiplicative factor more memory than TCNs. \u2022 Variable length inputs. Just like RNNs, which model inputs with variable lengths in a recurrent way, TCNs can also take in inputs of arbitrary lengths by sliding the 1D convolutional kernels. This means that TCNs can be adopted as drop-in replacements for RNNs for sequential data of arbitrary length. are marked by 1. Simply predicting the sum to be 1 should give an MSE of about 0.1767. First introduced by Hochreiter & Schmidhuber (1997), the adding problem has been used repeatedly as a stress test for sequence models (Martens & Sutskever, 2011; Pascanu et al., 2013; Le et al., 2015; Arjovsky et al., 2016; Zhang et al., 2016). Sequential MNIST and P-MNIST. Sequential MNIST is frequently used to test a recurrent network\u2019s ability to retain information from the distant past (Le et al., 2015; Zhang et al., 2016; Wisdom et al., 2016; Cooijmans et al., 2016; Krueger et al., 2017; Jing et al., 2017). In this task, MNIST images (LeCun et al., 1998) are presented to the model as a 784\u00d71 sequence for digit classi\ufb01cation. In the more challenging P-MNIST setting, the order of the sequence is permuted at random (Le et al., 2015; Arjovsky et al., 2016; Wisdom et al., 2016; Krueger et al., 2017). There are also two notable disadvantages to using TCNs. \u2022 Data storage during evaluation. In evaluation/testing, RNNs only need to maintain a hidden state and take in a current input xt in order to generate a prediction. In other words, a \u201csummary\u201d of the entire history is provided by the \ufb01xed-length set of vectors ht, and the actual observed sequence can be discarded. In contrast, TCNs need to take in the raw sequence up to the effective history length, thus possibly requiring more memory during evaluation. \u2022 Potential parameter change for a transfer of domain. Different domains can have different requirements on the amount of history the model needs in order to predict. Therefore, when transferring a model from a domain where only little memory is needed (i.e., small k and d) to a domain where much longer memory is required (i.e., much larger k and d), TCN may perform poorly for not having a suf\ufb01ciently large receptive \ufb01eld."
  },
  {
    "chunk_id": "19681428-a169-43fe-9854-3402a1204724",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 5,
    "retrieval_text": "4. Sequence Modeling Tasks Copy memory. In this task, each input sequence has length T + 20. The \ufb01rst 10 values are chosen randomly among the digits 1,...,8, with the rest being all zeros, except for the last 11 entries that are \ufb01lled with the digit \u20189\u2019 (the \ufb01rst \u20189\u2019 is a delimiter). The goal is to generate an output of the same length that is zero everywhere except the last 10 values after the delimiter, where the model is expected to repeat the 10 values it encountered at the start of the input. This task was used in prior works such as Zhang et al. (2016); Arjovsky et al. (2016); Wisdom et al. (2016); Jing et al. (2017). JSB Chorales and Nottingham. JSB Chorales (Allan & Williams, 2005) is a polyphonic music dataset consisting of the entire corpus of 382 four-part harmonized chorales by J. S. Bach. Each input is a sequence of elements. Each element is an 88-bit binary code that corresponds to the 88 keys on a piano, with 1 indicating a key that is pressed at a given time. Nottingham is a polyphonic music dataset based on a collection of 1,200 British and American folk tunes, and is much larger than JSB Chorales. JSB Chorales and Nottingham have been used in numerous empirical investigations of recurrent sequence modeling (Chung et al., 2014; Pascanu et al., 2014; Jozefowicz et al., 2015; Greff et al., 2017). The performance on both tasks is measured in terms of negative log-likelihood (NLL). We evaluate TCNs and RNNs on tasks that have been com- monly used to benchmark the performance of different RNN sequence modeling architectures (Hermans & Schrauwen, 2013; Chung et al., 2014; Pascanu et al., 2014; Le et al., 2015; Jozefowicz et al., 2015; Zhang et al., 2016). The intention is to conduct the evaluation on the \u201chome turf\u201d of RNN sequence models. We use a comprehensive set of synthetic stress tests along with real-world datasets from multiple domains. The adding problem. In this task, each input consists of a length-n sequence of depth 2, with all values randomly chosen in [0,1], and the second dimension being all zeros except for two elements that are marked by 1. The objective is to sum the two random values whose second dimensions PennTreebank. We used the PennTreebank (PTB) (Mar- cus et al., 1993) for both character-level and word-level language modeling. When used as a character-level lan- guage corpus, PTB contains 5,059K characters for training, 396K for validation, and 446K for testing, with an alphabet size of 50. When used as a word-level language corpus, PTB contains 888K words for training, 70K for validation, and 79K for testing, with a vocabulary size of 10K. This is a highly studied but relatively small language modeling dataset (Miyamoto & Cho, 2016; Krueger et al., 2017; Mer- ity et al., 2017). Wikitext-103. Wikitext-103 (Merity et al., 2016) is almost An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Table 1. Evaluation of TCNs and recurrent architectures on synthetic stress tests, polyphonic music modeling, character-level language modeling, and word-level language modeling. The generic TCN architecture outperforms canonical recurrent networks across a comprehensive suite of tasks and datasets. Current state-of-the-art results are listed in the supplement. \u00a9 means that lower is better. \u00bb means that higher is better. Sequence Modeling Task Model Size (+) Models LSTM GRU RNN TCN Seq. MNIST (accuracy\u201d) 70K 87.2 96.2 21.5 99.0 Permuted MNIST (accuracy) 70K 85.7 87.3 25.3 97.2 Adding problem T=600 (loss\u2018) 70K 0.164 5.3e-5 0.177 5.8e-5 Copy memory T=1000 (loss) 16K 0.0204 0.0197 0.0202 3.5e-5 Music JSB Chorales (loss) 300K 8.45 8.43 8.91 8.10 Music Nottingham (loss) 1M 3.29 3.46 4.05 3.07 Word-level PTB (perplexity\u2019) 13M 78.93 92.48 114.50 88.68 Word-level Wiki-103 (perplexity) - 48.4 - - 45.19 Word-level LAMBADA (perplexity) - 4186 - 14725 1279 Char-level PTB (bpe\u2019) 3M 1.36 1.37 1.48 1.31 Char-level text8 (bpc) 5M 1.50 1.53 1.69 1.45 110 times as large as PTB, featuring a vocabulary size of about 268K. The dataset contains 28K Wikipedia articles (about 103 million words) for training, 60 articles (about 218K words) for validation, and 60 articles (246K words) for testing. This is a more representative and realistic dataset than PTB, with a much larger vocabulary that includes many rare words, and has been used in Merity et al. (2016); Grave et al. (2017); Dauphin et al. (2017). LAMBADA. Introduced by Paperno et al. (2016), LAM- BADA is a dataset comprising 10K passages extracted from novels, with an average of 4.6 sentences as context, and 1 tar- get sentence the last word of which is to be predicted. This dataset was built so that a person can easily guess the miss- ing word when given the context sentences, but not when given only the target sentence without the context sentences. Most of the existing models fail on LAMBADA (Paperno et al., 2016; Grave et al., 2017). In general, better results on LAMBADA indicate that a model is better at capturing information from longer and broader context. The training data for LAMBADA is the full text of 2,662 novels with more than 200M words. The vocabulary size is about 93K. text8. We also used the text8 dataset for character-level language modeling (Mikolov et al., 2012). text8 is about 20 times larger than PTB, with about 100M characters from Wikipedia (90M for training, 5M for validation, and 5M for testing). The corpus contains 27 unique alphabets.",
    "raw_text": "4. Sequence Modeling Tasks Copy memory. In this task, each input sequence has length T + 20. The \ufb01rst 10 values are chosen randomly among the digits 1,...,8, with the rest being all zeros, except for the last 11 entries that are \ufb01lled with the digit \u20189\u2019 (the \ufb01rst \u20189\u2019 is a delimiter). The goal is to generate an output of the same length that is zero everywhere except the last 10 values after the delimiter, where the model is expected to repeat the 10 values it encountered at the start of the input. This task was used in prior works such as Zhang et al. (2016); Arjovsky et al. (2016); Wisdom et al. (2016); Jing et al. (2017). JSB Chorales and Nottingham. JSB Chorales (Allan & Williams, 2005) is a polyphonic music dataset consisting of the entire corpus of 382 four-part harmonized chorales by J. S. Bach. Each input is a sequence of elements. Each element is an 88-bit binary code that corresponds to the 88 keys on a piano, with 1 indicating a key that is pressed at a given time. Nottingham is a polyphonic music dataset based on a collection of 1,200 British and American folk tunes, and is much larger than JSB Chorales. JSB Chorales and Nottingham have been used in numerous empirical investigations of recurrent sequence modeling (Chung et al., 2014; Pascanu et al., 2014; Jozefowicz et al., 2015; Greff et al., 2017). The performance on both tasks is measured in terms of negative log-likelihood (NLL). We evaluate TCNs and RNNs on tasks that have been com- monly used to benchmark the performance of different RNN sequence modeling architectures (Hermans & Schrauwen, 2013; Chung et al., 2014; Pascanu et al., 2014; Le et al., 2015; Jozefowicz et al., 2015; Zhang et al., 2016). The intention is to conduct the evaluation on the \u201chome turf\u201d of RNN sequence models. We use a comprehensive set of synthetic stress tests along with real-world datasets from multiple domains. The adding problem. In this task, each input consists of a length-n sequence of depth 2, with all values randomly chosen in [0,1], and the second dimension being all zeros except for two elements that are marked by 1. The objective is to sum the two random values whose second dimensions PennTreebank. We used the PennTreebank (PTB) (Mar- cus et al., 1993) for both character-level and word-level language modeling. When used as a character-level lan- guage corpus, PTB contains 5,059K characters for training, 396K for validation, and 446K for testing, with an alphabet size of 50. When used as a word-level language corpus, PTB contains 888K words for training, 70K for validation, and 79K for testing, with a vocabulary size of 10K. This is a highly studied but relatively small language modeling dataset (Miyamoto & Cho, 2016; Krueger et al., 2017; Mer- ity et al., 2017). Wikitext-103. Wikitext-103 (Merity et al., 2016) is almost An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Table 1. Evaluation of TCNs and recurrent architectures on synthetic stress tests, polyphonic music modeling, character-level language modeling, and word-level language modeling. The generic TCN architecture outperforms canonical recurrent networks across a comprehensive suite of tasks and datasets. Current state-of-the-art results are listed in the supplement. \u00a9 means that lower is better. \u00bb means that higher is better. Sequence Modeling Task Model Size (+) Models LSTM GRU RNN TCN Seq. MNIST (accuracy\u201d) 70K 87.2 96.2 21.5 99.0 Permuted MNIST (accuracy) 70K 85.7 87.3 25.3 97.2 Adding problem T=600 (loss\u2018) 70K 0.164 5.3e-5 0.177 5.8e-5 Copy memory T=1000 (loss) 16K 0.0204 0.0197 0.0202 3.5e-5 Music JSB Chorales (loss) 300K 8.45 8.43 8.91 8.10 Music Nottingham (loss) 1M 3.29 3.46 4.05 3.07 Word-level PTB (perplexity\u2019) 13M 78.93 92.48 114.50 88.68 Word-level Wiki-103 (perplexity) - 48.4 - - 45.19 Word-level LAMBADA (perplexity) - 4186 - 14725 1279 Char-level PTB (bpe\u2019) 3M 1.36 1.37 1.48 1.31 Char-level text8 (bpc) 5M 1.50 1.53 1.69 1.45 110 times as large as PTB, featuring a vocabulary size of about 268K. The dataset contains 28K Wikipedia articles (about 103 million words) for training, 60 articles (about 218K words) for validation, and 60 articles (246K words) for testing. This is a more representative and realistic dataset than PTB, with a much larger vocabulary that includes many rare words, and has been used in Merity et al. (2016); Grave et al. (2017); Dauphin et al. (2017). LAMBADA. Introduced by Paperno et al. (2016), LAM- BADA is a dataset comprising 10K passages extracted from novels, with an average of 4.6 sentences as context, and 1 tar- get sentence the last word of which is to be predicted. This dataset was built so that a person can easily guess the miss- ing word when given the context sentences, but not when given only the target sentence without the context sentences. Most of the existing models fail on LAMBADA (Paperno et al., 2016; Grave et al., 2017). In general, better results on LAMBADA indicate that a model is better at capturing information from longer and broader context. The training data for LAMBADA is the full text of 2,662 novels with more than 200M words. The vocabulary size is about 93K. text8. We also used the text8 dataset for character-level language modeling (Mikolov et al., 2012). text8 is about 20 times larger than PTB, with about 100M characters from Wikipedia (90M for training, 5M for validation, and 5M for testing). The corpus contains 27 unique alphabets."
  },
  {
    "chunk_id": "895cd6a8-1818-4937-8f95-a3dcce25a7a9",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 6,
    "retrieval_text": "5. Experiments experiments reported in this section used exactly the same TCN architecture, just varying the depth of the network n and occasionally the kernel size k so that the receptive \ufb01eld covers enough context for predictions. We use an expo- nential dilation d = 2i for layer i in the network, and the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.002 for TCN, unless otherwise noted. We also empiri- cally \ufb01nd that gradient clipping helped convergence, and we pick the maximum norm for clipping from [0.3,1]. When training recurrent models, we use grid search to \ufb01nd a good set of hyperparameters (in particular, optimizer, recurrent drop p \u2208 [0.05,0.5], learning rate, gradient clipping, and initial forget-gate bias), while keeping the network around the same size as TCN. No other architectural elaborations, such as gating mechanisms or skip connections, were added to either TCNs or RNNs. Additional details and controlled experiments are provided in the supplementary material. 5.1. Synopsis of Results A synopsis of the results is shown in Table 1. Note that on several of these tasks, the generic, canonical recurrent architectures we study (e.g., LSTM, GRU) are not the state- of-the-art. (See the supplement for more details.) With this caveat, the results strongly suggest that the generic TCN architecture with minimal tuning outperforms canonical re- current architectures across a broad variety of sequence modeling tasks that are commonly used to benchmark the performance of recurrent architectures themselves. We now analyze these results in more detail. We compare the generic TCN architecture described in Sec- tion 3 to canonical recurrent architectures, namely LSTM, GRU, and vanilla RNN, with standard regularizations. All An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (a) T = 200 (b) T = 600 (a) T = 500 (b) T = 1000 Figure 2. Results on the adding problem for different sequence lengths T. TCNs outperform recurrent architectures. Figure 4. Result on the copy memory task for different sequence lengths T. TCNs outperform recurrent architectures.",
    "raw_text": "5. Experiments experiments reported in this section used exactly the same TCN architecture, just varying the depth of the network n and occasionally the kernel size k so that the receptive \ufb01eld covers enough context for predictions. We use an expo- nential dilation d = 2i for layer i in the network, and the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.002 for TCN, unless otherwise noted. We also empiri- cally \ufb01nd that gradient clipping helped convergence, and we pick the maximum norm for clipping from [0.3,1]. When training recurrent models, we use grid search to \ufb01nd a good set of hyperparameters (in particular, optimizer, recurrent drop p \u2208 [0.05,0.5], learning rate, gradient clipping, and initial forget-gate bias), while keeping the network around the same size as TCN. No other architectural elaborations, such as gating mechanisms or skip connections, were added to either TCNs or RNNs. Additional details and controlled experiments are provided in the supplementary material. 5.1. Synopsis of Results A synopsis of the results is shown in Table 1. Note that on several of these tasks, the generic, canonical recurrent architectures we study (e.g., LSTM, GRU) are not the state- of-the-art. (See the supplement for more details.) With this caveat, the results strongly suggest that the generic TCN architecture with minimal tuning outperforms canonical re- current architectures across a broad variety of sequence modeling tasks that are commonly used to benchmark the performance of recurrent architectures themselves. We now analyze these results in more detail. We compare the generic TCN architecture described in Sec- tion 3 to canonical recurrent architectures, namely LSTM, GRU, and vanilla RNN, with standard regularizations. All An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (a) T = 200 (b) T = 600 (a) T = 500 (b) T = 1000 Figure 2. Results on the adding problem for different sequence lengths T. TCNs outperform recurrent architectures. Figure 4. Result on the copy memory task for different sequence lengths T. TCNs outperform recurrent architectures."
  },
  {
    "chunk_id": "2c1609ab-8020-4efa-9203-3597854a42b5",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 7,
    "retrieval_text": "5.3. Polyphonic Music and Language Modeling (a) Sequential MNIST (b) P-MNIST Figure 3. Results on Sequential MNIST and P-MNIST. TCNs out- perform recurrent architectures. We now discuss the results on polyphonic music modeling, character-level language modeling, and word-level language modeling. These domains are dominated by recurrent archi- tectures, with many specialized designs developed for these tasks (Zhang et al., 2016; Ha et al., 2017; Krueger et al., 2017; Grave et al., 2017; Greff et al., 2017; Merity et al., 2017). We mention some of these specialized architectures when useful, but our primary goal is to compare the generic TCN model to similarly generic recurrent architectures, be- fore domain-speci\ufb01c tuning. The results are summarized in Table 1. 5.2. Synthetic Stress Tests The adding problem. Convergence results for the adding problem, for problem sizes T = 200 and 600, are shown in Figure 2. All models were chosen to have roughly 70K parameters. TCNs quickly converged to a virtually perfect solution (i.e., MSE near 0). GRUs also performed quite well, albeit slower to converge than TCNs. LSTMs and vanilla RNNs performed signi\ufb01cantly worse. Sequential MNIST and P-MNIST. Convergence results on sequential and permuted MNIST, run over 10 epochs, are shown in Figure 3. All models were con\ufb01gured to have roughly 70K parameters. For both problems, TCNs sub- stantially outperform the recurrent architectures, both in terms of convergence and in \ufb01nal accuracy on the task. For P-MNIST, TCNs outperform state-of-the-art results (95.9%) based on recurrent networks with Zoneout and Recurrent BatchNorm (Cooijmans et al., 2016; Krueger et al., 2017). Copy memory. Convergence results on the copy mem- ory task are shown in Figure 4. TCNs quickly converge to correct answers, while LSTMs and GRUs simply con- verge to the same loss as predicting all zeros. In this case we also compare to the recently-proposed EURNN (Jing et al., 2017), which was highlighted to perform well on this task. While both TCN and EURNN perform well for sequence length T = 500, the TCN has a clear advantage for T = 1000 and longer (in terms of both loss and rate of convergence). Polyphonic music. On Nottingham and JSB Chorales, the TCN with virtually no tuning outperforms the recurrent models by a considerable margin, and even outperforms some enhanced recurrent architectures for this task such as HF-RNN (Boulanger-Lewandowski et al., 2012) and Diago- nal RNN (Subakan & Smaragdis, 2017). Note however that other models such as the Deep Belief Net LSTM perform better still (Vohra et al., 2015); we believe this is likely due to the fact that the datasets are relatively small, and thus the right regularization method or generative modeling proce- dure can improve performance signi\ufb01cantly. This is largely orthogonal to the RNN/TCN distinction, as a similar variant of TCN may well be possible. Word-level language modeling. Language modeling re- mains one of the primary applications of recurrent networks and many recent works have focused on optimizing LSTMs for this task (Krueger et al., 2017; Merity et al., 2017). Our implementation follows standard practice that ties the weights of encoder and decoder layers for both TCN and RNNs (Press & Wolf, 2016), which signi\ufb01cantly reduces the number of parameters in the model. For training, we use SGD and anneal the learning rate by a factor of 0.5 for both TCN and RNNs when validation accuracy plateaus. On the smaller PTB corpus, an optimized LSTM architec- ture (with recurrent and embedding dropout, etc.) outper- forms the TCN, while the TCN outperforms both GRU and vanilla RNN. However, on the much larger Wikitext-103 corpus and the LAMBADA dataset (Paperno et al., 2016), without any hyperparameter search, the TCN outperforms An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling the LSTM results of Grave et al. (2017), achieving much lower perplexities. Character-level language modeling. On character-level language modeling (PTB and text8, accuracy measured in bits per character), the generic TCN outperforms regular- ized LSTMs and GRUs as well as methods such as Norm- stabilized LSTMs (Krueger & Memisevic, 2015). (Special- ized architectures exist that outperform all of these, see the supplement.)",
    "raw_text": "5.3. Polyphonic Music and Language Modeling (a) Sequential MNIST (b) P-MNIST Figure 3. Results on Sequential MNIST and P-MNIST. TCNs out- perform recurrent architectures. We now discuss the results on polyphonic music modeling, character-level language modeling, and word-level language modeling. These domains are dominated by recurrent archi- tectures, with many specialized designs developed for these tasks (Zhang et al., 2016; Ha et al., 2017; Krueger et al., 2017; Grave et al., 2017; Greff et al., 2017; Merity et al., 2017). We mention some of these specialized architectures when useful, but our primary goal is to compare the generic TCN model to similarly generic recurrent architectures, be- fore domain-speci\ufb01c tuning. The results are summarized in Table 1. 5.2. Synthetic Stress Tests The adding problem. Convergence results for the adding problem, for problem sizes T = 200 and 600, are shown in Figure 2. All models were chosen to have roughly 70K parameters. TCNs quickly converged to a virtually perfect solution (i.e., MSE near 0). GRUs also performed quite well, albeit slower to converge than TCNs. LSTMs and vanilla RNNs performed signi\ufb01cantly worse. Sequential MNIST and P-MNIST. Convergence results on sequential and permuted MNIST, run over 10 epochs, are shown in Figure 3. All models were con\ufb01gured to have roughly 70K parameters. For both problems, TCNs sub- stantially outperform the recurrent architectures, both in terms of convergence and in \ufb01nal accuracy on the task. For P-MNIST, TCNs outperform state-of-the-art results (95.9%) based on recurrent networks with Zoneout and Recurrent BatchNorm (Cooijmans et al., 2016; Krueger et al., 2017). Copy memory. Convergence results on the copy mem- ory task are shown in Figure 4. TCNs quickly converge to correct answers, while LSTMs and GRUs simply con- verge to the same loss as predicting all zeros. In this case we also compare to the recently-proposed EURNN (Jing et al., 2017), which was highlighted to perform well on this task. While both TCN and EURNN perform well for sequence length T = 500, the TCN has a clear advantage for T = 1000 and longer (in terms of both loss and rate of convergence). Polyphonic music. On Nottingham and JSB Chorales, the TCN with virtually no tuning outperforms the recurrent models by a considerable margin, and even outperforms some enhanced recurrent architectures for this task such as HF-RNN (Boulanger-Lewandowski et al., 2012) and Diago- nal RNN (Subakan & Smaragdis, 2017). Note however that other models such as the Deep Belief Net LSTM perform better still (Vohra et al., 2015); we believe this is likely due to the fact that the datasets are relatively small, and thus the right regularization method or generative modeling proce- dure can improve performance signi\ufb01cantly. This is largely orthogonal to the RNN/TCN distinction, as a similar variant of TCN may well be possible. Word-level language modeling. Language modeling re- mains one of the primary applications of recurrent networks and many recent works have focused on optimizing LSTMs for this task (Krueger et al., 2017; Merity et al., 2017). Our implementation follows standard practice that ties the weights of encoder and decoder layers for both TCN and RNNs (Press & Wolf, 2016), which signi\ufb01cantly reduces the number of parameters in the model. For training, we use SGD and anneal the learning rate by a factor of 0.5 for both TCN and RNNs when validation accuracy plateaus. On the smaller PTB corpus, an optimized LSTM architec- ture (with recurrent and embedding dropout, etc.) outper- forms the TCN, while the TCN outperforms both GRU and vanilla RNN. However, on the much larger Wikitext-103 corpus and the LAMBADA dataset (Paperno et al., 2016), without any hyperparameter search, the TCN outperforms An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling the LSTM results of Grave et al. (2017), achieving much lower perplexities. Character-level language modeling. On character-level language modeling (PTB and text8, accuracy measured in bits per character), the generic TCN outperforms regular- ized LSTMs and GRUs as well as methods such as Norm- stabilized LSTMs (Krueger & Memisevic, 2015). (Special- ized architectures exist that outperform all of these, see the supplement.)"
  },
  {
    "chunk_id": "01918466-cafb-4f02-a83f-91941269c718",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 8,
    "retrieval_text": "5.4. Memory Size of TCN and RNNs \u2014 TCN (10K) \u2014\u2014 _LSTM (10K) \u2014\u2014 GRU (10K) ---: Random Guess 3 8 eA s Testing Accuracy k 2 & 8 y is 0 50 100 150 200 250 Sequence length T One of the theoretical advantages of recurrent architectures is their unlimited memory: the theoretical ability to retain information through sequences of unlimited length. We now examine speci\ufb01cally how long the different architectures can retain information in practice. We focus on 1) the copy memory task, which is a stress test designed to evaluate long- term, distant information propagation in recurrent networks, and 2) the LAMBADA task, which tests both local and non-local textual understanding. Figure 5. Accuracy on the copy memory task for sequences of different lengths T. While TCN exhibits 100% accuracy for all sequence lengths, the LSTM and GRU degenerate to random guess- ing as T grows. The copy memory task is perfectly set up to examine a model\u2019s ability to retain information for different lengths of time. The requisite retention time can be controlled by varying the sequence length T. In contrast to Section 5.2, we now focus on the accuracy on the last 10 elements of the output sequence (which are the nontrivial elements that must be recalled). We used models of size 10K for both TCN and RNNs. that combines best practices such as dilations and residual connections with the causal convolutions needed for autore- gressive prediction. The experimental results indicate that TCN models substantially outperform generic recurrent ar- chitectures such as LSTMs and GRUs. We further studied long-range information propagation in convolutional and recurrent networks, and showed that the \u201cin\ufb01nite memory\u201d advantage of RNNs is largely absent in practice. TCNs exhibit longer memory than recurrent architectures with the same capacity. The results of this focused study are shown in Figure 5. TCNs consistently converge to 100% accuracy for all se- quence lengths, whereas LSTMs and GRUs of the same size quickly degenerate to random guessing as the sequence length T grows. The accuracy of the LSTM falls below 20% for T < 50, while the GRU falls below 20% for T < 200. These results indicate that TCNs are able to maintain a much longer effective history than their recurrent counterparts. This observation is backed up on real data by experiments on the large-scale LAMBADA dataset, which is speci\ufb01cally designed to test a model\u2019s ability to utilize broad context (Pa- perno et al., 2016). As shown in Table 1, TCN outperforms LSTMs and vanilla RNNs by a signi\ufb01cant margin in perplex- ity on LAMBADA, with a substantially smaller network and virtually no tuning. (State-of-the-art results on this dataset are even better, but only with the help of additional memory mechanisms (Grave et al., 2017).)",
    "raw_text": "5.4. Memory Size of TCN and RNNs \u2014 TCN (10K) \u2014\u2014 _LSTM (10K) \u2014\u2014 GRU (10K) ---: Random Guess 3 8 eA s Testing Accuracy k 2 & 8 y is 0 50 100 150 200 250 Sequence length T One of the theoretical advantages of recurrent architectures is their unlimited memory: the theoretical ability to retain information through sequences of unlimited length. We now examine speci\ufb01cally how long the different architectures can retain information in practice. We focus on 1) the copy memory task, which is a stress test designed to evaluate long- term, distant information propagation in recurrent networks, and 2) the LAMBADA task, which tests both local and non-local textual understanding. Figure 5. Accuracy on the copy memory task for sequences of different lengths T. While TCN exhibits 100% accuracy for all sequence lengths, the LSTM and GRU degenerate to random guess- ing as T grows. The copy memory task is perfectly set up to examine a model\u2019s ability to retain information for different lengths of time. The requisite retention time can be controlled by varying the sequence length T. In contrast to Section 5.2, we now focus on the accuracy on the last 10 elements of the output sequence (which are the nontrivial elements that must be recalled). We used models of size 10K for both TCN and RNNs. that combines best practices such as dilations and residual connections with the causal convolutions needed for autore- gressive prediction. The experimental results indicate that TCN models substantially outperform generic recurrent ar- chitectures such as LSTMs and GRUs. We further studied long-range information propagation in convolutional and recurrent networks, and showed that the \u201cin\ufb01nite memory\u201d advantage of RNNs is largely absent in practice. TCNs exhibit longer memory than recurrent architectures with the same capacity. The results of this focused study are shown in Figure 5. TCNs consistently converge to 100% accuracy for all se- quence lengths, whereas LSTMs and GRUs of the same size quickly degenerate to random guessing as the sequence length T grows. The accuracy of the LSTM falls below 20% for T < 50, while the GRU falls below 20% for T < 200. These results indicate that TCNs are able to maintain a much longer effective history than their recurrent counterparts. This observation is backed up on real data by experiments on the large-scale LAMBADA dataset, which is speci\ufb01cally designed to test a model\u2019s ability to utilize broad context (Pa- perno et al., 2016). As shown in Table 1, TCN outperforms LSTMs and vanilla RNNs by a signi\ufb01cant margin in perplex- ity on LAMBADA, with a substantially smaller network and virtually no tuning. (State-of-the-art results on this dataset are even better, but only with the help of additional memory mechanisms (Grave et al., 2017).)"
  },
  {
    "chunk_id": "17e0bc4f-bb8a-4230-be4c-8b631863758f",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 8,
    "retrieval_text": "6. Conclusion We have presented an empirical evaluation of generic convo- lutional and recurrent architectures across a comprehensive suite of sequence modeling tasks. To this end, we have described a simple temporal convolutional network (TCN) Numerous advanced schemes for regularizing and opti- mizing LSTMs have been proposed (Press & Wolf, 2016; Krueger et al., 2017; Merity et al., 2017; Campos et al., 2018). These schemes have signi\ufb01cantly advanced the ac- curacy achieved by LSTM-based architectures on some datasets. The TCN has not yet bene\ufb01tted from this con- certed community-wide investment into architectural and algorithmic elaborations. We see such investment as desir- able and expect it to yield advances in TCN performance that are commensurate with the advances seen in recent years in LSTM performance. We will release the code for our project to encourage this exploration. The preeminence enjoyed by recurrent networks in sequence modeling may be largely a vestige of history. Until recently, before the introduction of architectural elements such as dilated convolutions and residual connections, convolutional architectures were indeed weaker. Our results indicate that with these elements, a simple convolutional architecture is more effective across diverse sequence modeling tasks than recurrent architectures such as LSTMs. Due to the comparable clarity and simplicity of TCNs, we conclude that convolutional networks should be regarded as a natural starting point and a powerful toolkit for sequence modeling. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling References networks for long-term dependencies. In NIPS, 1995. Allan, Moray and Williams, Christopher. Harmonising chorales by probabilistic inference. In NIPS, 2005. Elman, Jeffrey L. Finding structure in time. Cognitive Science, 14 (2), 1990. Arjovsky, Martin, Shah, Amar, and Bengio, Yoshua. Unitary evolution recurrent neural networks. In ICML, 2016. Ba, Lei Jimmy, Kiros, Ryan, and Hinton, Geoffrey E. Layer normalization. arXiv:1607.06450, 2016. Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to align and translate. In ICLR, 2015. Bengio, Yoshua, Simard, Patrice, and Frasconi, Paolo. Learning long-term dependencies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural Networks, 5(2), 1994. Bottou, L\u00b4eon, Soulie, F Fogelman, Blanchet, Pascal, and Li\u00b4enard, Jean-Sylvain. Speaker-independent isolated digit recognition: Multilayer perceptrons vs. dynamic time warping. Neural Net- works, 3(4), 1990. Gehring, Jonas, Auli, Michael, Grangier, David, and Dauphin, Yann. A convolutional encoder model for neural machine trans- lation. In ACL, 2017a. Gehring, Jonas, Auli, Michael, Grangier, David, Yarats, Denis, and Dauphin, Yann N. Convolutional sequence to sequence learning. In ICML, 2017b. Gers, Felix A, Schraudolph, Nicol N, and Schmidhuber, J\u00a8urgen. Learning precise timing with lstm recurrent networks. JMLR, 3, 2002. Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. Deep Learning. MIT Press, 2016. Grave, Edouard, Joulin, Armand, and Usunier, Nicolas. Improving neural language models with a continuous cache. In ICLR, 2017. Boulanger-Lewandowski, Nicolas, Bengio, Yoshua, and Vincent, Pascal. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. arXiv:1206.6392, 2012. Graves, Alex. Supervised Sequence Labelling with Recurrent Neural Networks. Springer, 2012. Graves, Alex. Generating sequences with recurrent neural net- works. arXiv:1308.0850, 2013. Bradbury, James, Merity, Stephen, Xiong, Caiming, and Socher, Richard. Quasi-recurrent neural networks. In ICLR, 2017. Campos, Victor, Jou, Brendan, Gir\u00b4o i Nieto, Xavier, Torres, Jordi, and Chang, Shih-Fu. Skip RNN: Learning to skip state updates in recurrent neural networks. In ICLR, 2018. Chang, Shiyu, Zhang, Yang, Han, Wei, Yu, Mo, Guo, Xiaoxiao, Tan, Wei, Cui, Xiaodong, Witbrock, Michael J., Hasegawa- Johnson, Mark A., and Huang, Thomas S. Dilated recurrent neural networks. In NIPS, 2017. Cho, Kyunghyun, Van Merri\u00a8enboer, Bart, Bahdanau, Dzmitry, and Bengio, Yoshua. On the properties of neural machine translation: Encoder-decoder approaches. arXiv:1409.1259, 2014. Greff, Klaus, Srivastava, Rupesh Kumar, Koutn\u00b4\u0131k, Jan, Steune- brink, Bas R., and Schmidhuber, J\u00a8urgen. LSTM: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28(10), 2017. Ha, David, Dai, Andrew, and Le, Quoc V. HyperNetworks. In ICLR, 2017. He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image recognition. In CVPR, 2016. Hermans, Michiel and Schrauwen, Benjamin. Training and analysing deep recurrent neural networks. In NIPS, 2013. Hinton, Geoffrey E. Connectionist learning procedures. Arti\ufb01cial Intelligence, 40(1-3), 1989. Chung, Junyoung, Gulcehre, Caglar, Cho, KyungHyun, and Ben- gio, Yoshua. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv:1412.3555, 2014. Hochreiter, Sepp and Schmidhuber, J\u00a8urgen. Long short-term mem- ory. Neural Computation, 9(8), 1997. Chung, Junyoung, Ahn, Sungjin, and Bengio, Yoshua. Hierarchical multiscale recurrent neural networks. arXiv:1609.01704, 2016. Collobert, Ronan and Weston, Jason. A uni\ufb01ed architecture for nat- ural language processing: Deep neural networks with multitask learning. In ICML, 2008. Collobert, Ronan, Weston, Jason, Bottou, L\u00b4eon, Karlen, Michael, Kavukcuoglu, Koray, and Kuksa, Pavel P. Natural language processing (almost) from scratch. JMLR, 12, 2011. Jing, Li, Shen, Yichen, Dubcek, Tena, Peurifoy, John, Skirlo, Scott, LeCun, Yann, Tegmark, Max, and Solja\u02c7ci\u00b4c, Marin. Tunable ef\ufb01cient unitary neural networks (EUNN) and their application to RNNs. In ICML, 2017. Johnson, Rie and Zhang, Tong. Effective use of word order for text categorization with convolutional neural networks. In HLT- NAACL, 2015. Johnson, Rie and Zhang, Tong. Deep pyramid convolutional neural networks for text categorization. In ACL, 2017. Conneau, Alexis, Schwenk, Holger, LeCun, Yann, and Barrault, Lo\u00a8\u0131c. Very deep convolutional networks for text classi\ufb01cation. In European Chapter of the Association for Computational Linguistics (EACL), 2017. Jozefowicz, Rafal, Zaremba, Wojciech, and Sutskever, Ilya. An empirical exploration of recurrent network architectures. In ICML, 2015. Cooijmans, Tim, Ballas, Nicolas, Laurent, C\u00b4esar, G\u00a8ulc\u00b8ehre, C\u00b8a\u02d8glar, and Courville, Aaron. Recurrent batch normalization. In ICLR, 2016. Kalchbrenner, Nal, Grefenstette, Edward, and Blunsom, Phil. A convolutional neural network for modelling sentences. In ACL, 2014. Dauphin, Yann N., Fan, Angela, Auli, Michael, and Grangier, David. Language modeling with gated convolutional networks. In ICML, 2017. Kalchbrenner, Nal, Espeholt, Lasse, Simonyan, Karen, van den Oord, A\u00a8aron, Graves, Alex, and Kavukcuoglu, Koray. Neural machine translation in linear time. arXiv:1610.10099, 2016. dos Santos, C\u00b4\u0131cero Nogueira and Zadrozny, Bianca. Learning character-level representations for part-of-speech tagging. In ICML, 2014. El Hihi, Salah and Bengio, Yoshua. Hierarchical recurrent neural Kim, Yoon. Convolutional neural networks for sentence classi\ufb01ca- tion. In EMNLP, 2014. Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. In ICLR, 2015. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Koutnik, Jan, Greff, Klaus, Gomez, Faustino, and Schmidhuber, Juergen. A clockwork RNN. In ICML, 2014.",
    "raw_text": "6. Conclusion We have presented an empirical evaluation of generic convo- lutional and recurrent architectures across a comprehensive suite of sequence modeling tasks. To this end, we have described a simple temporal convolutional network (TCN) Numerous advanced schemes for regularizing and opti- mizing LSTMs have been proposed (Press & Wolf, 2016; Krueger et al., 2017; Merity et al., 2017; Campos et al., 2018). These schemes have signi\ufb01cantly advanced the ac- curacy achieved by LSTM-based architectures on some datasets. The TCN has not yet bene\ufb01tted from this con- certed community-wide investment into architectural and algorithmic elaborations. We see such investment as desir- able and expect it to yield advances in TCN performance that are commensurate with the advances seen in recent years in LSTM performance. We will release the code for our project to encourage this exploration. The preeminence enjoyed by recurrent networks in sequence modeling may be largely a vestige of history. Until recently, before the introduction of architectural elements such as dilated convolutions and residual connections, convolutional architectures were indeed weaker. Our results indicate that with these elements, a simple convolutional architecture is more effective across diverse sequence modeling tasks than recurrent architectures such as LSTMs. Due to the comparable clarity and simplicity of TCNs, we conclude that convolutional networks should be regarded as a natural starting point and a powerful toolkit for sequence modeling. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling References networks for long-term dependencies. In NIPS, 1995. Allan, Moray and Williams, Christopher. Harmonising chorales by probabilistic inference. In NIPS, 2005. Elman, Jeffrey L. Finding structure in time. Cognitive Science, 14 (2), 1990. Arjovsky, Martin, Shah, Amar, and Bengio, Yoshua. Unitary evolution recurrent neural networks. In ICML, 2016. Ba, Lei Jimmy, Kiros, Ryan, and Hinton, Geoffrey E. Layer normalization. arXiv:1607.06450, 2016. Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to align and translate. In ICLR, 2015. Bengio, Yoshua, Simard, Patrice, and Frasconi, Paolo. Learning long-term dependencies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural Networks, 5(2), 1994. Bottou, L\u00b4eon, Soulie, F Fogelman, Blanchet, Pascal, and Li\u00b4enard, Jean-Sylvain. Speaker-independent isolated digit recognition: Multilayer perceptrons vs. dynamic time warping. Neural Net- works, 3(4), 1990. Gehring, Jonas, Auli, Michael, Grangier, David, and Dauphin, Yann. A convolutional encoder model for neural machine trans- lation. In ACL, 2017a. Gehring, Jonas, Auli, Michael, Grangier, David, Yarats, Denis, and Dauphin, Yann N. Convolutional sequence to sequence learning. In ICML, 2017b. Gers, Felix A, Schraudolph, Nicol N, and Schmidhuber, J\u00a8urgen. Learning precise timing with lstm recurrent networks. JMLR, 3, 2002. Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. Deep Learning. MIT Press, 2016. Grave, Edouard, Joulin, Armand, and Usunier, Nicolas. Improving neural language models with a continuous cache. In ICLR, 2017. Boulanger-Lewandowski, Nicolas, Bengio, Yoshua, and Vincent, Pascal. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. arXiv:1206.6392, 2012. Graves, Alex. Supervised Sequence Labelling with Recurrent Neural Networks. Springer, 2012. Graves, Alex. Generating sequences with recurrent neural net- works. arXiv:1308.0850, 2013. Bradbury, James, Merity, Stephen, Xiong, Caiming, and Socher, Richard. Quasi-recurrent neural networks. In ICLR, 2017. Campos, Victor, Jou, Brendan, Gir\u00b4o i Nieto, Xavier, Torres, Jordi, and Chang, Shih-Fu. Skip RNN: Learning to skip state updates in recurrent neural networks. In ICLR, 2018. Chang, Shiyu, Zhang, Yang, Han, Wei, Yu, Mo, Guo, Xiaoxiao, Tan, Wei, Cui, Xiaodong, Witbrock, Michael J., Hasegawa- Johnson, Mark A., and Huang, Thomas S. Dilated recurrent neural networks. In NIPS, 2017. Cho, Kyunghyun, Van Merri\u00a8enboer, Bart, Bahdanau, Dzmitry, and Bengio, Yoshua. On the properties of neural machine translation: Encoder-decoder approaches. arXiv:1409.1259, 2014. Greff, Klaus, Srivastava, Rupesh Kumar, Koutn\u00b4\u0131k, Jan, Steune- brink, Bas R., and Schmidhuber, J\u00a8urgen. LSTM: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28(10), 2017. Ha, David, Dai, Andrew, and Le, Quoc V. HyperNetworks. In ICLR, 2017. He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image recognition. In CVPR, 2016. Hermans, Michiel and Schrauwen, Benjamin. Training and analysing deep recurrent neural networks. In NIPS, 2013. Hinton, Geoffrey E. Connectionist learning procedures. Arti\ufb01cial Intelligence, 40(1-3), 1989. Chung, Junyoung, Gulcehre, Caglar, Cho, KyungHyun, and Ben- gio, Yoshua. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv:1412.3555, 2014. Hochreiter, Sepp and Schmidhuber, J\u00a8urgen. Long short-term mem- ory. Neural Computation, 9(8), 1997. Chung, Junyoung, Ahn, Sungjin, and Bengio, Yoshua. Hierarchical multiscale recurrent neural networks. arXiv:1609.01704, 2016. Collobert, Ronan and Weston, Jason. A uni\ufb01ed architecture for nat- ural language processing: Deep neural networks with multitask learning. In ICML, 2008. Collobert, Ronan, Weston, Jason, Bottou, L\u00b4eon, Karlen, Michael, Kavukcuoglu, Koray, and Kuksa, Pavel P. Natural language processing (almost) from scratch. JMLR, 12, 2011. Jing, Li, Shen, Yichen, Dubcek, Tena, Peurifoy, John, Skirlo, Scott, LeCun, Yann, Tegmark, Max, and Solja\u02c7ci\u00b4c, Marin. Tunable ef\ufb01cient unitary neural networks (EUNN) and their application to RNNs. In ICML, 2017. Johnson, Rie and Zhang, Tong. Effective use of word order for text categorization with convolutional neural networks. In HLT- NAACL, 2015. Johnson, Rie and Zhang, Tong. Deep pyramid convolutional neural networks for text categorization. In ACL, 2017. Conneau, Alexis, Schwenk, Holger, LeCun, Yann, and Barrault, Lo\u00a8\u0131c. Very deep convolutional networks for text classi\ufb01cation. In European Chapter of the Association for Computational Linguistics (EACL), 2017. Jozefowicz, Rafal, Zaremba, Wojciech, and Sutskever, Ilya. An empirical exploration of recurrent network architectures. In ICML, 2015. Cooijmans, Tim, Ballas, Nicolas, Laurent, C\u00b4esar, G\u00a8ulc\u00b8ehre, C\u00b8a\u02d8glar, and Courville, Aaron. Recurrent batch normalization. In ICLR, 2016. Kalchbrenner, Nal, Grefenstette, Edward, and Blunsom, Phil. A convolutional neural network for modelling sentences. In ACL, 2014. Dauphin, Yann N., Fan, Angela, Auli, Michael, and Grangier, David. Language modeling with gated convolutional networks. In ICML, 2017. Kalchbrenner, Nal, Espeholt, Lasse, Simonyan, Karen, van den Oord, A\u00a8aron, Graves, Alex, and Kavukcuoglu, Koray. Neural machine translation in linear time. arXiv:1610.10099, 2016. dos Santos, C\u00b4\u0131cero Nogueira and Zadrozny, Bianca. Learning character-level representations for part-of-speech tagging. In ICML, 2014. El Hihi, Salah and Bengio, Yoshua. Hierarchical recurrent neural Kim, Yoon. Convolutional neural networks for sentence classi\ufb01ca- tion. In EMNLP, 2014. Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. In ICLR, 2015. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Koutnik, Jan, Greff, Klaus, Gomez, Faustino, and Schmidhuber, Juergen. A clockwork RNN. In ICML, 2014."
  },
  {
    "chunk_id": "c4619076-71c8-4480-9aa7-39725ee50264",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 10,
    "retrieval_text": "Krueger, David and Memisevic, Roland. Regularizing RNNs by stabilizing activations. arXiv:1511.08400, 2015. language models. arXiv:1608.05859, 2016. Salimans, Tim and Kingma, Diederik P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In NIPS, 2016. Krueger, David, Maharaj, Tegan, Kram\u00b4ar, J\u00b4anos, Pezeshki, Mo- hammad, Ballas, Nicolas, Ke, Nan Rosemary, Goyal, Anirudh, Bengio, Yoshua, Larochelle, Hugo, Courville, Aaron C., and Pal, Chris. Zoneout: Regularizing RNNs by randomly preserv- ing hidden activations. In ICLR, 2017. Le, Quoc V, Jaitly, Navdeep, and Hinton, Geoffrey E. A simple way to initialize recurrent networks of recti\ufb01ed linear units. arXiv:1504.00941, 2015. Lea, Colin, Flynn, Michael D., Vidal, Ren\u00b4e, Reiter, Austin, and Hager, Gregory D. Temporal convolutional networks for action segmentation and detection. In CVPR, 2017. LeCun, Yann, Boser, Bernhard, Denker, John S., Henderson, Donnie, Howard, Richard E., Hubbard, Wayne, and Jackel, Lawrence D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4), 1989. LeCun, Yann, Bottou, L\u00b4eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to document recognition. Pro- ceedings of the IEEE, 86(11), 1998. Schuster, Mike and Paliwal, Kuldip K. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45 (11), 1997. Sejnowski, Terrence J. and Rosenberg, Charles R. Parallel net- works that learn to pronounce English text. Complex Systems, 1, 1987. Shi, Xingjian, Chen, Zhourong, Wang, Hao, Yeung, Dit-Yan, Wong, Wai-Kin, and Woo, Wang-chun. Convolutional LSTM network: A machine learning approach for precipitation now- casting. In NIPS, 2015. Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from over\ufb01tting. JMLR, 15(1), 2014. Subakan, Y Cem and Smaragdis, Paris. Diagonal RNNs in sym- bolic music modeling. arXiv:1704.05420, 2017. Sutskever, Ilya, Martens, James, and Hinton, Geoffrey E. Generat- ing text with recurrent neural networks. In ICML, 2011. Long, Jonathan, Shelhamer, Evan, and Darrell, Trevor. Fully convolutional networks for semantic segmentation. In CVPR, 2015. Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural networks. In NIPS, 2014. Marcus, Mitchell P, Marcinkiewicz, Mary Ann, and Santorini, Beatrice. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2), 1993. van den Oord, A\u00a8aron, Dieleman, Sander, Zen, Heiga, Simonyan, Karen, Vinyals, Oriol, Graves, Alex, Kalchbrenner, Nal, Senior, Andrew W., and Kavukcuoglu, Koray. WaveNet: A generative model for raw audio. arXiv:1609.03499, 2016. Martens, James and Sutskever, Ilya. Learning recurrent neural networks with Hessian-free optimization. In ICML, 2011. Melis, G\u00b4abor, Dyer, Chris, and Blunsom, Phil. On the state of the art of evaluation in neural language models. In ICLR, 2018. Merity, Stephen, Xiong, Caiming, Bradbury, James, and Socher, Richard. Pointer sentinel mixture models. arXiv:1609.07843, 2016. Vohra, Raunaq, Goel, Kratarth, and Sahoo, JK. Modeling temporal dependencies in data using a DBN-LSTM. In Data Science and Advanced Analytics (DSAA), 2015. Waibel, Alex, Hanazawa, Toshiyuki, Hinton, Geoffrey, Shikano, Kiyohiro, and Lang, Kevin J. Phoneme recognition using time- delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(3), 1989. Merity, Stephen, Keskar, Nitish Shirish, and Socher, Richard. Regularizing and optimizing LSTM language models. arXiv:1708.02182, 2017. Mikolov, Tom\u00b4a\u02c7s, Sutskever, Ilya, Deoras, Anoop, Le, Hai-Son, Kombrink, Stefan, and Cernocky, Jan. Subword language mod- eling with neural networks. Preprint, 2012. Miyamoto, Yasumasa and Cho, Kyunghyun. Gated word-character recurrent language model. arXiv:1606.01700, 2016. Werbos, Paul J. Backpropagation through time: What it does and how to do it. Proceedings of the IEEE, 78(10), 1990. Wisdom, Scott, Powers, Thomas, Hershey, John, Le Roux, Jonathan, and Atlas, Les. Full-capacity unitary recurrent neural networks. In NIPS, 2016. Wu, Yuhuai, Zhang, Saizheng, Zhang, Ying, Bengio, Yoshua, and Salakhutdinov, Ruslan R. On multiplicative integration with recurrent neural networks. In NIPS, 2016. Nair, Vinod and Hinton, Geoffrey E. Recti\ufb01ed linear units improve restricted Boltzmann machines. In ICML, 2010. Ng, Andrew. Sequence Models (Course 5 of Deep Learning Spe- cialization). Coursera, 2018. Paperno, Denis, Kruszewski, Germ\u00b4an, Lazaridou, Angeliki, Pham, Quan Ngoc, Bernardi, Raffaella, Pezzelle, Sandro, Baroni, Marco, Boleda, Gemma, and Fern\u00b4andez, Raquel. The LAM- BADA dataset: Word prediction requiring a broad discourse context. arXiv:1606.06031, 2016. Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the dif\ufb01culty of training recurrent neural networks. In ICML, 2013. Pascanu, Razvan, G\u00a8ulc\u00b8ehre, C\u00b8aglar, Cho, Kyunghyun, and Bengio, Yoshua. How to construct deep recurrent neural networks. In ICLR, 2014. Press, O\ufb01r and Wolf, Lior. Using the output embedding to improve Yang, Zhilin, Dai, Zihang, Salakhutdinov, Ruslan, and Cohen, William W. Breaking the softmax bottleneck: A high-rank RNN language model. ICLR, 2018. Yin, Wenpeng, Kann, Katharina, Yu, Mo, and Sch\u00a8utze, Hinrich. Comparative study of CNN and RNN for natural language pro- cessing. arXiv:1702.01923, 2017. Yu, Fisher and Koltun, Vladlen. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016. Zhang, Saizheng, Wu, Yuhuai, Che, Tong, Lin, Zhouhan, Memi- sevic, Roland, Salakhutdinov, Ruslan R, and Bengio, Yoshua. Architectural complexity measures of recurrent neural networks. In NIPS, 2016. Zhang, Xiang, Zhao, Junbo Jake, and LeCun, Yann. Character- level convolutional networks for text classi\ufb01cation. In NIPS, 2015.",
    "raw_text": "Krueger, David and Memisevic, Roland. Regularizing RNNs by stabilizing activations. arXiv:1511.08400, 2015. language models. arXiv:1608.05859, 2016. Salimans, Tim and Kingma, Diederik P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In NIPS, 2016. Krueger, David, Maharaj, Tegan, Kram\u00b4ar, J\u00b4anos, Pezeshki, Mo- hammad, Ballas, Nicolas, Ke, Nan Rosemary, Goyal, Anirudh, Bengio, Yoshua, Larochelle, Hugo, Courville, Aaron C., and Pal, Chris. Zoneout: Regularizing RNNs by randomly preserv- ing hidden activations. In ICLR, 2017. Le, Quoc V, Jaitly, Navdeep, and Hinton, Geoffrey E. A simple way to initialize recurrent networks of recti\ufb01ed linear units. arXiv:1504.00941, 2015. Lea, Colin, Flynn, Michael D., Vidal, Ren\u00b4e, Reiter, Austin, and Hager, Gregory D. Temporal convolutional networks for action segmentation and detection. In CVPR, 2017. LeCun, Yann, Boser, Bernhard, Denker, John S., Henderson, Donnie, Howard, Richard E., Hubbard, Wayne, and Jackel, Lawrence D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4), 1989. LeCun, Yann, Bottou, L\u00b4eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to document recognition. Pro- ceedings of the IEEE, 86(11), 1998. Schuster, Mike and Paliwal, Kuldip K. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45 (11), 1997. Sejnowski, Terrence J. and Rosenberg, Charles R. Parallel net- works that learn to pronounce English text. Complex Systems, 1, 1987. Shi, Xingjian, Chen, Zhourong, Wang, Hao, Yeung, Dit-Yan, Wong, Wai-Kin, and Woo, Wang-chun. Convolutional LSTM network: A machine learning approach for precipitation now- casting. In NIPS, 2015. Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from over\ufb01tting. JMLR, 15(1), 2014. Subakan, Y Cem and Smaragdis, Paris. Diagonal RNNs in sym- bolic music modeling. arXiv:1704.05420, 2017. Sutskever, Ilya, Martens, James, and Hinton, Geoffrey E. Generat- ing text with recurrent neural networks. In ICML, 2011. Long, Jonathan, Shelhamer, Evan, and Darrell, Trevor. Fully convolutional networks for semantic segmentation. In CVPR, 2015. Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural networks. In NIPS, 2014. Marcus, Mitchell P, Marcinkiewicz, Mary Ann, and Santorini, Beatrice. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2), 1993. van den Oord, A\u00a8aron, Dieleman, Sander, Zen, Heiga, Simonyan, Karen, Vinyals, Oriol, Graves, Alex, Kalchbrenner, Nal, Senior, Andrew W., and Kavukcuoglu, Koray. WaveNet: A generative model for raw audio. arXiv:1609.03499, 2016. Martens, James and Sutskever, Ilya. Learning recurrent neural networks with Hessian-free optimization. In ICML, 2011. Melis, G\u00b4abor, Dyer, Chris, and Blunsom, Phil. On the state of the art of evaluation in neural language models. In ICLR, 2018. Merity, Stephen, Xiong, Caiming, Bradbury, James, and Socher, Richard. Pointer sentinel mixture models. arXiv:1609.07843, 2016. Vohra, Raunaq, Goel, Kratarth, and Sahoo, JK. Modeling temporal dependencies in data using a DBN-LSTM. In Data Science and Advanced Analytics (DSAA), 2015. Waibel, Alex, Hanazawa, Toshiyuki, Hinton, Geoffrey, Shikano, Kiyohiro, and Lang, Kevin J. Phoneme recognition using time- delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(3), 1989. Merity, Stephen, Keskar, Nitish Shirish, and Socher, Richard. Regularizing and optimizing LSTM language models. arXiv:1708.02182, 2017. Mikolov, Tom\u00b4a\u02c7s, Sutskever, Ilya, Deoras, Anoop, Le, Hai-Son, Kombrink, Stefan, and Cernocky, Jan. Subword language mod- eling with neural networks. Preprint, 2012. Miyamoto, Yasumasa and Cho, Kyunghyun. Gated word-character recurrent language model. arXiv:1606.01700, 2016. Werbos, Paul J. Backpropagation through time: What it does and how to do it. Proceedings of the IEEE, 78(10), 1990. Wisdom, Scott, Powers, Thomas, Hershey, John, Le Roux, Jonathan, and Atlas, Les. Full-capacity unitary recurrent neural networks. In NIPS, 2016. Wu, Yuhuai, Zhang, Saizheng, Zhang, Ying, Bengio, Yoshua, and Salakhutdinov, Ruslan R. On multiplicative integration with recurrent neural networks. In NIPS, 2016. Nair, Vinod and Hinton, Geoffrey E. Recti\ufb01ed linear units improve restricted Boltzmann machines. In ICML, 2010. Ng, Andrew. Sequence Models (Course 5 of Deep Learning Spe- cialization). Coursera, 2018. Paperno, Denis, Kruszewski, Germ\u00b4an, Lazaridou, Angeliki, Pham, Quan Ngoc, Bernardi, Raffaella, Pezzelle, Sandro, Baroni, Marco, Boleda, Gemma, and Fern\u00b4andez, Raquel. The LAM- BADA dataset: Word prediction requiring a broad discourse context. arXiv:1606.06031, 2016. Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the dif\ufb01culty of training recurrent neural networks. In ICML, 2013. Pascanu, Razvan, G\u00a8ulc\u00b8ehre, C\u00b8aglar, Cho, Kyunghyun, and Bengio, Yoshua. How to construct deep recurrent neural networks. In ICLR, 2014. Press, O\ufb01r and Wolf, Lior. Using the output embedding to improve Yang, Zhilin, Dai, Zihang, Salakhutdinov, Ruslan, and Cohen, William W. Breaking the softmax bottleneck: A high-rank RNN language model. ICLR, 2018. Yin, Wenpeng, Kann, Katharina, Yu, Mo, and Sch\u00a8utze, Hinrich. Comparative study of CNN and RNN for natural language pro- cessing. arXiv:1702.01923, 2017. Yu, Fisher and Koltun, Vladlen. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016. Zhang, Saizheng, Wu, Yuhuai, Che, Tong, Lin, Zhouhan, Memi- sevic, Roland, Salakhutdinov, Ruslan R, and Bengio, Yoshua. Architectural complexity measures of recurrent neural networks. In NIPS, 2016. Zhang, Xiang, Zhao, Junbo Jake, and LeCun, Yann. Character- level convolutional networks for text classi\ufb01cation. In NIPS, 2015."
  },
  {
    "chunk_id": "cf4f82f1-9a0d-42ce-9c3f-655934dec33b",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 11,
    "retrieval_text": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Supplementary Material A. Hyperparameters Settings C. Effect of Filter Size and Residual Block A.1. Hyperparameters for TCN Table 2 lists the hyperparameters we used when applying the generic TCN model on various tasks and datasets. The most important factor for picking parameters is to make sure that the TCN has a suf\ufb01ciently large receptive \ufb01eld by choosing k and d that can cover the amount of context needed for the task. As discussed in Section 5, the number of hidden units was chosen so that the model size is approximately at the same level as the recurrent models with which we are comparing. In Table 2, a gradient clip of N/A means no gradient clip- ping was applied. In larger tasks (e.g., language modeling), we empirically found that gradient clipping (we randomly picked a threshold from [0.3,1]) helps with regularizing TCN and accelerating convergence. All weights were initialized from a Gaussian disitribution N(0,0.01). In general, we found TCN to be relatively in- sensitive to hyperparameter changes, as long as the effective history (i.e., receptive \ufb01eld) size is suf\ufb01cient. A.2. Hyperparameters for LSTM/GRU Table 3 reports hyperparameter settings that were used for the LSTM. These values are picked from hyperparameter search for LSTMs that have up to 3 layers, and the optimiz- ers are chosen from {SGD, Adam, RMSprop, Adagrad}. For certain larger datasets, we adopted the settings used in prior work (e.g., Grave et al. (2017) on Wikitext-103). GRU hyperparameters were chosen in a similar fashion, but typically with more hidden units than in LSTM to keep the total network size approximately the same (since a GRU cell is more compact). In this section we brie\ufb02y study the effects of different com- ponents of a TCN layer. Overall, we believe dilation is required for modeling long-term dependencies, and so we mainly focus on two other factors here: the \ufb01lter size k used by each layer, and the effect of residual blocks. We perform a series of controlled experiments, with the results of the ablative analysis shown in Figure 6. As be- fore, we kept the model size and depth exactly the same for different models, so that the dilation factor is strictly con- trolled. The experiments were conducted on three different tasks: copy memory, permuted MNIST (P-MNIST), and Penn Treebank word-level language modeling. These ex- periments con\ufb01rm that both factors (\ufb01lter size and residual connections) contribute to sequence modeling performance. Filter size k. In both the copy memory and the P-MNIST tasks, we observed faster convergence and better accuracy for larger \ufb01lter sizes. In particular, looking at Figure 6a, a TCN with \ufb01lter size \u2264 3 only converges to the same level as random guessing. In contrast, on word-level language mod- eling, a smaller kernel with \ufb01lter size of k = 3 works best. We believe this is because a smaller kernel (along with \ufb01xed dilation) tends to focus more on the local context, which is especially important for PTB language modeling (in fact, the very success of n-gram models suggests that only a relatively short memory is needed for modeling language). Residual block. In all three scenarios that we compared here, we observed that the residual function stabilized train- ing and brought faster convergence with better \ufb01nal results. Especially in language modeling, we found that residual connections contribute substantially to performance (See Figure 6f).",
    "raw_text": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Supplementary Material A. Hyperparameters Settings C. Effect of Filter Size and Residual Block A.1. Hyperparameters for TCN Table 2 lists the hyperparameters we used when applying the generic TCN model on various tasks and datasets. The most important factor for picking parameters is to make sure that the TCN has a suf\ufb01ciently large receptive \ufb01eld by choosing k and d that can cover the amount of context needed for the task. As discussed in Section 5, the number of hidden units was chosen so that the model size is approximately at the same level as the recurrent models with which we are comparing. In Table 2, a gradient clip of N/A means no gradient clip- ping was applied. In larger tasks (e.g., language modeling), we empirically found that gradient clipping (we randomly picked a threshold from [0.3,1]) helps with regularizing TCN and accelerating convergence. All weights were initialized from a Gaussian disitribution N(0,0.01). In general, we found TCN to be relatively in- sensitive to hyperparameter changes, as long as the effective history (i.e., receptive \ufb01eld) size is suf\ufb01cient. A.2. Hyperparameters for LSTM/GRU Table 3 reports hyperparameter settings that were used for the LSTM. These values are picked from hyperparameter search for LSTMs that have up to 3 layers, and the optimiz- ers are chosen from {SGD, Adam, RMSprop, Adagrad}. For certain larger datasets, we adopted the settings used in prior work (e.g., Grave et al. (2017) on Wikitext-103). GRU hyperparameters were chosen in a similar fashion, but typically with more hidden units than in LSTM to keep the total network size approximately the same (since a GRU cell is more compact). In this section we brie\ufb02y study the effects of different com- ponents of a TCN layer. Overall, we believe dilation is required for modeling long-term dependencies, and so we mainly focus on two other factors here: the \ufb01lter size k used by each layer, and the effect of residual blocks. We perform a series of controlled experiments, with the results of the ablative analysis shown in Figure 6. As be- fore, we kept the model size and depth exactly the same for different models, so that the dilation factor is strictly con- trolled. The experiments were conducted on three different tasks: copy memory, permuted MNIST (P-MNIST), and Penn Treebank word-level language modeling. These ex- periments con\ufb01rm that both factors (\ufb01lter size and residual connections) contribute to sequence modeling performance. Filter size k. In both the copy memory and the P-MNIST tasks, we observed faster convergence and better accuracy for larger \ufb01lter sizes. In particular, looking at Figure 6a, a TCN with \ufb01lter size \u2264 3 only converges to the same level as random guessing. In contrast, on word-level language mod- eling, a smaller kernel with \ufb01lter size of k = 3 works best. We believe this is because a smaller kernel (along with \ufb01xed dilation) tends to focus more on the local context, which is especially important for PTB language modeling (in fact, the very success of n-gram models suggests that only a relatively short memory is needed for modeling language). Residual block. In all three scenarios that we compared here, we observed that the residual function stabilized train- ing and brought faster convergence with better \ufb01nal results. Especially in language modeling, we found that residual connections contribute substantially to performance (See Figure 6f)."
  },
  {
    "chunk_id": "55e01bc2-c8c4-4d13-89a0-17bc571ce932",
    "modality": "text",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": 11,
    "retrieval_text": "B. State-of-the-Art Results As previously noted, the generic TCN and LSTM/GRU models we used can be outperformed by more specialized architectures on some tasks. State-of-the-art results are summarized in Table 4. The same TCN architecture is used across all tasks. Note that the size of the state-of-the-art model may be different from the size of the TCN. D. Gating Mechanisms One component that had been used in prior work on con- volutional architectures for language modeling is the gated activation (van den Oord et al., 2016; Dauphin et al., 2017). We have chosen not to use gating in the generic TCN model. We now examine this choice more closely. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Table 2. TCN parameter settings for experiments in Section 5. TCN SETTINGS Dataset/Task Subtask k n Hidden Dropout Grad Clip Note T = 200 6 7 27 The Adding Problem T = 400 7 7 27 0.0 N/A T = 600 8 8 24 Seq. MNIST - 7 6 8 8 25 20 0.0 N/A Permuted MNIST - 7 6 8 8 25 20 0.0 N/A T = 500 6 9 10 Copy Memory Task T = 1000 8 8 10 0.05 1.0 RMSprop 5e-4 T = 2000 8 9 10 Music JSB Chorales - 3 2 150 0.5 0.4 Music Nottingham - 6 4 150 0.2 0.4 PTB 3 4 600 0.5 Embed. size 600 Word-level LM Wiki-103 LAMBADA 4 3 5 5 1000 500 0.4 0.4 Embed. size 400 Embed. size 500 Char-level LM PTB text8 3 2 3 5 450 520 0.1 0.15 Embed. size 100 Dauphin et al. (2017) compared the effects of gated linear units (GLU) and gated tanh units (GTU), and adopted GLU in their non-dilated gated ConvNet. Following the same choice, we now compare TCNs using ReLU and TCNs with gating (GLU), represented by an elementwise prod- uct between two convolutional layers, with one of them also passing through a sigmoid function \u03c3(x). Note that the gates architecture uses approximately twice as many convolutional layers as the ReLU-TCN. The results are shown in Table 5, where we kept the number of model parameters at about the same size. The GLU does further improve TCN accuracy on certain language modeling datasets like PTB, which agrees with prior work. However, we do not observe comparable bene\ufb01ts on other tasks, such as polyphonic music modeling or synthetic stress tests that require longer information retention. On the copy memory task with T = 1000, we found that TCN with gating converged to a worse result than TCN with ReLU (though still better than recurrent models). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Table 3. LSTM parameter settings for experiments in Section 5. LSTM SETTINGS (KEY PARAMETERS) Dataset/Task Subtask n Hidden Dropout Grad Clip Bias Note T = 200 2 77 50 5.0 SGD 1e-3 The Adding Problem T = 400 2 77 0.0 50 10.0 Adam 2e-3 T = 600 1 130 5 1.0 - Seq. MNIST - 1 130 0.0 1 1.0 RMSprop 1e-3 Permuted MNIST - 1 130 0.0 1 10.0 RMSprop 1e-3 T = 500 1 50 0.25 Copy Memory Task T = 1000 1 50 0.05 1 - RMSprop/Adam T = 2000 3 28 1 Music JSB Chorales - 2 200 0.2 1 10.0 SGD/Adam Music Nottingham - 3 1 280 500 0.1 0.5 1 - - Adam 4e-3 PTB 3 700 0.4 0.3 1.0 SGD 30, Emb. 700, etc. Word-level LM Wiki-103 - - - - - Grave et al. (2017) LAMBADA - - - - - Grave et al. (2017) Char-level LM PTB text8 2 1 600 1024 0.1 0.15 0.5 0.5 - - Emb. size 120 Adam 1e-2 Table 4. State-of-the-art (SoTA) results for tasks in Section 5. TCN VS. SOTA RESULTS Task TCN Result Size SoTA Size Model Seq. MNIST (acc.) 99.0 21K 99.0 21K Dilated GRU (Chang et al., 2017) P-MNIST (acc.) 97.2 42K 95.9 42K Zoneout (Krueger et al., 2017) Adding Prob. 600 (loss) 5.8e-5 70K 5.3e-5 70K Regularized GRU Copy Memory 1000 (loss) 3.5e-5 70K 0.011 70K EURNN (Jing et al., 2017) JSB Chorales (loss) 8.10 300K 3.47 - DBN+LSTM (Vohra et al., 2015) Nottingham (loss) 3.07 1M 1.32 - DBN+LSTM (Vohra et al., 2015) Word PTB (ppl) 88.68 13M 47.7 22M AWD-LSTM-MoS + Dynamic Eval. (Yang et al., 2018) Word Wiki-103 (ppl) 45.19 148M 40.4 >300M Neural Cache Model (Large) (Grave et al., 2017) Word LAMBADA (ppl) 1279 56M 138 >100M Neural Cache Model (Large) (Grave et al., 2017) Char PTB (bpc) 1.31 3M 1.22 14M 2-LayerNorm HyperLSTM (Ha et al., 2017) Char text8 (bpc) 1.45 4.6M 1.29 >12M HM-LSTM (Chung et al., 2016) An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (a) Different k on Copy Memory Task (b) Different k on P-MNIST (c) Different k on PTB (word) (d) Residual on Copy Memory Task (e) Residual on P-MNIST (f) Residual on PTB (word) Figure 6. Controlled experiments that study the effect of different components of the TCN model. Table 5. An evaluation of gating in TCN. A plain TCN is compared to a TCN that uses gated activations. Task TCN TCN + Gating Sequential MNIST (acc.) 99.0 99.0 Permuted MNIST (acc.) 97.2 96.9 Adding Problem T = 600 (loss) 5.8e-5 5.6e-5 Copy Memory T = 1000 (loss) 3.5e-5 0.00508 JSB Chorales (loss) 8.10 8.13 Nottingham (loss) 3.07 3.12 Word-level PTB (ppl) 88.68 87.94 Char-level PTB (bpc) 1.31 1.306 Char text8 (bpc) 1.45 1.485",
    "raw_text": "B. State-of-the-Art Results As previously noted, the generic TCN and LSTM/GRU models we used can be outperformed by more specialized architectures on some tasks. State-of-the-art results are summarized in Table 4. The same TCN architecture is used across all tasks. Note that the size of the state-of-the-art model may be different from the size of the TCN. D. Gating Mechanisms One component that had been used in prior work on con- volutional architectures for language modeling is the gated activation (van den Oord et al., 2016; Dauphin et al., 2017). We have chosen not to use gating in the generic TCN model. We now examine this choice more closely. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Table 2. TCN parameter settings for experiments in Section 5. TCN SETTINGS Dataset/Task Subtask k n Hidden Dropout Grad Clip Note T = 200 6 7 27 The Adding Problem T = 400 7 7 27 0.0 N/A T = 600 8 8 24 Seq. MNIST - 7 6 8 8 25 20 0.0 N/A Permuted MNIST - 7 6 8 8 25 20 0.0 N/A T = 500 6 9 10 Copy Memory Task T = 1000 8 8 10 0.05 1.0 RMSprop 5e-4 T = 2000 8 9 10 Music JSB Chorales - 3 2 150 0.5 0.4 Music Nottingham - 6 4 150 0.2 0.4 PTB 3 4 600 0.5 Embed. size 600 Word-level LM Wiki-103 LAMBADA 4 3 5 5 1000 500 0.4 0.4 Embed. size 400 Embed. size 500 Char-level LM PTB text8 3 2 3 5 450 520 0.1 0.15 Embed. size 100 Dauphin et al. (2017) compared the effects of gated linear units (GLU) and gated tanh units (GTU), and adopted GLU in their non-dilated gated ConvNet. Following the same choice, we now compare TCNs using ReLU and TCNs with gating (GLU), represented by an elementwise prod- uct between two convolutional layers, with one of them also passing through a sigmoid function \u03c3(x). Note that the gates architecture uses approximately twice as many convolutional layers as the ReLU-TCN. The results are shown in Table 5, where we kept the number of model parameters at about the same size. The GLU does further improve TCN accuracy on certain language modeling datasets like PTB, which agrees with prior work. However, we do not observe comparable bene\ufb01ts on other tasks, such as polyphonic music modeling or synthetic stress tests that require longer information retention. On the copy memory task with T = 1000, we found that TCN with gating converged to a worse result than TCN with ReLU (though still better than recurrent models). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Table 3. LSTM parameter settings for experiments in Section 5. LSTM SETTINGS (KEY PARAMETERS) Dataset/Task Subtask n Hidden Dropout Grad Clip Bias Note T = 200 2 77 50 5.0 SGD 1e-3 The Adding Problem T = 400 2 77 0.0 50 10.0 Adam 2e-3 T = 600 1 130 5 1.0 - Seq. MNIST - 1 130 0.0 1 1.0 RMSprop 1e-3 Permuted MNIST - 1 130 0.0 1 10.0 RMSprop 1e-3 T = 500 1 50 0.25 Copy Memory Task T = 1000 1 50 0.05 1 - RMSprop/Adam T = 2000 3 28 1 Music JSB Chorales - 2 200 0.2 1 10.0 SGD/Adam Music Nottingham - 3 1 280 500 0.1 0.5 1 - - Adam 4e-3 PTB 3 700 0.4 0.3 1.0 SGD 30, Emb. 700, etc. Word-level LM Wiki-103 - - - - - Grave et al. (2017) LAMBADA - - - - - Grave et al. (2017) Char-level LM PTB text8 2 1 600 1024 0.1 0.15 0.5 0.5 - - Emb. size 120 Adam 1e-2 Table 4. State-of-the-art (SoTA) results for tasks in Section 5. TCN VS. SOTA RESULTS Task TCN Result Size SoTA Size Model Seq. MNIST (acc.) 99.0 21K 99.0 21K Dilated GRU (Chang et al., 2017) P-MNIST (acc.) 97.2 42K 95.9 42K Zoneout (Krueger et al., 2017) Adding Prob. 600 (loss) 5.8e-5 70K 5.3e-5 70K Regularized GRU Copy Memory 1000 (loss) 3.5e-5 70K 0.011 70K EURNN (Jing et al., 2017) JSB Chorales (loss) 8.10 300K 3.47 - DBN+LSTM (Vohra et al., 2015) Nottingham (loss) 3.07 1M 1.32 - DBN+LSTM (Vohra et al., 2015) Word PTB (ppl) 88.68 13M 47.7 22M AWD-LSTM-MoS + Dynamic Eval. (Yang et al., 2018) Word Wiki-103 (ppl) 45.19 148M 40.4 >300M Neural Cache Model (Large) (Grave et al., 2017) Word LAMBADA (ppl) 1279 56M 138 >100M Neural Cache Model (Large) (Grave et al., 2017) Char PTB (bpc) 1.31 3M 1.22 14M 2-LayerNorm HyperLSTM (Ha et al., 2017) Char text8 (bpc) 1.45 4.6M 1.29 >12M HM-LSTM (Chung et al., 2016) An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (a) Different k on Copy Memory Task (b) Different k on P-MNIST (c) Different k on PTB (word) (d) Residual on Copy Memory Task (e) Residual on P-MNIST (f) Residual on PTB (word) Figure 6. Controlled experiments that study the effect of different components of the TCN model. Table 5. An evaluation of gating in TCN. A plain TCN is compared to a TCN that uses gated activations. Task TCN TCN + Gating Sequential MNIST (acc.) 99.0 99.0 Permuted MNIST (acc.) 97.2 96.9 Adding Problem T = 600 (loss) 5.8e-5 5.6e-5 Copy Memory T = 1000 (loss) 3.5e-5 0.00508 JSB Chorales (loss) 8.10 8.13 Nottingham (loss) 3.07 3.12 Word-level PTB (ppl) 88.68 87.94 Char-level PTB (bpc) 1.31 1.306 Char text8 (bpc) 1.45 1.485"
  },
  {
    "chunk_id": "4fefec45-5bd6-42da-a4cf-adbd09ef598e",
    "modality": "image",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of two photographs placed side by side. On the left side, there's a photograph of a whiteboard with various hand-drawn graphs and charts. These include line graphs, bar graphs, and pie charts, each representing different data sets. The graphs are labeled with text such as \"Speed,\" \"Distance,\" \"Time,\" \"Power,\" and \"Efficiency.\" There are also annotations like \"100%,\" \"50%,\" \"25%,\" and \"10%\" on the line graphs, indicating percentages of completion or progress. The right side of the image shows a computer screen with a similar set of graphs and charts, suggesting that the whiteboard is being used to explain or present data in a visual format.\n\nThe style of the image appears to be a screenshot from a video call or presentation, as indicated by the presence of a webcam feed on the right side showing a person's face, which is partially obscured by the computer screen. The overall impression is that someone is sharing information about data analysis or performance metrics during a remote meeting or class. ",
    "raw_text": null
  },
  {
    "chunk_id": "1e812c77-27cc-4acb-a2c6-7008e093cc04",
    "modality": "image",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of two separate graphs, each depicting different data sets with various statistical measures overlaid. On the left side of the image, there's a graph titled \"Temperature\" with three subplots:\n\n1. A line graph showing temperature measurements over time, with a horizontal axis labeled \"Time (s)\" ranging from 0 to 600 seconds and a vertical axis labeled \"Temperature (\u00b0C)\". The data points are plotted in blue, and there's a red dashed line that represents the mean temperature.\n\n2. A bar graph representing the standard deviation of the temperature measurements, with bars of varying heights corresponding to different time intervals. The horizontal axis is labeled \"Time (s)\" and the vertical axis is labeled \"Standard Deviation (\u00b0C)\".\n\n3. A histogram showing the distribution of temperature data points, with a bin width of 10 degrees Celsius. The x-axis is labeled \"Temperature (\u00b0C)\" and the y-axis is labeled \"Frequency\", indicating the number of occurrences within each temperature range.\n\nOn the right side of the image, there's another graph titled \"Current\" with three subplots:\n\n1. A line graph showing current measurements over time, with a horizontal axis labeled \"Time (s)\" ranging from 0 to 600 seconds and a vertical axis labeled \"Current (A)\". The data points are plotted in blue, and there's a red dashed line that represents the mean current.\n\n2. A bar graph representing the standard deviation of the current measurements, with bars of varying heights corresponding to different time intervals. The horizontal axis is labeled \"Time (s)\" and the vertical axis is labeled \"Standard Deviation (A)\".\n\n3. A histogram showing the distribution of current data points, with a bin width of 10 amperes. The x-axis is labeled \"Current (A)\" and the y-axis is labeled \"Frequency\", indicating the number of occurrences within each current range.\n\nBoth graphs have a common horizontal axis for time, ranging from 0 to 600 seconds, which suggests that both sets of data are being measured simultaneously over this time period. The image also contains text annotations and mathematical symbols, such as \"T\" for temperature, \"I\" for current, \"\u03c3\" for standard deviation, and \"\u03bc\" for mean.\n\nThe style of the image is a technical or scientific presentation, commonly used in engineering or physics to display time-series data with statistical measures. The graphs are presented in a clear and organized manner, with each subplot providing specific information about the temperature and current measurements at different times. ",
    "raw_text": null
  },
  {
    "chunk_id": "a03c713f-51b2-4b18-8e62-637da67a5025",
    "modality": "image",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of three separate plots, each representing different data sets related to a scientific study or experiment. The style of the image suggests that it is a screenshot from a presentation or a report, as indicated by the watermark \"GNU Image Manipulation Program\" in the bottom right corner.\n\n1. Top Plot: This plot shows three line graphs with different data sets represented by lines of varying colors and styles. The x-axis represents time in seconds, ranging from 0 to approximately 600 seconds, while the y-axis is labeled \"T/C,\" which likely stands for a ratio or a temperature coefficient. Each graph has a title at the top, but the text is too small to read clearly.\n\n2. Middle Plot: This plot features two line graphs with different data sets. The x-axis represents time in seconds, ranging from 0 to approximately 600 seconds, and the y-axis is labeled \"T/C,\" which likely stands for a ratio or a temperature coefficient. Each graph has a title at the top, but the text is too small to read clearly.\n\n3. Bottom Plot: This plot shows a line graph with a single data set represented by a green line. The x-axis represents time in seconds, ranging from 0 to approximately 600 seconds, and the y-axis is labeled \"T/C,\" which likely stands for a ratio or a temperature coefficient. Each graph has a title at the top, but the text is too small to read clearly.\n\nEach plot includes a legend with symbols corresponding to the lines in the graphs, indicating the different data sets being represented. The plots are overlaid on a gray background and are presented in a way that suggests they are part of a larger discussion or analysis related to temperature coefficients or ratios over time. ",
    "raw_text": null
  },
  {
    "chunk_id": "85610589-03a6-4d93-a7ed-b3d7b163f90e",
    "modality": "image",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of three separate plots, each representing different data sets related to machine learning performance metrics.\n\n1. Top left plot: This plot shows a line graph with two lines, one in red and the other in blue, representing two different models or datasets. Both lines are plotted against an x-axis labeled \"Listening Time (s)\" and a y-axis labeled \"Accuracy\". The red line represents a model named \"P.MNIST\", while the blue line represents a model named \"S.MNIST\". There is also a legend on the right side of the plot that distinguishes between the two lines.\n\n2. Top right plot: This plot is a bar graph with three bars, each representing a different model or dataset. The bars are labeled \"P.MNIST\", \"S.MNIST\", and \"C.MNIST\". Each bar has a height corresponding to an accuracy value, which is indicated by the y-axis on the right side of the plot.\n\n3. Bottom left plot: This plot is a scatter plot with two sets of data points, one in red and the other in blue, representing two different models or datasets. The x-axis is labeled \"Listening Time (s)\" and the y-axis is labeled \"Accuracy\". There are also two lines on the graph, one in red and the other in blue, which seem to represent a trend line for each dataset.\n\nEach plot has a title that reads \"Sequential MNIST\", indicating that the data pertains to a sequence recognition task using the MNIST dataset. The plots are arranged in a 3x1 grid format, with the top row containing two plots and the bottom row containing one. The style of the image is technical and informational, typical of scientific or engineering presentations. ",
    "raw_text": null
  },
  {
    "chunk_id": "786699e9-20a1-4781-aa8d-85f17d055800",
    "modality": "image",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": null,
    "retrieval_text": " The image is a graphical representation of data, specifically a line graph with annotations. At the top left corner, there's a label \"T/C\" followed by a sequence of numbers: 10K, 10K, GRU, 10K, and 10K, indicating different categories or groups. The x-axis is labeled \"Sequence length (t)\" with values ranging from 50 to 250 in increments of 50. The y-axis is labeled \"Accuracy\" with values ranging from 0 to 100.\n\nOn the right side of the graph, there are three bars representing different groups: LSTM (blue), GRU (red), and Random (green). Each bar has a label indicating the type of model or method it represents. The blue line labeled \"LSTM\" shows a gradual increase in accuracy as sequence length increases, with a peak at 10K and a slight decrease thereafter. The red line labeled \"GRU\" also shows an increase in accuracy but with a more pronounced peak at 10K, followed by a sharp drop. The green bar labeled \"Random\" is flat, indicating no change in accuracy across the sequence lengths shown.\n\nBelow the graph, there's a legend explaining the symbols used: GRU (a red rectangle), LSTM (a blue rectangle), and Random (a green rectangle). Each symbol corresponds to one of the bars on the graph.\n\nThe image also contains text annotations that provide additional information about the data presented. The text \"T/C\" is likely an abbreviation for \"Training/Control,\" suggesting a comparison between different models or methods during training and control phases. The text \"10K\" indicates a specific sequence length of 10,000 tokens.\n\nThe style of the image suggests it's a scientific or technical presentation, possibly from a research paper or conference poster, designed to compare the performance of different machine learning models across varying sequence lengths. ",
    "raw_text": null
  },
  {
    "chunk_id": "1a688d64-d994-41e4-95ae-40cb490f51e9",
    "modality": "image",
    "source_pdf": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of four separate graphs, each depicting different data sets related to machine learning performance metrics. These graphs are arranged in a grid format with two rows and two columns.\n\nIn the top left graph, there's a line chart showing \"Different from Copy\" on the x-axis and \"Memory\" on the y-axis. The chart has multiple lines representing different data sets, each with its own color and label.\n\nThe top right graph is another line chart, this time with \"Copy\" on the x-axis and \"Memory\" on the y-axis. It also features several lines representing various data sets.\n\nThe bottom left graph is a bar chart with \"Different from Copy\" on the x-axis and \"Memory\" on the y-axis. Each bar represents a different data set, and they are color-coded for easy identification.\n\nFinally, the bottom right graph is a scatter plot with \"Copy\" on the x-axis and \"Memory\" on the y-axis. It also includes several lines representing different data sets, each with its own color and label.\n\nEach graph has a title that reads \"Different from Copy,\" indicating that they are comparing some form of \"Copy\" metric against a \"Different from Copy\" metric across various memory conditions. The specific details about the axes labels, the units of measurement for \"Memory,\" and the exact nature of the data sets (e.g., what \"Copy\" refers to) are not provided in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "b0909465-9d41-4645-a1b3-ef21a88fcaaf",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 1,
    "retrieval_text": "9 2019 1 0 2 r a M 1 2 ] V C . s c [ 4 v 1 8 3 4 0 . 1 0 8 1 : v i X r a MobileNetV2: Inverted Residuals and Linear Bottlenecks Mark Sandler Andrew Howard Menglong Zhu Andrey Zhmoginov Liang-Chieh Chen Google Inc. {sandler, howarda, menglong, azhmogin, lcchen}@google.com Abstract applications. In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art perfor- mance of mobile models on multiple tasks and bench- marks as well as across a spectrum of different model sizes. We also describe ef\ufb01cient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottle- neck layers. The intermediate expansion layer uses lightweight depthwise convolutions to \ufb01lter features as a source of non-linearity. Additionally, we \ufb01nd that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demon- strate that this improves performance and provide an in- tuition that led to this design. Finally, our approach allows decoupling of the in- put/output domains from the expressiveness of the trans- formation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classi\ufb01cation, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters. This paper introduces a new neural network architec- ture that is speci\ufb01cally tailored for mobile and resource constrained environments. Our network pushes the state of the art for mobile tailored computer vision models, by signi\ufb01cantly decreasing the number of operations and memory needed while retaining the same accuracy. Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This mod- ule takes as an input a low-dimensional compressed representation which is \ufb01rst expanded to high dimen- sion and \ufb01ltered with a lightweight depthwise convo- lution. Features are subsequently projected back to a low-dimensional representation with a linear convolu- tion. The of\ufb01cial implementation is available as part of TensorFlow-Slim model library in [4]. This module can be ef\ufb01ciently implemented using standard operations in any modern framework and al- lows our models to beat state of the art along multiple performance points using standard benchmarks. Fur- thermore, this convolutional module is particularly suit- able for mobile designs, because it allows to signi\ufb01- cantly reduce the memory footprint needed during in- ference by never fully materializing large intermediate tensors. This reduces the need for main memory access in many embedded hardware designs, that provide small amounts of very fast software controlled cache memory.",
    "raw_text": "9 2019 1 0 2 r a M 1 2 ] V C . s c [ 4 v 1 8 3 4 0 . 1 0 8 1 : v i X r a MobileNetV2: Inverted Residuals and Linear Bottlenecks Mark Sandler Andrew Howard Menglong Zhu Andrey Zhmoginov Liang-Chieh Chen Google Inc. {sandler, howarda, menglong, azhmogin, lcchen}@google.com Abstract applications. In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art perfor- mance of mobile models on multiple tasks and bench- marks as well as across a spectrum of different model sizes. We also describe ef\ufb01cient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottle- neck layers. The intermediate expansion layer uses lightweight depthwise convolutions to \ufb01lter features as a source of non-linearity. Additionally, we \ufb01nd that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demon- strate that this improves performance and provide an in- tuition that led to this design. Finally, our approach allows decoupling of the in- put/output domains from the expressiveness of the trans- formation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classi\ufb01cation, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters. This paper introduces a new neural network architec- ture that is speci\ufb01cally tailored for mobile and resource constrained environments. Our network pushes the state of the art for mobile tailored computer vision models, by signi\ufb01cantly decreasing the number of operations and memory needed while retaining the same accuracy. Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This mod- ule takes as an input a low-dimensional compressed representation which is \ufb01rst expanded to high dimen- sion and \ufb01ltered with a lightweight depthwise convo- lution. Features are subsequently projected back to a low-dimensional representation with a linear convolu- tion. The of\ufb01cial implementation is available as part of TensorFlow-Slim model library in [4]. This module can be ef\ufb01ciently implemented using standard operations in any modern framework and al- lows our models to beat state of the art along multiple performance points using standard benchmarks. Fur- thermore, this convolutional module is particularly suit- able for mobile designs, because it allows to signi\ufb01- cantly reduce the memory footprint needed during in- ference by never fully materializing large intermediate tensors. This reduces the need for main memory access in many embedded hardware designs, that provide small amounts of very fast software controlled cache memory."
  },
  {
    "chunk_id": "1c12df83-0fd3-42fe-9eb2-7d99cafd911b",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 1,
    "retrieval_text": "2. Related Work 1. Introduction Neural networks have revolutionized many areas of machine intelligence, enabling superhuman accuracy for challenging image recognition tasks. However, the drive to improve accuracy often comes at a cost: modern state of the art networks require high computational resources beyond the capabilities of many mobile and embedded Tuning deep neural architectures to strike an optimal balance between accuracy and performance has been an area of active research for the last several years. Both manual architecture search and improvements in training algorithms, carried out by numerous teams has lead to dramatic improvements over early designs such as AlexNet [5], VGGNet [6], GoogLeNet [7]. , and ResNet [8]. Recently there has been lots of progress in algorithmic architecture exploration included hyper- parameter optimization [9, 10, 11] as well as various methods of network pruning [12, 13, 14, 15, 16, 17] and connectivity learning [18, 19]. A substantial amount of work has also been dedicated to changing the connectiv- ity structure of the internal convolutional blocks such as in Shuf\ufb02eNet [20] or introducing sparsity [21] and oth- ers [22]. Recently, [23, 24, 25, 26], opened up a new direc- tion of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search. However one drawback is that the resulting net- works end up very complex. In this paper, we pursue the goal of developing better intuition about how neural net- works operate and use that to guide the simplest possible network design. Our approach should be seen as compli- mentary to the one described in [23] and related work. In this vein our approach is similar to those taken by [20, 22] and allows to further improve the performance, while providing a glimpse on its internal operation. Our network design is based on MobileNetV1 [27]. It re- tains its simplicity and does not require any special op- erators while signi\ufb01cantly improves its accuracy, achiev- ing state of the art on multiple image classi\ufb01cation and detection tasks for mobile applications.",
    "raw_text": "2. Related Work 1. Introduction Neural networks have revolutionized many areas of machine intelligence, enabling superhuman accuracy for challenging image recognition tasks. However, the drive to improve accuracy often comes at a cost: modern state of the art networks require high computational resources beyond the capabilities of many mobile and embedded Tuning deep neural architectures to strike an optimal balance between accuracy and performance has been an area of active research for the last several years. Both manual architecture search and improvements in training algorithms, carried out by numerous teams has lead to dramatic improvements over early designs such as AlexNet [5], VGGNet [6], GoogLeNet [7]. , and ResNet [8]. Recently there has been lots of progress in algorithmic architecture exploration included hyper- parameter optimization [9, 10, 11] as well as various methods of network pruning [12, 13, 14, 15, 16, 17] and connectivity learning [18, 19]. A substantial amount of work has also been dedicated to changing the connectiv- ity structure of the internal convolutional blocks such as in Shuf\ufb02eNet [20] or introducing sparsity [21] and oth- ers [22]. Recently, [23, 24, 25, 26], opened up a new direc- tion of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search. However one drawback is that the resulting net- works end up very complex. In this paper, we pursue the goal of developing better intuition about how neural net- works operate and use that to guide the simplest possible network design. Our approach should be seen as compli- mentary to the one described in [23] and related work. In this vein our approach is similar to those taken by [20, 22] and allows to further improve the performance, while providing a glimpse on its internal operation. Our network design is based on MobileNetV1 [27]. It re- tains its simplicity and does not require any special op- erators while signi\ufb01cantly improves its accuracy, achiev- ing state of the art on multiple image classi\ufb01cation and detection tasks for mobile applications."
  },
  {
    "chunk_id": "78d8b55d-278c-4a74-ae4d-2af83d78dc7c",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 2,
    "retrieval_text": "3. Preliminaries, discussion and intuition 3.1. Depthwise Separable Convolutions Depthwise Separable Convolutions are a key build- ing block for many ef\ufb01cient neural network architectures [27, 28, 20] and we use them in the present work as well. The basic idea is to replace a full convolutional opera- tor with a factorized version that splits convolution into two separate layers. The \ufb01rst layer is called a depthwise convolution, it performs lightweight \ufb01ltering by apply- ing a single convolutional \ufb01lter per input channel. The second layer is a 1 \u00d7 1 convolution, called a pointwise convolution, which is responsible for building new fea- tures through computing linear combinations of the in- put channels. Standard convolution takes an hi \u00d7 wi \u00d7 di in- put tensor Li, and applies convolutional kernel K \u2208 Rk\u00d7k\u00d7di\u00d7dj to produce an hi \u00d7 wi \u00d7 dj output ten- sor Lj. Standard convolutional layers have the compu- tational cost of hi \u00b7 wi \u00b7 di \u00b7 dj \u00b7 k \u00b7 k. Depthwise separable convolutions are a drop-in re- placement for standard convolutional layers. Empiri- cally they work almost as well as regular convolutions but only cost: hi \u00b7 wi \u00b7 di(k2 + dj) (1) which is the sum of the depthwise and 1 \u00d7 1 pointwise convolutions. Effectively depthwise separable convolu- tion reduces computation compared to traditional layers by almost a factor of k21. MobileNetV2 uses k = 3 (3 \u00d7 3 depthwise separable convolutions) so the compu- tational cost is 8 to 9 times smaller than that of standard convolutions at only a small reduction in accuracy [27]. 3.2. Linear Bottlenecks Consider a deep neural network consisting of n layers Li each of which has an activation tensor of dimensions hi \u00d7 wi \u00d7 di. Throughout this section we will be dis- cussing the basic properties of these activation tensors, which we will treat as containers of hi \u00d7 wi \u201cpixels\u201d with di dimensions. Informally, for an input set of real images, we say that the set of layer activations (for any layer Li) forms a \u201cmanifold of interest\u201d. It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. other words, when we look at all individual d-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional sub- space2. In At a \ufb01rst glance, such a fact could then be captured and exploited by simply reducing the dimensionality of a layer thus reducing the dimensionality of the oper- ating space. This has been successfully exploited by MobileNetV1 [27] to effectively trade off between com- putation and accuracy via a width multiplier parameter, and has been incorporated into ef\ufb01cient model designs of other networks as well [20]. Following that intuition, the width multiplier approach allows one to reduce the dimensionality of the activation space until the mani- fold of interest spans this entire space. However, this intuition breaks down when we recall that deep convo- lutional neural networks actually have non-linear per co- ordinate transformations, such as ReLU. For example, ReLU applied to a line in 1D space produces a \u2019ray\u2019, where as in Rn space, it generally results in a piece-wise linear curve with n-joints. It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interiorS are obtained via a lin- ear transformation B of the input, thus indicating that the part of the input space corresponding to the full di- mensional output, is limited to a linear transformation. In other words, deep networks only have the power of a linear classi\ufb01er on the non-zero volume part of the 1more precisely, by a factor k2dj/(k2 + dj) 2Note that dimensionality of the manifold differs from the dimen- sionality of a subspace that could be embedded via a linear transfor- mation. Outputidim=2 _Output/dim=3 Figure 1: Examples of ReLU transformations of low-dimensional manifolds embedded in higher-dimensional spaces. In these examples the initial spiral is embedded into an n-dimensional space using random matrix T followed by ReLU, and then projected back to the 2D space using T \u22121. In examples above n = 2,3 result in information loss where certain points of the manifold collapse into each other, while for n = 15 to 30 the transformation is highly non-convex. (a) Regular (b) Separable Regular Convolution \u2018separable Convolution Black (c) Separable with linear bottleneck (d) Bottleneck with ex- pansion layer Figure 2: Evolution of separable convolution blocks. The diagonally hatched texture indicates layers that do not contain non-linearities. The last (lightly colored) layer indicates the beginning of the next block. Note: 2d and 2c are equivalent blocks when stacked. Best viewed in color. output domain. We refer to supplemental material for a more formal statement. On the other hand, when ReLU collapses the chan- nel, it inevitably loses information in that channel. How- ever if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental ma- terials, we show that if the input manifold can be em- bedded into a signi\ufb01cantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions. To summarize, we have highlighted two properties that are indicative of the requirement that the manifold of interest should lie in a low-dimensional subspace of the higher-dimensional activation space: 1. If the manifold of interest remains non-zero vol- ume after ReLU transformation, it corresponds to a linear transformation. (a) Residual block (b) Inverted residual block Figure 3: The difference between residual block [8, 30] and inverted residual. Diagonally hatched layers do not use non-linearities. We use thickness of each block to indicate its relative number of channels. Note how clas- sical residuals connects the layers with high number of channels, whereas the inverted residuals connect the bot- tlenecks. Best viewed in color. 2. ReLU is capable of preserving complete informa- tion about the input manifold, but only if the input manifold lies in a low-dimensional subspace of the input space. These two insights provide us with an empirical hint for optimizing existing neural architectures: assuming the manifold of interest is low-dimensional we can cap- ture this by inserting linear bottleneck layers into the convolutional blocks. Experimental evidence suggests that using linear layers is crucial as it prevents non- linearities from destroying too much information. Section 6, we show empirically that using non-linear layers in bottlenecks indeed hurts the performance by several percent, further validating our hypothesis3. We note that similar reports where non-linearity was helped were reported in [29] where non-linearity was removed from the input of the traditional residual block and that lead to improved performance on CIFAR dataset. In For the remainder of this paper we will be utilizing bottleneck convolutions. We will refer to the ratio be- tween the size of the input bottleneck and the inner size as the expansion ratio.",
    "raw_text": "3. Preliminaries, discussion and intuition 3.1. Depthwise Separable Convolutions Depthwise Separable Convolutions are a key build- ing block for many ef\ufb01cient neural network architectures [27, 28, 20] and we use them in the present work as well. The basic idea is to replace a full convolutional opera- tor with a factorized version that splits convolution into two separate layers. The \ufb01rst layer is called a depthwise convolution, it performs lightweight \ufb01ltering by apply- ing a single convolutional \ufb01lter per input channel. The second layer is a 1 \u00d7 1 convolution, called a pointwise convolution, which is responsible for building new fea- tures through computing linear combinations of the in- put channels. Standard convolution takes an hi \u00d7 wi \u00d7 di in- put tensor Li, and applies convolutional kernel K \u2208 Rk\u00d7k\u00d7di\u00d7dj to produce an hi \u00d7 wi \u00d7 dj output ten- sor Lj. Standard convolutional layers have the compu- tational cost of hi \u00b7 wi \u00b7 di \u00b7 dj \u00b7 k \u00b7 k. Depthwise separable convolutions are a drop-in re- placement for standard convolutional layers. Empiri- cally they work almost as well as regular convolutions but only cost: hi \u00b7 wi \u00b7 di(k2 + dj) (1) which is the sum of the depthwise and 1 \u00d7 1 pointwise convolutions. Effectively depthwise separable convolu- tion reduces computation compared to traditional layers by almost a factor of k21. MobileNetV2 uses k = 3 (3 \u00d7 3 depthwise separable convolutions) so the compu- tational cost is 8 to 9 times smaller than that of standard convolutions at only a small reduction in accuracy [27]. 3.2. Linear Bottlenecks Consider a deep neural network consisting of n layers Li each of which has an activation tensor of dimensions hi \u00d7 wi \u00d7 di. Throughout this section we will be dis- cussing the basic properties of these activation tensors, which we will treat as containers of hi \u00d7 wi \u201cpixels\u201d with di dimensions. Informally, for an input set of real images, we say that the set of layer activations (for any layer Li) forms a \u201cmanifold of interest\u201d. It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. other words, when we look at all individual d-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional sub- space2. In At a \ufb01rst glance, such a fact could then be captured and exploited by simply reducing the dimensionality of a layer thus reducing the dimensionality of the oper- ating space. This has been successfully exploited by MobileNetV1 [27] to effectively trade off between com- putation and accuracy via a width multiplier parameter, and has been incorporated into ef\ufb01cient model designs of other networks as well [20]. Following that intuition, the width multiplier approach allows one to reduce the dimensionality of the activation space until the mani- fold of interest spans this entire space. However, this intuition breaks down when we recall that deep convo- lutional neural networks actually have non-linear per co- ordinate transformations, such as ReLU. For example, ReLU applied to a line in 1D space produces a \u2019ray\u2019, where as in Rn space, it generally results in a piece-wise linear curve with n-joints. It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interiorS are obtained via a lin- ear transformation B of the input, thus indicating that the part of the input space corresponding to the full di- mensional output, is limited to a linear transformation. In other words, deep networks only have the power of a linear classi\ufb01er on the non-zero volume part of the 1more precisely, by a factor k2dj/(k2 + dj) 2Note that dimensionality of the manifold differs from the dimen- sionality of a subspace that could be embedded via a linear transfor- mation. Outputidim=2 _Output/dim=3 Figure 1: Examples of ReLU transformations of low-dimensional manifolds embedded in higher-dimensional spaces. In these examples the initial spiral is embedded into an n-dimensional space using random matrix T followed by ReLU, and then projected back to the 2D space using T \u22121. In examples above n = 2,3 result in information loss where certain points of the manifold collapse into each other, while for n = 15 to 30 the transformation is highly non-convex. (a) Regular (b) Separable Regular Convolution \u2018separable Convolution Black (c) Separable with linear bottleneck (d) Bottleneck with ex- pansion layer Figure 2: Evolution of separable convolution blocks. The diagonally hatched texture indicates layers that do not contain non-linearities. The last (lightly colored) layer indicates the beginning of the next block. Note: 2d and 2c are equivalent blocks when stacked. Best viewed in color. output domain. We refer to supplemental material for a more formal statement. On the other hand, when ReLU collapses the chan- nel, it inevitably loses information in that channel. How- ever if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental ma- terials, we show that if the input manifold can be em- bedded into a signi\ufb01cantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions. To summarize, we have highlighted two properties that are indicative of the requirement that the manifold of interest should lie in a low-dimensional subspace of the higher-dimensional activation space: 1. If the manifold of interest remains non-zero vol- ume after ReLU transformation, it corresponds to a linear transformation. (a) Residual block (b) Inverted residual block Figure 3: The difference between residual block [8, 30] and inverted residual. Diagonally hatched layers do not use non-linearities. We use thickness of each block to indicate its relative number of channels. Note how clas- sical residuals connects the layers with high number of channels, whereas the inverted residuals connect the bot- tlenecks. Best viewed in color. 2. ReLU is capable of preserving complete informa- tion about the input manifold, but only if the input manifold lies in a low-dimensional subspace of the input space. These two insights provide us with an empirical hint for optimizing existing neural architectures: assuming the manifold of interest is low-dimensional we can cap- ture this by inserting linear bottleneck layers into the convolutional blocks. Experimental evidence suggests that using linear layers is crucial as it prevents non- linearities from destroying too much information. Section 6, we show empirically that using non-linear layers in bottlenecks indeed hurts the performance by several percent, further validating our hypothesis3. We note that similar reports where non-linearity was helped were reported in [29] where non-linearity was removed from the input of the traditional residual block and that lead to improved performance on CIFAR dataset. In For the remainder of this paper we will be utilizing bottleneck convolutions. We will refer to the ratio be- tween the size of the input bottleneck and the inner size as the expansion ratio."
  },
  {
    "chunk_id": "3d320f96-74f4-4c94-a914-efec358dda0d",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 3,
    "retrieval_text": "3.3. Inverted residuals The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the ten- sor, we use shortcuts directly between the bottlenecks. 3We note that in the presence of shortcuts the information loss is actually less strong. Figure 3 provides a schematic visualization of the differ- ence in the designs. The motivation for inserting short- cuts is similar to that of classical residual connections: we want to improve the ability of a gradient to propagate across multiplier layers. However, the inverted design is considerably more memory ef\ufb01cient (see Section 5 for details), as well as works slightly better in our experi- ments. Running time and parameter count for bottleneck convolution The basic implementation structure is il- lustrated in Table 1. For a block of size h x w, ex- pansion factor t and kernel size k with d\u2019 input chan- nels and d\u201d output channels, the total number of multi- ply add required is h - w- d! - t(d! + k? + d\u201d). Com- pared with (1) this expression has an extra term, as in- deed we have an extra 1 x 1 convolution, however the nature of our networks allows us to utilize much smaller input and output dimensions. In Table 3 we compare the needed sizes for each resolution between MobileNetV1, MobileNetV2 and ShuffleNet. 3.4. Information \ufb02ow interpretation One interesting property of our architecture is that it provides a natural separation between the input/output domains of the building blocks (bottleneck layers), and the layer transformation \u2013 that is a non-linear function that converts input to the output. The former can be seen as the capacity of the network at each layer, whereas the latter as the expressiveness. This is in contrast with tra- ditional convolutional blocks, both regular and separa- ble, where both expressiveness and capacity are tangled together and are functions of the output layer depth. In particular, in our case, when inner layer depth is 0 the underlying convolution is the identity function thanks to the shortcut connection. When the expansion ratio is smaller than 1, this is a classical residual con- volutional block [8, 30]. However, for our purposes we show that expansion ratio greater than 1 is the most use- ful. This interpretation allows us to study the expressive- ness of the network separately from its capacity and we believe that further exploration of this separation is war- ranted to provide a better understanding of the network properties.",
    "raw_text": "3.3. Inverted residuals The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the ten- sor, we use shortcuts directly between the bottlenecks. 3We note that in the presence of shortcuts the information loss is actually less strong. Figure 3 provides a schematic visualization of the differ- ence in the designs. The motivation for inserting short- cuts is similar to that of classical residual connections: we want to improve the ability of a gradient to propagate across multiplier layers. However, the inverted design is considerably more memory ef\ufb01cient (see Section 5 for details), as well as works slightly better in our experi- ments. Running time and parameter count for bottleneck convolution The basic implementation structure is il- lustrated in Table 1. For a block of size h x w, ex- pansion factor t and kernel size k with d\u2019 input chan- nels and d\u201d output channels, the total number of multi- ply add required is h - w- d! - t(d! + k? + d\u201d). Com- pared with (1) this expression has an extra term, as in- deed we have an extra 1 x 1 convolution, however the nature of our networks allows us to utilize much smaller input and output dimensions. In Table 3 we compare the needed sizes for each resolution between MobileNetV1, MobileNetV2 and ShuffleNet. 3.4. Information \ufb02ow interpretation One interesting property of our architecture is that it provides a natural separation between the input/output domains of the building blocks (bottleneck layers), and the layer transformation \u2013 that is a non-linear function that converts input to the output. The former can be seen as the capacity of the network at each layer, whereas the latter as the expressiveness. This is in contrast with tra- ditional convolutional blocks, both regular and separa- ble, where both expressiveness and capacity are tangled together and are functions of the output layer depth. In particular, in our case, when inner layer depth is 0 the underlying convolution is the identity function thanks to the shortcut connection. When the expansion ratio is smaller than 1, this is a classical residual con- volutional block [8, 30]. However, for our purposes we show that expansion ratio greater than 1 is the most use- ful. This interpretation allows us to study the expressive- ness of the network separately from its capacity and we believe that further exploration of this separation is war- ranted to provide a better understanding of the network properties."
  },
  {
    "chunk_id": "ff59ccd0-b472-4770-945e-18aa475abadf",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 4,
    "retrieval_text": "4. Model Architecture Now we describe our architecture in detail. As dis- cussed in the previous section the basic building block is a bottleneck depth-separable convolution with resid- uals. The detailed structure of this block is shown in Input Operator Output hxwxk 1x1 conv2d,ReLU6 | hx w x (tk) hx wx tk | 3x3 dwises=s,ReLU6 | 4 x % x (tk) &x\"xtk | \u2014 linear Ixl conv2d Ax Bx k Table 1: Bottleneck residual block transforming from k to k\u2019 channels, with stride s, and expansion factor t. Table 1. The architecture of MobileNetV2 contains the initial fully convolution layer with 32 \ufb01lters, followed by 19 residual bottleneck layers described in the Ta- ble 2. We use ReLU6 as the non-linearity because of its robustness when used with low-precision computa- tion [27]. We always use kernel size 3\u00d73 as is standard for modern networks, and utilize dropout and batch nor- malization during training. With the exception of the \ufb01rst layer, we use constant expansion rate throughout the network. In our experi- ments we \ufb01nd that expansion rates between 5 and 10 re- sult in nearly identical performance curves, with smaller networks being better off with slightly smaller expan- sion rates and larger networks having slightly better per- formance with larger expansion rates. For all our main experiments we use expansion factor of 6 applied to the size of the input tensor. For example, for a bottleneck layer that takes 64-channel input tensor and produces a tensor with 128 channels, the intermedi- ate expansion layer is then 64 \u00b7 6 = 384 channels. Trade-off hyper parameters As in [27] we tailor our architecture to different performance points, by using the input image resolution and width multiplier as tun- able hyper parameters, that can be adjusted depending on desired accuracy/performance trade-offs. Our pri- mary network (width multiplier 1, 224 \u00d7 224), has a computational cost of 300 million multiply-adds and uses 3.4 million parameters. We explore the perfor- mance trade offs, for input resolutions from 96 to 224, and width multipliers of 0.35 to 1.4. The network com- putational cost ranges from 7 multiply adds to 585M MAdds, while the model size vary between 1.7M and 6.9M parameters. One minor implementation difference, with [27] is that for multipliers less than one, we apply width multi- plier to all layers except the very last convolutional layer. This improves performance for smaller models. Input Operator t c n s 2242 \u00d7 3 conv2d - 32 1 2 1122 \u00d7 32 bottleneck 1 16 1 1 1122 \u00d7 16 bottleneck 6 24 2 2 562 \u00d7 24 bottleneck 6 32 3 2 282 \u00d7 32 bottleneck 6 64 4 2 142 \u00d7 64 bottleneck 6 96 3 1 142 \u00d7 96 bottleneck 6 160 3 2 72 \u00d7 160 bottleneck 6 320 1 1 72 \u00d7 320 conv2d 1x1 - 1280 1 1 72 \u00d7 1280 avgpool 7x7 - - 1 - 1 \u00d7 1 \u00d7 1280 conv2d 1x1 - k - Table 2: MobileNetV2 : Each line describes a sequence of 1 or more identical (modulo stride) layers, repeated n times. All layers in the same sequence have the same number c of output channels. The \ufb01rst layer of each sequence has a stride s and all others use stride 1. All spatial convolutions use 3 \u00d7 3 kernels. The expansion factor t is always applied to the input size as described in Table 1. Size MobileNetV1 MobileNetV2 Shuf\ufb02eNet (2x,g=3) 112x112 64/1600 16/400 32/800 56x56 128/800 32/200 48/300 28x28 256/400 64/100 400/600K 14x14 512/200 160/62 800/310 7x7 1024/199 320/32 1600/156 1x1 1024/2 1280/2 1600/3 max 1600K 400K 600K Table 3: The max number of channels/memory (in Kb) that needs to be materialized at each spatial res- olution for different architectures. We assume 16-bit \ufb02oats for activations. For Shuf\ufb02eNet, we use 2x,g = 3 that matches the performance of MobileNetV1 and MobileNetV2. For the \ufb01rst layer of MobileNetV2 and Shuf\ufb02eNet we can employ the trick described in Sec- tion 5 to reduce memory requirement. Even though Shuf\ufb02eNet employs bottlenecks elsewhere, the non- bottleneck tensors still need to be materialized due to the presence of shortcuts between the non-bottleneck ten- sors.",
    "raw_text": "4. Model Architecture Now we describe our architecture in detail. As dis- cussed in the previous section the basic building block is a bottleneck depth-separable convolution with resid- uals. The detailed structure of this block is shown in Input Operator Output hxwxk 1x1 conv2d,ReLU6 | hx w x (tk) hx wx tk | 3x3 dwises=s,ReLU6 | 4 x % x (tk) &x\"xtk | \u2014 linear Ixl conv2d Ax Bx k Table 1: Bottleneck residual block transforming from k to k\u2019 channels, with stride s, and expansion factor t. Table 1. The architecture of MobileNetV2 contains the initial fully convolution layer with 32 \ufb01lters, followed by 19 residual bottleneck layers described in the Ta- ble 2. We use ReLU6 as the non-linearity because of its robustness when used with low-precision computa- tion [27]. We always use kernel size 3\u00d73 as is standard for modern networks, and utilize dropout and batch nor- malization during training. With the exception of the \ufb01rst layer, we use constant expansion rate throughout the network. In our experi- ments we \ufb01nd that expansion rates between 5 and 10 re- sult in nearly identical performance curves, with smaller networks being better off with slightly smaller expan- sion rates and larger networks having slightly better per- formance with larger expansion rates. For all our main experiments we use expansion factor of 6 applied to the size of the input tensor. For example, for a bottleneck layer that takes 64-channel input tensor and produces a tensor with 128 channels, the intermedi- ate expansion layer is then 64 \u00b7 6 = 384 channels. Trade-off hyper parameters As in [27] we tailor our architecture to different performance points, by using the input image resolution and width multiplier as tun- able hyper parameters, that can be adjusted depending on desired accuracy/performance trade-offs. Our pri- mary network (width multiplier 1, 224 \u00d7 224), has a computational cost of 300 million multiply-adds and uses 3.4 million parameters. We explore the perfor- mance trade offs, for input resolutions from 96 to 224, and width multipliers of 0.35 to 1.4. The network com- putational cost ranges from 7 multiply adds to 585M MAdds, while the model size vary between 1.7M and 6.9M parameters. One minor implementation difference, with [27] is that for multipliers less than one, we apply width multi- plier to all layers except the very last convolutional layer. This improves performance for smaller models. Input Operator t c n s 2242 \u00d7 3 conv2d - 32 1 2 1122 \u00d7 32 bottleneck 1 16 1 1 1122 \u00d7 16 bottleneck 6 24 2 2 562 \u00d7 24 bottleneck 6 32 3 2 282 \u00d7 32 bottleneck 6 64 4 2 142 \u00d7 64 bottleneck 6 96 3 1 142 \u00d7 96 bottleneck 6 160 3 2 72 \u00d7 160 bottleneck 6 320 1 1 72 \u00d7 320 conv2d 1x1 - 1280 1 1 72 \u00d7 1280 avgpool 7x7 - - 1 - 1 \u00d7 1 \u00d7 1280 conv2d 1x1 - k - Table 2: MobileNetV2 : Each line describes a sequence of 1 or more identical (modulo stride) layers, repeated n times. All layers in the same sequence have the same number c of output channels. The \ufb01rst layer of each sequence has a stride s and all others use stride 1. All spatial convolutions use 3 \u00d7 3 kernels. The expansion factor t is always applied to the input size as described in Table 1. Size MobileNetV1 MobileNetV2 Shuf\ufb02eNet (2x,g=3) 112x112 64/1600 16/400 32/800 56x56 128/800 32/200 48/300 28x28 256/400 64/100 400/600K 14x14 512/200 160/62 800/310 7x7 1024/199 320/32 1600/156 1x1 1024/2 1280/2 1600/3 max 1600K 400K 600K Table 3: The max number of channels/memory (in Kb) that needs to be materialized at each spatial res- olution for different architectures. We assume 16-bit \ufb02oats for activations. For Shuf\ufb02eNet, we use 2x,g = 3 that matches the performance of MobileNetV1 and MobileNetV2. For the \ufb01rst layer of MobileNetV2 and Shuf\ufb02eNet we can employ the trick described in Sec- tion 5 to reduce memory requirement. Even though Shuf\ufb02eNet employs bottlenecks elsewhere, the non- bottleneck tensors still need to be materialized due to the presence of shortcuts between the non-bottleneck ten- sors."
  },
  {
    "chunk_id": "1ddc9fae-8e0d-4c8d-bc0e-99c16328e88c",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 5,
    "retrieval_text": "5. Implementation Notes 5.1. Memory ef\ufb01cient inference The inverted residual bottleneck layers allow a partic- ularly memory ef\ufb01cient implementation which is very important for mobile applications. A standard ef\ufb01- conv 1x1, Relu6 os Dwise 3x3, stride=s, Relu6 (b) MobileNet[27 Pannen \u2018Channel Ste (c) ShuffleNet [20] (d) Mobilenet V2 Figure 4: Comparison of convolutional blocks for dif- ferent architectures. Shuf\ufb02eNet uses Group Convolu- tions [20] and shuf\ufb02ing, it also uses conventional resid- ual approach where inner blocks are narrower than out- put. Shuf\ufb02eNet and NasNet illustrations are from re- spective papers. cient implementation of inference that uses for instance TensorFlow[31] or Caffe [32], builds a directed acyclic compute hypergraph G, consisting of edges represent- ing the operations and nodes representing tensors of in- termediate computation. The computation is scheduled in order to minimize the total number of tensors that needs to be stored in memory. In the most general case, it searches over all plausible computation orders \u03a3(G) and picks the one that minimizes M(G)= min. max seo o |A]| + size(m;). where R(i,\u03c0,G) is the list of intermediate tensors that are connected to any of \u03c0i ...\u03c0n nodes, |A| represents the size of the tensor A and size(i) is the total amount of memory needed for internal storage during operation i. For graphs that have only trivial parallel structure (such as residual connection), there is only one non- trivial feasible computation order, and thus the total amount and a bound on the memory needed for infer- ence on compute graph G can be simpli\ufb01ed: M(G) = max max} D> [4|+ > [Bl + lop A\u20acOPiny BEOP out (2) Or to restate, the amount of memory is simply the max- imum total size of combined inputs and outputs across all operations. In what follows we show that if we treat a bottleneck residual block as a single operation (and treat inner convolution as a disposable tensor), the total amount of memory would be dominated by the size of bottleneck tensors, rather than the size of tensors that are internal to bottleneck (and much larger). Bottleneck Residual Block A bottleneck block oper- ator F(a) shown in Figure 3b can be expressed as a composition of three operators F(x) = [Ao N o B]x, where A is a linear transformation A : R\u00b0*8** R**s*\", N is a non-linear per-channel transformation: Nos RSX\" _y RSX and B is again a linear transformation to the output domain: B : Re xs'xn _, R: xs! Xk For our networks VV = ReLU6 o dwise o ReLU6, but the results apply to any per-channel transformation. Suppose the size of the input domain is |2| and the size of the output domain is |y|, then the memory required to compute F(X) can be as low as |s?k| + |s\u2019?k\u2019| + O(max(s?, s\u2019\u201d)). The algorithm is based on the fact that the inner ten- sor I can be represented as concatenation of t tensors, of size n/t each and our function can then be represented as t t F(x) = So (Aio N o B;)(z) i=1 by accumulating the sum, we only require one interme- diate block of size n/t to be kept in memory at all times. Using n = t we end up having to keep only a single channel of the intermediate representation at all times. The two constraints that enabled us to use this trick is (a) the fact that the inner transformation (which includes non-linearity and depthwise) is per-channel, and (b) the consecutive non-per-channel operators have signi\ufb01cant ratio of the input size to the output. For most of the tra- ditional neural networks, such trick would not produce a signi\ufb01cant improvement. We note that, the number of multiply-adds opera- tors needed to compute F(X) using t-way split is in- dependent of t, however in existing implementations we \ufb01nd that replacing one matrix multiplication with sev- eral smaller ones hurts runtime performance due to in- (2) ns 96x96 wats] 28x128 v2.0 +P] *O 160x160 -? el 9ars 192x192 rn & 65.0 224x224 a\" oe das NasNet : Pow 8 2a MobileNetV1 a aiid 375 ShuffleNet anne Fs.) S525] \u00a7 50.0 = . <ars + . 450 a \u00b0 425 \u00b0 400 375] 24546 is 25 304050 7510150200300 400560 600 Multiply-Adds, Millions Figure 5: Performance curve of MobileNetV2 vs MobileNetV1, Shuf\ufb02eNet, NAS. For our networks we use multipliers 0.35, 0.5, 0.75, 1.0 for all resolutions, and additional 1.4 for for 224. Best viewed in color. \u2014 Shortcut between bottlenecks \u2014 Shortcut between expansions \u2014 Wo residual \u2014 Linear batieneck \u2014 Relus in bottleneck (a) Impact of non-linearity in (b) Impact of variations in the bottleneck layer. residual blocks. Figure 6: The impact of non-linearities and various types of shortcut (residual) connections. creased cache misses. We \ufb01nd that this approach is the most helpful to be used with t being a small constant between 2 and 5. It signi\ufb01cantly reduces the memory requirement, but still allows one to utilize most of the ef- \ufb01ciencies gained by using highly optimized matrix mul- tiplication and convolution operators provided by deep learning frameworks. It remains to be seen if special framework level optimization may lead to further run- time improvements.",
    "raw_text": "5. Implementation Notes 5.1. Memory ef\ufb01cient inference The inverted residual bottleneck layers allow a partic- ularly memory ef\ufb01cient implementation which is very important for mobile applications. A standard ef\ufb01- conv 1x1, Relu6 os Dwise 3x3, stride=s, Relu6 (b) MobileNet[27 Pannen \u2018Channel Ste (c) ShuffleNet [20] (d) Mobilenet V2 Figure 4: Comparison of convolutional blocks for dif- ferent architectures. Shuf\ufb02eNet uses Group Convolu- tions [20] and shuf\ufb02ing, it also uses conventional resid- ual approach where inner blocks are narrower than out- put. Shuf\ufb02eNet and NasNet illustrations are from re- spective papers. cient implementation of inference that uses for instance TensorFlow[31] or Caffe [32], builds a directed acyclic compute hypergraph G, consisting of edges represent- ing the operations and nodes representing tensors of in- termediate computation. The computation is scheduled in order to minimize the total number of tensors that needs to be stored in memory. In the most general case, it searches over all plausible computation orders \u03a3(G) and picks the one that minimizes M(G)= min. max seo o |A]| + size(m;). where R(i,\u03c0,G) is the list of intermediate tensors that are connected to any of \u03c0i ...\u03c0n nodes, |A| represents the size of the tensor A and size(i) is the total amount of memory needed for internal storage during operation i. For graphs that have only trivial parallel structure (such as residual connection), there is only one non- trivial feasible computation order, and thus the total amount and a bound on the memory needed for infer- ence on compute graph G can be simpli\ufb01ed: M(G) = max max} D> [4|+ > [Bl + lop A\u20acOPiny BEOP out (2) Or to restate, the amount of memory is simply the max- imum total size of combined inputs and outputs across all operations. In what follows we show that if we treat a bottleneck residual block as a single operation (and treat inner convolution as a disposable tensor), the total amount of memory would be dominated by the size of bottleneck tensors, rather than the size of tensors that are internal to bottleneck (and much larger). Bottleneck Residual Block A bottleneck block oper- ator F(a) shown in Figure 3b can be expressed as a composition of three operators F(x) = [Ao N o B]x, where A is a linear transformation A : R\u00b0*8** R**s*\", N is a non-linear per-channel transformation: Nos RSX\" _y RSX and B is again a linear transformation to the output domain: B : Re xs'xn _, R: xs! Xk For our networks VV = ReLU6 o dwise o ReLU6, but the results apply to any per-channel transformation. Suppose the size of the input domain is |2| and the size of the output domain is |y|, then the memory required to compute F(X) can be as low as |s?k| + |s\u2019?k\u2019| + O(max(s?, s\u2019\u201d)). The algorithm is based on the fact that the inner ten- sor I can be represented as concatenation of t tensors, of size n/t each and our function can then be represented as t t F(x) = So (Aio N o B;)(z) i=1 by accumulating the sum, we only require one interme- diate block of size n/t to be kept in memory at all times. Using n = t we end up having to keep only a single channel of the intermediate representation at all times. The two constraints that enabled us to use this trick is (a) the fact that the inner transformation (which includes non-linearity and depthwise) is per-channel, and (b) the consecutive non-per-channel operators have signi\ufb01cant ratio of the input size to the output. For most of the tra- ditional neural networks, such trick would not produce a signi\ufb01cant improvement. We note that, the number of multiply-adds opera- tors needed to compute F(X) using t-way split is in- dependent of t, however in existing implementations we \ufb01nd that replacing one matrix multiplication with sev- eral smaller ones hurts runtime performance due to in- (2) ns 96x96 wats] 28x128 v2.0 +P] *O 160x160 -? el 9ars 192x192 rn & 65.0 224x224 a\" oe das NasNet : Pow 8 2a MobileNetV1 a aiid 375 ShuffleNet anne Fs.) S525] \u00a7 50.0 = . <ars + . 450 a \u00b0 425 \u00b0 400 375] 24546 is 25 304050 7510150200300 400560 600 Multiply-Adds, Millions Figure 5: Performance curve of MobileNetV2 vs MobileNetV1, Shuf\ufb02eNet, NAS. For our networks we use multipliers 0.35, 0.5, 0.75, 1.0 for all resolutions, and additional 1.4 for for 224. Best viewed in color. \u2014 Shortcut between bottlenecks \u2014 Shortcut between expansions \u2014 Wo residual \u2014 Linear batieneck \u2014 Relus in bottleneck (a) Impact of non-linearity in (b) Impact of variations in the bottleneck layer. residual blocks. Figure 6: The impact of non-linearities and various types of shortcut (residual) connections. creased cache misses. We \ufb01nd that this approach is the most helpful to be used with t being a small constant between 2 and 5. It signi\ufb01cantly reduces the memory requirement, but still allows one to utilize most of the ef- \ufb01ciencies gained by using highly optimized matrix mul- tiplication and convolution operators provided by deep learning frameworks. It remains to be seen if special framework level optimization may lead to further run- time improvements."
  },
  {
    "chunk_id": "9a7aebac-6bbd-45b4-a5b4-c453634aa6d0",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 6,
    "retrieval_text": "6. Experiments 6.1. ImageNet Classi\ufb01cation Training setup We train our models using TensorFlow[31]. We use the standard RMSPropOp- timizer with both decay and momentum set to 0.9. We use batch normalization after every layer, and the standard weight decay is set to 0.00004. Following MobileNetV1[27] setup we use initial learning rate of 0.045, and learning rate decay rate of 0.98 per epoch. We use 16 GPU asynchronous workers, and a batch size of 96. Results We compare our networks against MobileNetV1, Shuf\ufb02eNet and NASNet-A models. The statistics of a few selected models is shown in Table 4 with the full performance graph shown in Figure 5. 6.2. Object Detection We evaluate and compare the performance of MobileNetV2 and MobileNetV1 as feature extractors [33] for object detection with a modi\ufb01ed version of the Single Shot Detector (SSD) [34] on COCO dataset [2]. We also compare to YOLOv2 [35] and original SSD (with VGG-16 [6] as base network) as baselines. We do not compare performance with other architectures such as Faster-RCNN [36] and RFCN [37] since our focus is on mobile/real-time models. SSDLite: In this paper, we introduce a mobile friendly variant of regular SSD. We replace all the regu- lar convolutions with separable convolutions (depthwise followed by 1 \u00d7 1 projection) in SSD prediction lay- ers. This design is in line with the overall design of MobileNets and is seen to be much more computation- ally ef\ufb01cient. We call this modi\ufb01ed version SSDLite. Compared to regular SSD, SSDLite dramatically re- duces both parameter count and computational cost as shown in Table 5. For MobileNetV1, we follow the setup in [33]. For MobileNetV2, the \ufb01rst layer of SSDLite is attached to the expansion of layer 15 (with output stride of 16). The second and the rest of SSDLite layers are attached on top of the last layer (with output stride of 32). This setup is consistent with MobileNetV1 as all layers are attached to the feature map of the same output strides. Network Top 1 Params MAdds CPU MobileNetV1 70.6 4.2M 575M 113ms Shuf\ufb02eNet (1.5) 71.5 3.4M 292M - Shuf\ufb02eNet (x2) 73.7 5.4M 524M - NasNet-A 74.0 5.3M 564M 183ms MobileNetV2 72.0 3.4M 300M 75ms MobileNetV2 (1.4) 74.7 6.9M 585M 143ms Table 4: Performance on ImageNet, comparison for dif- ferent networks. As is common practice for ops, we count the total number of Multiply-Adds. In the last column we report running time in milliseconds (ms) for a single large core of the Google Pixel 1 phone (using TF-Lite). We do not report Shuf\ufb02eNet numbers as ef\ufb01- cient group convolutions and shuf\ufb02ing are not yet sup- ported. Params MAdds SSD[34] 14.8M 1.25B SSDLite 2.1M 0.35B Table 5: Comparison of the size and the computa- tional cost between SSD and SSDLite con\ufb01gured with MobileNetV2 and making predictions for 80 classes. Network mAP Params MAdd CPU SSD300[34] 23.2 36.1M 35.2B - SSD512[34] 26.8 36.1M 99.5B - YOLOv2[35] 21.6 50.7M 17.5B - MNet V1 + SSDLite 22.2 5.1M 1.3B 270ms MNet V2 + SSDLite 22.1 4.3M 0.8B 200ms Table 6: Performance comparison of MobileNetV2 + SSDLite and other realtime detectors on the COCO dataset object detection task. MobileNetV2 + SSDLite achieves competitive accuracy with signi\ufb01cantly fewer parameters and smaller computational complexity. All models are trained on trainval35k and evaluated on test-dev. SSD/YOLOv2 numbers are from [35]. The running time is reported for the large core of the Google Pixel 1 phone, using an internal version of the TF-Lite engine. Both MobileNet models are trained and evalu- ated with Open Source TensorFlow Object Detection API [38]. The input resolution of both models is 320 \u00d7 320. We benchmark and compare both mAP (COCO challenge metrics), number of parameters and number of Multiply-Adds. The results are shown in Table 6. MobileNetV2 SSDLite is not only the most ef\ufb01cient model, but also the most accurate of the three. No- tably, MobileNetV2 SSDLite is 20\u00d7 more ef\ufb01cient and 10\u00d7 smaller while still outperforms YOLOv2 on COCO dataset.",
    "raw_text": "6. Experiments 6.1. ImageNet Classi\ufb01cation Training setup We train our models using TensorFlow[31]. We use the standard RMSPropOp- timizer with both decay and momentum set to 0.9. We use batch normalization after every layer, and the standard weight decay is set to 0.00004. Following MobileNetV1[27] setup we use initial learning rate of 0.045, and learning rate decay rate of 0.98 per epoch. We use 16 GPU asynchronous workers, and a batch size of 96. Results We compare our networks against MobileNetV1, Shuf\ufb02eNet and NASNet-A models. The statistics of a few selected models is shown in Table 4 with the full performance graph shown in Figure 5. 6.2. Object Detection We evaluate and compare the performance of MobileNetV2 and MobileNetV1 as feature extractors [33] for object detection with a modi\ufb01ed version of the Single Shot Detector (SSD) [34] on COCO dataset [2]. We also compare to YOLOv2 [35] and original SSD (with VGG-16 [6] as base network) as baselines. We do not compare performance with other architectures such as Faster-RCNN [36] and RFCN [37] since our focus is on mobile/real-time models. SSDLite: In this paper, we introduce a mobile friendly variant of regular SSD. We replace all the regu- lar convolutions with separable convolutions (depthwise followed by 1 \u00d7 1 projection) in SSD prediction lay- ers. This design is in line with the overall design of MobileNets and is seen to be much more computation- ally ef\ufb01cient. We call this modi\ufb01ed version SSDLite. Compared to regular SSD, SSDLite dramatically re- duces both parameter count and computational cost as shown in Table 5. For MobileNetV1, we follow the setup in [33]. For MobileNetV2, the \ufb01rst layer of SSDLite is attached to the expansion of layer 15 (with output stride of 16). The second and the rest of SSDLite layers are attached on top of the last layer (with output stride of 32). This setup is consistent with MobileNetV1 as all layers are attached to the feature map of the same output strides. Network Top 1 Params MAdds CPU MobileNetV1 70.6 4.2M 575M 113ms Shuf\ufb02eNet (1.5) 71.5 3.4M 292M - Shuf\ufb02eNet (x2) 73.7 5.4M 524M - NasNet-A 74.0 5.3M 564M 183ms MobileNetV2 72.0 3.4M 300M 75ms MobileNetV2 (1.4) 74.7 6.9M 585M 143ms Table 4: Performance on ImageNet, comparison for dif- ferent networks. As is common practice for ops, we count the total number of Multiply-Adds. In the last column we report running time in milliseconds (ms) for a single large core of the Google Pixel 1 phone (using TF-Lite). We do not report Shuf\ufb02eNet numbers as ef\ufb01- cient group convolutions and shuf\ufb02ing are not yet sup- ported. Params MAdds SSD[34] 14.8M 1.25B SSDLite 2.1M 0.35B Table 5: Comparison of the size and the computa- tional cost between SSD and SSDLite con\ufb01gured with MobileNetV2 and making predictions for 80 classes. Network mAP Params MAdd CPU SSD300[34] 23.2 36.1M 35.2B - SSD512[34] 26.8 36.1M 99.5B - YOLOv2[35] 21.6 50.7M 17.5B - MNet V1 + SSDLite 22.2 5.1M 1.3B 270ms MNet V2 + SSDLite 22.1 4.3M 0.8B 200ms Table 6: Performance comparison of MobileNetV2 + SSDLite and other realtime detectors on the COCO dataset object detection task. MobileNetV2 + SSDLite achieves competitive accuracy with signi\ufb01cantly fewer parameters and smaller computational complexity. All models are trained on trainval35k and evaluated on test-dev. SSD/YOLOv2 numbers are from [35]. The running time is reported for the large core of the Google Pixel 1 phone, using an internal version of the TF-Lite engine. Both MobileNet models are trained and evalu- ated with Open Source TensorFlow Object Detection API [38]. The input resolution of both models is 320 \u00d7 320. We benchmark and compare both mAP (COCO challenge metrics), number of parameters and number of Multiply-Adds. The results are shown in Table 6. MobileNetV2 SSDLite is not only the most ef\ufb01cient model, but also the most accurate of the three. No- tably, MobileNetV2 SSDLite is 20\u00d7 more ef\ufb01cient and 10\u00d7 smaller while still outperforms YOLOv2 on COCO dataset."
  },
  {
    "chunk_id": "e1fc86c2-23bf-41e2-b07d-1c4e3f23c943",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 7,
    "retrieval_text": "6.3. Semantic Segmentation In this section, we compare MobileNetV1 and MobileNetV2 models used as feature extractors with DeepLabv3 [39] for the task of mobile semantic seg- mentation. DeepLabv3 adopts atrous convolution [40, 41, 42], a powerful tool to explicitly control the reso- lution of computed feature maps, and builds \ufb01ve paral- lel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) [43] containing three 3 \u00d7 3 convolu- tions with different atrous rates, (b) 1 \u00d7 1 convolution head, and (c) Image-level features [44]. We denote by output stride the ratio of input image spatial resolution to \ufb01nal output resolution, which is controlled by apply- ing the atrous convolution properly. For semantic seg- mentation, we usually employ output stride = 16 or 8 for denser feature maps. We conduct the experiments on the PASCAL VOC 2012 dataset [3], with extra anno- tated images from [45] and evaluation metric mIOU. To build a mobile model, we experimented with three design variations: (1) different feature extractors, (2) simplifying the DeepLabv3 heads for faster computa- tion, and (3) different inference strategies for boost- ing the performance. Our results are summarized in Table 7. We have observed that: (a) the inference strategies, including multi-scale inputs and adding left- right \ufb02ipped images, signi\ufb01cantly increase the MAdds and thus are not suitable for on-device applications, (b) using output stride = 16 is more ef\ufb01cient than output stride = 8, (c) MobileNetV1 is already a pow- erful feature extractor and only requires about 4.9 \u2212 5.7 times fewer MAdds than ResNet-101 [8] (e.g., mIOU: 78.56 vs 82.70, and MAdds: 941.9B vs 4870.6B), (d) it is more ef\ufb01cient to build DeepLabv3 heads on top of the second last feature map of MobileNetV2 than on the original last-layer feature map, since the second to last feature map contains 320 channels instead of 1280, and by doing so, we attain similar performance, but require about 2.5 times fewer operations than the MobileNetV1 counterparts, and (e) DeepLabv3 heads are computa- tionally expensive and removing the ASPP module sig- ni\ufb01cantly reduces the MAdds with only a slight perfor- mance degradation. In the end of the Table 7, we identify a potential candidate for on-device applications (in bold face), which attains 75.32% mIOU and only requires 2.75B MAdds.",
    "raw_text": "6.3. Semantic Segmentation In this section, we compare MobileNetV1 and MobileNetV2 models used as feature extractors with DeepLabv3 [39] for the task of mobile semantic seg- mentation. DeepLabv3 adopts atrous convolution [40, 41, 42], a powerful tool to explicitly control the reso- lution of computed feature maps, and builds \ufb01ve paral- lel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) [43] containing three 3 \u00d7 3 convolu- tions with different atrous rates, (b) 1 \u00d7 1 convolution head, and (c) Image-level features [44]. We denote by output stride the ratio of input image spatial resolution to \ufb01nal output resolution, which is controlled by apply- ing the atrous convolution properly. For semantic seg- mentation, we usually employ output stride = 16 or 8 for denser feature maps. We conduct the experiments on the PASCAL VOC 2012 dataset [3], with extra anno- tated images from [45] and evaluation metric mIOU. To build a mobile model, we experimented with three design variations: (1) different feature extractors, (2) simplifying the DeepLabv3 heads for faster computa- tion, and (3) different inference strategies for boost- ing the performance. Our results are summarized in Table 7. We have observed that: (a) the inference strategies, including multi-scale inputs and adding left- right \ufb02ipped images, signi\ufb01cantly increase the MAdds and thus are not suitable for on-device applications, (b) using output stride = 16 is more ef\ufb01cient than output stride = 8, (c) MobileNetV1 is already a pow- erful feature extractor and only requires about 4.9 \u2212 5.7 times fewer MAdds than ResNet-101 [8] (e.g., mIOU: 78.56 vs 82.70, and MAdds: 941.9B vs 4870.6B), (d) it is more ef\ufb01cient to build DeepLabv3 heads on top of the second last feature map of MobileNetV2 than on the original last-layer feature map, since the second to last feature map contains 320 channels instead of 1280, and by doing so, we attain similar performance, but require about 2.5 times fewer operations than the MobileNetV1 counterparts, and (e) DeepLabv3 heads are computa- tionally expensive and removing the ASPP module sig- ni\ufb01cantly reduces the MAdds with only a slight perfor- mance degradation. In the end of the Table 7, we identify a potential candidate for on-device applications (in bold face), which attains 75.32% mIOU and only requires 2.75B MAdds."
  },
  {
    "chunk_id": "e4f53aee-08e9-48f8-a20f-6338c2b8fd54",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 8,
    "retrieval_text": "6.4. Ablation study Inverted residual connections. The importance of residual connection has been studied extensively [8, 30, 46]. The new result reported in this paper is that the shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers (see Figure 6b for comparison). Importance of linear bottlenecks. The linear bottle- neck models are strictly less powerful than models with non-linearities, because the activations can always op- erate in linear regime with appropriate changes to bi- ases and scaling. However our experiments shown in Figure 6a indicate that linear bottlenecks improve per- formance, providing support that non-linearity destroys information in low-dimensional space. Network OS ASPP MF | mIOU_ Params MAdds MNet V1 16 v 75.29 11.15M_ 14.25B 8 vo v | 78.56 11.15M_ 941.9B MNet V2* | 16 v 75.70 452M 5.8B 8 v v | 7842 452M 387B MNet V2* | 16 75.32 2.11M 2.75B 8 v | 77.33 211M \u2014 152.6B ResNet-101 | 16 v 80.49 58.16M 81.0B 8 v v | 82.70 58.16M 4870.6B Table 7: MobileNet + DeepLabv3 inference strategy on the PASCAL VOC 2012 validation set. MNet V2*: Second last feature map is used for DeepLabv3 heads, which includes (1) Atrous Spatial Pyramid Pool- ing (ASPP) module, and (2) 1 \u00d7 1 convolution as well as image-pooling feature. OS: output stride that con- trols the output resolution of the segmentation map. MF: Multi-scale and left-right \ufb02ipped inputs during test. All of the models have been pretrained on COCO. The po- tential candidate for on-device applications is shown in bold face. PASCAL images have dimension 512 \u00d7 512 and atrous convolution allows us to control output fea- ture resolution without increasing the number of param- eters. 7. Conclusions and future work We described a very simple network architecture that allowed us to build a family of highly ef\ufb01cient mobile models. Our basic building unit, has several proper- ties that make it particularly suitable for mobile appli- cations. It allows very memory-ef\ufb01cient inference and relies utilize standard operations present in all neural frameworks. For the ImageNet dataset, our architecture improves the state of the art for wide range of performance points. For object detection task, our network outperforms state-of-art realtime detectors on COCO dataset both in terms of accuracy and model complexity. Notably, our architecture combined with the SSDLite detection mod- ule is 20\u00d7 less computation and 10\u00d7 less parameters than YOLOv2. On the theoretical side: the proposed convolutional block has a unique property that allows to separate the network expressiveness (encoded by expansion layers) from its capacity (encoded by bottleneck inputs). Ex- ploring this is an important direction for future research. Acknowledgments We would like to thank Matt Streeter and Sergey Ioffe for their helpful feedback and discussion.",
    "raw_text": "6.4. Ablation study Inverted residual connections. The importance of residual connection has been studied extensively [8, 30, 46]. The new result reported in this paper is that the shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers (see Figure 6b for comparison). Importance of linear bottlenecks. The linear bottle- neck models are strictly less powerful than models with non-linearities, because the activations can always op- erate in linear regime with appropriate changes to bi- ases and scaling. However our experiments shown in Figure 6a indicate that linear bottlenecks improve per- formance, providing support that non-linearity destroys information in low-dimensional space. Network OS ASPP MF | mIOU_ Params MAdds MNet V1 16 v 75.29 11.15M_ 14.25B 8 vo v | 78.56 11.15M_ 941.9B MNet V2* | 16 v 75.70 452M 5.8B 8 v v | 7842 452M 387B MNet V2* | 16 75.32 2.11M 2.75B 8 v | 77.33 211M \u2014 152.6B ResNet-101 | 16 v 80.49 58.16M 81.0B 8 v v | 82.70 58.16M 4870.6B Table 7: MobileNet + DeepLabv3 inference strategy on the PASCAL VOC 2012 validation set. MNet V2*: Second last feature map is used for DeepLabv3 heads, which includes (1) Atrous Spatial Pyramid Pool- ing (ASPP) module, and (2) 1 \u00d7 1 convolution as well as image-pooling feature. OS: output stride that con- trols the output resolution of the segmentation map. MF: Multi-scale and left-right \ufb02ipped inputs during test. All of the models have been pretrained on COCO. The po- tential candidate for on-device applications is shown in bold face. PASCAL images have dimension 512 \u00d7 512 and atrous convolution allows us to control output fea- ture resolution without increasing the number of param- eters. 7. Conclusions and future work We described a very simple network architecture that allowed us to build a family of highly ef\ufb01cient mobile models. Our basic building unit, has several proper- ties that make it particularly suitable for mobile appli- cations. It allows very memory-ef\ufb01cient inference and relies utilize standard operations present in all neural frameworks. For the ImageNet dataset, our architecture improves the state of the art for wide range of performance points. For object detection task, our network outperforms state-of-art realtime detectors on COCO dataset both in terms of accuracy and model complexity. Notably, our architecture combined with the SSDLite detection mod- ule is 20\u00d7 less computation and 10\u00d7 less parameters than YOLOv2. On the theoretical side: the proposed convolutional block has a unique property that allows to separate the network expressiveness (encoded by expansion layers) from its capacity (encoded by bottleneck inputs). Ex- ploring this is an important direction for future research. Acknowledgments We would like to thank Matt Streeter and Sergey Ioffe for their helpful feedback and discussion."
  },
  {
    "chunk_id": "201059e1-89b4-4c90-ac28-30a416719378",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 9,
    "retrieval_text": "References [1] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Im- agenet large scale visual recognition challenge. Int. J. Comput. Vision, 115(3):211\u2013252, December 2015. 1 [2] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 1, 7 [3] Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserma. The pascal visual object classes challenge a retrospective. IJCV, 2014. 1, 8 [4] Mobilenetv2 source code. Available from https://github.com/tensorflow/ models/tree/master/research/slim/ nets/mobilenet. 1 [5] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi\ufb01cation with deep convolu- tional neural networks. In Bartlett et al. [48], pages 1106\u20131114. 1 [6] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 1, 7 [7] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Ra- binovich. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 1\u20139. IEEE Computer Society, 2015. 1 [8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. CoRR, abs/1512.03385, 2015. 1, 3, 4, 8 [9] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281\u2013305, 2012. 1 [10] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of ma- chine learning algorithms. In Bartlett et al. [48], pages 2960\u20132968. 1 [11] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, and Ryan P. Adams. Scalable bayesian optimization using deep neu- ral networks. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd Interna- tional Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 2171\u20132180. JMLR.org, 2015. 1 [12] Babak Hassibi and David G. Stork. Second or- der derivatives for network pruning: Optimal brain surgeon. In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, Advances in Neural In- formation Processing Systems 5, [NIPS Confer- ence, Denver, Colorado, USA, November 30 - De- cember 3, 1992], pages 164\u2013171. Morgan Kauf- mann, 1992. 2 [13] Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In David S. Touretzky, editor, Advances in Neural Information Process- ing Systems 2, [NIPS Conference, Denver, Col- orado, USA, November 27-30, 1989], pages 598\u2013 605. Morgan Kaufmann, 1989. 2 [14] Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connec- tions for ef\ufb01cient neural network. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: An- nual Conference on Neural Information Process- ing Systems 2015, December 7-12, 2015, Mon- treal, Quebec, Canada, pages 1135\u20131143, 2015. 2 [15] Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Shijian Tang, Erich Elsen, Bryan Catanzaro, John Tran, and William J. Dally. DSD: regulariz- ing deep neural networks with dense-sparse-dense training \ufb02ow. CoRR, abs/1607.04381, 2016. 2 [16] Yiwen Guo, Anbang Yao, and Yurong Chen. Dy- namic network surgery for ef\ufb01cient dnns. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, ed- itors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Infor- mation Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1379\u20131387, 2016. 2 [17] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning \ufb01lters for ef\ufb01cient convnets. CoRR, abs/1608.08710, 2016. 2 [18] Karim Ahmed and Lorenzo Torresani. Connec- tivity learning in multi-branch networks. CoRR, abs/1709.09582, 2017. 2 [19] Tom Veniat and Ludovic Denoyer. Learning time- ef\ufb01cient deep architectures with budgeted super networks. CoRR, abs/1706.00046, 2017. 2 [20] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shuf\ufb02enet: An extremely ef\ufb01cient convolutional neural network for mobile devices. CoRR, abs/1707.01083, 2017. 2, 5 [21] Soravit Changpinyo, Mark Sandler, and Andrey Zhmoginov. The power of sparsity in convolu- tional neural networks. CoRR, abs/1702.06257, 2017. 2 [22] Min Wang, Baoyuan Liu, and Hassan Foroosh. Design of ef\ufb01cient convolutional layers using sin- gle intra-channel convolution, topological subdivi- sioning and spatial \u201dbottleneck\u201d structure. CoRR, abs/1608.04337, 2016. 2 [23] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable archi- tectures for scalable image recognition. CoRR, abs/1707.07012, 2017. 2, 5 [24] Lingxi Xie and Alan L. Yuille. Genetic CNN. CoRR, abs/1703.01513, 2017. 2 [25] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V. Le, and Alexey Kurakin. Large-scale evolution of image classi\ufb01ers. In Doina Pre- cup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 2902\u20132911. PMLR, 2017. 2 [26] Barret Zoph and Quoc V. Le. Neural architec- ture search with reinforcement learning. CoRR, abs/1611.01578, 2016. 2 [27] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef\ufb01cient convolutional neural net- works for mobile vision applications. CoRR, abs/1704.04861, 2017. 2, 4, 5, 6",
    "raw_text": "References [1] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Im- agenet large scale visual recognition challenge. Int. J. Comput. Vision, 115(3):211\u2013252, December 2015. 1 [2] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 1, 7 [3] Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserma. The pascal visual object classes challenge a retrospective. IJCV, 2014. 1, 8 [4] Mobilenetv2 source code. Available from https://github.com/tensorflow/ models/tree/master/research/slim/ nets/mobilenet. 1 [5] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi\ufb01cation with deep convolu- tional neural networks. In Bartlett et al. [48], pages 1106\u20131114. 1 [6] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 1, 7 [7] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Ra- binovich. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 1\u20139. IEEE Computer Society, 2015. 1 [8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. CoRR, abs/1512.03385, 2015. 1, 3, 4, 8 [9] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281\u2013305, 2012. 1 [10] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of ma- chine learning algorithms. In Bartlett et al. [48], pages 2960\u20132968. 1 [11] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, and Ryan P. Adams. Scalable bayesian optimization using deep neu- ral networks. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd Interna- tional Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 2171\u20132180. JMLR.org, 2015. 1 [12] Babak Hassibi and David G. Stork. Second or- der derivatives for network pruning: Optimal brain surgeon. In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, Advances in Neural In- formation Processing Systems 5, [NIPS Confer- ence, Denver, Colorado, USA, November 30 - De- cember 3, 1992], pages 164\u2013171. Morgan Kauf- mann, 1992. 2 [13] Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In David S. Touretzky, editor, Advances in Neural Information Process- ing Systems 2, [NIPS Conference, Denver, Col- orado, USA, November 27-30, 1989], pages 598\u2013 605. Morgan Kaufmann, 1989. 2 [14] Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connec- tions for ef\ufb01cient neural network. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: An- nual Conference on Neural Information Process- ing Systems 2015, December 7-12, 2015, Mon- treal, Quebec, Canada, pages 1135\u20131143, 2015. 2 [15] Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Shijian Tang, Erich Elsen, Bryan Catanzaro, John Tran, and William J. Dally. DSD: regulariz- ing deep neural networks with dense-sparse-dense training \ufb02ow. CoRR, abs/1607.04381, 2016. 2 [16] Yiwen Guo, Anbang Yao, and Yurong Chen. Dy- namic network surgery for ef\ufb01cient dnns. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, ed- itors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Infor- mation Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1379\u20131387, 2016. 2 [17] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning \ufb01lters for ef\ufb01cient convnets. CoRR, abs/1608.08710, 2016. 2 [18] Karim Ahmed and Lorenzo Torresani. Connec- tivity learning in multi-branch networks. CoRR, abs/1709.09582, 2017. 2 [19] Tom Veniat and Ludovic Denoyer. Learning time- ef\ufb01cient deep architectures with budgeted super networks. CoRR, abs/1706.00046, 2017. 2 [20] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shuf\ufb02enet: An extremely ef\ufb01cient convolutional neural network for mobile devices. CoRR, abs/1707.01083, 2017. 2, 5 [21] Soravit Changpinyo, Mark Sandler, and Andrey Zhmoginov. The power of sparsity in convolu- tional neural networks. CoRR, abs/1702.06257, 2017. 2 [22] Min Wang, Baoyuan Liu, and Hassan Foroosh. Design of ef\ufb01cient convolutional layers using sin- gle intra-channel convolution, topological subdivi- sioning and spatial \u201dbottleneck\u201d structure. CoRR, abs/1608.04337, 2016. 2 [23] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable archi- tectures for scalable image recognition. CoRR, abs/1707.07012, 2017. 2, 5 [24] Lingxi Xie and Alan L. Yuille. Genetic CNN. CoRR, abs/1703.01513, 2017. 2 [25] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V. Le, and Alexey Kurakin. Large-scale evolution of image classi\ufb01ers. In Doina Pre- cup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 2902\u20132911. PMLR, 2017. 2 [26] Barret Zoph and Quoc V. Le. Neural architec- ture search with reinforcement learning. CoRR, abs/1611.01578, 2016. 2 [27] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ef\ufb01cient convolutional neural net- works for mobile vision applications. CoRR, abs/1704.04861, 2017. 2, 4, 5, 6"
  },
  {
    "chunk_id": "9100ee6f-51e0-4da0-8c1b-92e3107a166d",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 10,
    "retrieval_text": "[28] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 2 [29] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. CoRR, abs/1610.02915, 2016. 3 [30] Saining Xie, Ross B. Girshick, Piotr Doll\u00b4ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. CoRR, abs/1611.05431, 2016. 3, 4, 8 [31] Mart\u00b4\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfel- low, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00b4e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00b4egas, Oriol Vinyals, Pete Warden, Martin Wat- tenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software avail- able from tensor\ufb02ow.org. 5, 6 [32] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed- ding. arXiv preprint arXiv:1408.5093, 2014. 5 [33] Jonathan Huang, Vivek Rathod, Chen Sun, Men- glong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. In CVPR, 2017. 7 [34] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 7 [35] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. arXiv preprint arXiv:1612.08242, 2016. 7 [36] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Ad- vances in neural information processing systems, pages 91\u201399, 2015. 7 [37] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R- fcn: Object detection via region-based fully con- volutional networks. In Advances in neural infor- mation processing systems, pages 379\u2013387, 2016. 7 [38] Jonathan Huang, Vivek Rathod, Derek Chow, Chen Sun, and Menglong Zhu. Tensor\ufb02ow object detection api, 2017. 7 [39] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017. 7 [40] Matthias Holschneider, Richard Kronland- Martinet, Jean Morlet, and Ph Tchamitchian. A real-time algorithm for signal analysis with the help of the wavelet transform. In Wavelets: Time-Frequency Methods and Phase Space, pages 289\u2013297. 1989. 7 [41] Pierre Sermanet, David Eigen, Xiang Zhang, Micha\u00a8el Mathieu, Rob Fergus, and Yann Le- Cun. Overfeat: Integrated recognition, localiza- tion and detection using convolutional networks. arXiv:1312.6229, 2013. 7 [42] George Papandreou, Iasonas Kokkinos, and Pierre- Andre Savalle. Modeling local and global defor- mations in deep learning: Epitomic convolution, multiple instance learning, and sliding window de- tection. In CVPR, 2015. 7 [43] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 2017. 7 [44] Wei Liu, Andrew Rabinovich, and Alexander C. Berg. Parsenet: Looking wider to see better. CoRR, abs/1506.04579, 2015. 7 [45] Bharath Hariharan, Pablo Arbel\u00b4aez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Se- mantic contours from inverse detectors. In ICCV, 2011. 8 [46] Christian Szegedy, Sergey Ioffe, and Vincent Van- houcke. Inception-v4, inception-resnet and the im- pact of residual connections on learning. CoRR, abs/1602.07261, 2016. 8 [47] Guido Mont\u00b4ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Proceedings of the 27th International Conference on Neural Infor- mation Processing Systems, NIPS\u201914, pages 2924\u2013 2932, Cambridge, MA, USA, 2014. MIT Press. 13 [48] Peter L. Bartlett, Fernando C. N. Pereira, Christo- pher J. C. Burges, L\u00b4eon Bottou, and Kilian Q. Weinberger, editors. Advances in Neural Infor- mation Processing Systems 25: 26th Annual Con- ference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3- 6, 2012, Lake Tahoe, Nevada, United States, 2012. 9",
    "raw_text": "[28] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 2 [29] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. CoRR, abs/1610.02915, 2016. 3 [30] Saining Xie, Ross B. Girshick, Piotr Doll\u00b4ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. CoRR, abs/1611.05431, 2016. 3, 4, 8 [31] Mart\u00b4\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfel- low, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00b4e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00b4egas, Oriol Vinyals, Pete Warden, Martin Wat- tenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software avail- able from tensor\ufb02ow.org. 5, 6 [32] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed- ding. arXiv preprint arXiv:1408.5093, 2014. 5 [33] Jonathan Huang, Vivek Rathod, Chen Sun, Men- glong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. In CVPR, 2017. 7 [34] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 7 [35] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. arXiv preprint arXiv:1612.08242, 2016. 7 [36] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Ad- vances in neural information processing systems, pages 91\u201399, 2015. 7 [37] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R- fcn: Object detection via region-based fully con- volutional networks. In Advances in neural infor- mation processing systems, pages 379\u2013387, 2016. 7 [38] Jonathan Huang, Vivek Rathod, Derek Chow, Chen Sun, and Menglong Zhu. Tensor\ufb02ow object detection api, 2017. 7 [39] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017. 7 [40] Matthias Holschneider, Richard Kronland- Martinet, Jean Morlet, and Ph Tchamitchian. A real-time algorithm for signal analysis with the help of the wavelet transform. In Wavelets: Time-Frequency Methods and Phase Space, pages 289\u2013297. 1989. 7 [41] Pierre Sermanet, David Eigen, Xiang Zhang, Micha\u00a8el Mathieu, Rob Fergus, and Yann Le- Cun. Overfeat: Integrated recognition, localiza- tion and detection using convolutional networks. arXiv:1312.6229, 2013. 7 [42] George Papandreou, Iasonas Kokkinos, and Pierre- Andre Savalle. Modeling local and global defor- mations in deep learning: Epitomic convolution, multiple instance learning, and sliding window de- tection. In CVPR, 2015. 7 [43] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 2017. 7 [44] Wei Liu, Andrew Rabinovich, and Alexander C. Berg. Parsenet: Looking wider to see better. CoRR, abs/1506.04579, 2015. 7 [45] Bharath Hariharan, Pablo Arbel\u00b4aez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Se- mantic contours from inverse detectors. In ICCV, 2011. 8 [46] Christian Szegedy, Sergey Ioffe, and Vincent Van- houcke. Inception-v4, inception-resnet and the im- pact of residual connections on learning. CoRR, abs/1602.07261, 2016. 8 [47] Guido Mont\u00b4ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Proceedings of the 27th International Conference on Neural Infor- mation Processing Systems, NIPS\u201914, pages 2924\u2013 2932, Cambridge, MA, USA, 2014. MIT Press. 13 [48] Peter L. Bartlett, Fernando C. N. Pereira, Christo- pher J. C. Burges, L\u00b4eon Bottou, and Kilian Q. Weinberger, editors. Advances in Neural Infor- mation Processing Systems 25: 26th Annual Con- ference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3- 6, 2012, Lake Tahoe, Nevada, United States, 2012. 9"
  },
  {
    "chunk_id": "f59144eb-8533-4df9-a1b5-25f527a21e55",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 11,
    "retrieval_text": "A. Bottleneck transformation In this section we study the properties of an operator AReLU(Bz), where \u00ab \u20ac R\u201d represents an n-channel pixel, B is an m Xx n matrix and A is an n x m matrix. We argue that ifm < n, transformations of this form can only exploit non-linearity at the cost of losing infor- mation. In contrast, ifn < m, such transforms can be highly non-linear but still invertible with high probabil- ity (for the initial random weights). First we show that ReLU is an identity transforma- tion for any point that lies in the interior of its image. Lemma 1 Let S(X) = {ReLU(x)|x \u2208 X}. If a vol- ume of S(X) is non-zero, then interiorS(X) \u2286 X. Proof: Let S\u2019 = interior ReLU(S). First we note that if x \u20ac S\u2019, then x; > 0 for all i. Indeed, image of ReLU does not contain points with negative coordinates, and points with zero-valued coordinates can not be interior points. Therefore for each x \u20ac S\u2019, 2 = ReLU(z) as desired. Mi It follows that for an arbitrary composition of inter- leaved linear transformation and ReLU operators, if it preserves non-zero volume, that part of the input space X that is preserved over such a composition is a lin- ear transformation, and thus is likely to be a minor con- tributor to the power of deep networks. However, this is a fairly weak statement. Indeed, if the input mani- fold can be embedded into (n \u2212 1)-dimensional mani- fold (out of n dimensions total), the lemma is trivially true, since the starting volume is 0. In what follows we show that when the dimensionality of input manifold is signi\ufb01cantly lower we can ensure that there will be no information loss. Since the ReLU(x) nonlinearity is a surjective func- tion mapping the entire ray x \u2264 0 to 0, using this nonlin- earity in a neural network can result in information loss. Once ReLU collapses a subset of the input manifold to a smaller-dimensional output, the following network lay- ers can no longer distinguish between collapsed input samples. In the following, we show that bottlenecks with suf\ufb01ciently large expansion layers are resistant to infor- mation loss caused by the presence of ReLU activation functions. Lemma 2 (Invertibility of ReLU) Consider an opera- tor ReLU(Bx), where B is an m \u00d7 n matrix and x \u2208 Rn. Let y0 = ReLU(Bx0) for some x0 \u2208 Rn, then equation y0 = ReLU(Bx) has a unique solution with respect to x if and only if y0 has at least n non-zero val- ues and there are n linearly independent rows of B that correspond to non-zero coordinates of y0. Proof: Denote the set of non-zero coordinates of yo as T and let yr and Br be restrictions of y and B to the subspace defined by T. If |T| < n, we have yr = Brxo where Br is under-determined with at least one solution xo, thus there are infinitely many so- lutions. Now consider the case of |T'| > n and let the rank of Br be n. Suppose there is an additional solu- tion z; # ao such that yy = ReLU(B2), then we have yr = Brxo = Bray, which cannot be satisfied unless t= 27,.8 One of the corollaries of this lemma says that if m >> n, we only need a small fraction of values of Bx to be positive for ReLU(Bz) to be invertible. The constraints of the lemma 2 can be empirically validated for real networks and real inputs and hence we can be assured that information is indeed preserved. We further show that with respect to initialization, we can be sure that these constraints are satis\ufb01ed with high proba- bility. Note that for random initialization the conditions of lemma 2 are satis\ufb01ed due to initialization symmetries. However even for trained graphs these constraints can be empirically validated by running the network over valid inputs and verifying that all or most inputs are above the threshold. On Figure 7 we show how this distribu- tion looks for different MobileNetV2 layers. At step 0 the activation patterns concentrate around having half of the positive channel (as predicted by initialization sym- metries). For fully trained network, while the standard deviation grew signi\ufb01cantly, all but the two layers are still above the invertibility thresholds. We believe fur- ther study of this is warranted and might lead to helpful insights on network design. Theorem 1 Let S be a compact n-dimensional subman- ifold of Rn. Consider a family of functions fB(x) = ReLU(Bx) from Rn to Rm parameterized by m \u00d7 n matrices B \u2208 B. Let p(B) be a probability density on the space of all matrices B that satis\ufb01es: \u2022 P(Z) = 0 for any measure-zero subset Z \u2282 B; \u2022 (a symmetry condition) p(DB) = p(B) for any B \u2208 B and any m \u00d7 m diagonal matrix D with all diagonal elements being either +1 or \u22121. Then, the average n-volume of the subset of S that is collapsed by fB to a lower-dimensional manifold is V \u2212 Nm,nV 2m , where V = volS and mon 7 k=0 Proof: For any \u03c3 = (s1,...,sm) with sk \u2208 {\u22121,+1}, let Q\u03c3 = {x \u2208 Rm|xisi > 0} be a corre- sponding quadrant in Rm. For any n-dimensional sub- manifold \u0393 \u2282 Rm, ReLU acts as a bijection on \u0393 \u2229 Q\u03c3 if \u03c3 has at least n positive values4 and contracts \u0393 \u2229 Q\u03c3 otherwise. Also notice that the intersection of BS with Rm\\(\u222a\u03c3Q\u03c3) is almost surely (n\u22121)-dimensional. The average n-volume of S that is not collapsed by applying ReLU to BS is therefore given by: EB[V\u03c3(B)], (3) \u03c3\u2208\u03a3n where U,, = {(81,-.-,5m)| 0, (sx) = n}, @ is a step function and V,(B) is a volume of the largest subset of S that is mapped by B to Q,. Now let us calcu- late Ep[V,(B)]. Recalling that p(/DB) = p(B) for any D = diag(s1,...,: Sm) with s, \u20ac {\u20141,+1}, this average can be rewritten as Ez;Ep[V,(DB)]. Notic- ing that the subset of S mapped by DB to Q, is also mapped by B to D~!Q,, we immediately obtain 4unless at least one of the positive coordinates for all x \u2208 \u0393 \u2229 Q\u03c3 is \ufb01xed, which would not be the case for almost all B and \u0393 = BS (a) At step 0 (b) Fully trained Figure 7: Distribution of activation patterns. The x-axis is the layer index, and we show minimum/maximum/average number of positive channels after each convolution with ReLU. y-axis is either absolute or relative number of chan- nels. The \u201cthreshold\u201d line indicates the ReLU invertibility threshold - that is the number of positive dimensions is higher than the input space. In our case this is 1/6 fraction of the channels. Note how at the beginning of the train- ing on Figure 7a the distribution is much more tightly concentrated around the mean. After the training has \ufb01nished (Figure 7b), the average hasn\u2019t changed but the standard deviation grew dramatically. Best viewed in color.",
    "raw_text": "A. Bottleneck transformation In this section we study the properties of an operator AReLU(Bz), where \u00ab \u20ac R\u201d represents an n-channel pixel, B is an m Xx n matrix and A is an n x m matrix. We argue that ifm < n, transformations of this form can only exploit non-linearity at the cost of losing infor- mation. In contrast, ifn < m, such transforms can be highly non-linear but still invertible with high probabil- ity (for the initial random weights). First we show that ReLU is an identity transforma- tion for any point that lies in the interior of its image. Lemma 1 Let S(X) = {ReLU(x)|x \u2208 X}. If a vol- ume of S(X) is non-zero, then interiorS(X) \u2286 X. Proof: Let S\u2019 = interior ReLU(S). First we note that if x \u20ac S\u2019, then x; > 0 for all i. Indeed, image of ReLU does not contain points with negative coordinates, and points with zero-valued coordinates can not be interior points. Therefore for each x \u20ac S\u2019, 2 = ReLU(z) as desired. Mi It follows that for an arbitrary composition of inter- leaved linear transformation and ReLU operators, if it preserves non-zero volume, that part of the input space X that is preserved over such a composition is a lin- ear transformation, and thus is likely to be a minor con- tributor to the power of deep networks. However, this is a fairly weak statement. Indeed, if the input mani- fold can be embedded into (n \u2212 1)-dimensional mani- fold (out of n dimensions total), the lemma is trivially true, since the starting volume is 0. In what follows we show that when the dimensionality of input manifold is signi\ufb01cantly lower we can ensure that there will be no information loss. Since the ReLU(x) nonlinearity is a surjective func- tion mapping the entire ray x \u2264 0 to 0, using this nonlin- earity in a neural network can result in information loss. Once ReLU collapses a subset of the input manifold to a smaller-dimensional output, the following network lay- ers can no longer distinguish between collapsed input samples. In the following, we show that bottlenecks with suf\ufb01ciently large expansion layers are resistant to infor- mation loss caused by the presence of ReLU activation functions. Lemma 2 (Invertibility of ReLU) Consider an opera- tor ReLU(Bx), where B is an m \u00d7 n matrix and x \u2208 Rn. Let y0 = ReLU(Bx0) for some x0 \u2208 Rn, then equation y0 = ReLU(Bx) has a unique solution with respect to x if and only if y0 has at least n non-zero val- ues and there are n linearly independent rows of B that correspond to non-zero coordinates of y0. Proof: Denote the set of non-zero coordinates of yo as T and let yr and Br be restrictions of y and B to the subspace defined by T. If |T| < n, we have yr = Brxo where Br is under-determined with at least one solution xo, thus there are infinitely many so- lutions. Now consider the case of |T'| > n and let the rank of Br be n. Suppose there is an additional solu- tion z; # ao such that yy = ReLU(B2), then we have yr = Brxo = Bray, which cannot be satisfied unless t= 27,.8 One of the corollaries of this lemma says that if m >> n, we only need a small fraction of values of Bx to be positive for ReLU(Bz) to be invertible. The constraints of the lemma 2 can be empirically validated for real networks and real inputs and hence we can be assured that information is indeed preserved. We further show that with respect to initialization, we can be sure that these constraints are satis\ufb01ed with high proba- bility. Note that for random initialization the conditions of lemma 2 are satis\ufb01ed due to initialization symmetries. However even for trained graphs these constraints can be empirically validated by running the network over valid inputs and verifying that all or most inputs are above the threshold. On Figure 7 we show how this distribu- tion looks for different MobileNetV2 layers. At step 0 the activation patterns concentrate around having half of the positive channel (as predicted by initialization sym- metries). For fully trained network, while the standard deviation grew signi\ufb01cantly, all but the two layers are still above the invertibility thresholds. We believe fur- ther study of this is warranted and might lead to helpful insights on network design. Theorem 1 Let S be a compact n-dimensional subman- ifold of Rn. Consider a family of functions fB(x) = ReLU(Bx) from Rn to Rm parameterized by m \u00d7 n matrices B \u2208 B. Let p(B) be a probability density on the space of all matrices B that satis\ufb01es: \u2022 P(Z) = 0 for any measure-zero subset Z \u2282 B; \u2022 (a symmetry condition) p(DB) = p(B) for any B \u2208 B and any m \u00d7 m diagonal matrix D with all diagonal elements being either +1 or \u22121. Then, the average n-volume of the subset of S that is collapsed by fB to a lower-dimensional manifold is V \u2212 Nm,nV 2m , where V = volS and mon 7 k=0 Proof: For any \u03c3 = (s1,...,sm) with sk \u2208 {\u22121,+1}, let Q\u03c3 = {x \u2208 Rm|xisi > 0} be a corre- sponding quadrant in Rm. For any n-dimensional sub- manifold \u0393 \u2282 Rm, ReLU acts as a bijection on \u0393 \u2229 Q\u03c3 if \u03c3 has at least n positive values4 and contracts \u0393 \u2229 Q\u03c3 otherwise. Also notice that the intersection of BS with Rm\\(\u222a\u03c3Q\u03c3) is almost surely (n\u22121)-dimensional. The average n-volume of S that is not collapsed by applying ReLU to BS is therefore given by: EB[V\u03c3(B)], (3) \u03c3\u2208\u03a3n where U,, = {(81,-.-,5m)| 0, (sx) = n}, @ is a step function and V,(B) is a volume of the largest subset of S that is mapped by B to Q,. Now let us calcu- late Ep[V,(B)]. Recalling that p(/DB) = p(B) for any D = diag(s1,...,: Sm) with s, \u20ac {\u20141,+1}, this average can be rewritten as Ez;Ep[V,(DB)]. Notic- ing that the subset of S mapped by DB to Q, is also mapped by B to D~!Q,, we immediately obtain 4unless at least one of the positive coordinates for all x \u2208 \u0393 \u2229 Q\u03c3 is \ufb01xed, which would not be the case for almost all B and \u0393 = BS (a) At step 0 (b) Fully trained Figure 7: Distribution of activation patterns. The x-axis is the layer index, and we show minimum/maximum/average number of positive channels after each convolution with ReLU. y-axis is either absolute or relative number of chan- nels. The \u201cthreshold\u201d line indicates the ReLU invertibility threshold - that is the number of positive dimensions is higher than the input space. In our case this is 1/6 fraction of the channels. Note how at the beginning of the train- ing on Figure 7a the distribution is much more tightly concentrated around the mean. After the training has \ufb01nished (Figure 7b), the average hasn\u2019t changed but the standard deviation grew dramatically. Best viewed in color."
  },
  {
    "chunk_id": "e97235aa-3143-4e00-9970-264d4192fb51",
    "modality": "text",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": 13,
    "retrieval_text": ">, Voldiag(o\u2019)B] = O,, Vo/[B] = vol S and there- fore Ep[V,(B)| = 2~\u2122 vol S. Substituting this and [En| = peo\u201d (7) into Eq. 3 concludes the proof. i k=0 k Notice that for sufficiently large expansion layers with m > n, the fraction of collapsed space Nin jn /2\u201d can be bounded by: Nm,n 2m \u2265 1\u2212 mn+1 2mn! \u2265 1\u22122(n+1) log m\u2212m \u2265 1\u22122\u2212m/2 and therefore ReLU(Bx) performs a nonlinear transfor- mation while preserving information with high proba- bility. We discussed how bottlenecks can prevent manifold collapse, but increasing the size of the bottleneck expan- sion may also make it possible for the network to repre- sent more complex functions. Following the main re- sults of [47], one can show, for example, that for any in- teger L \u2265 1 and p > 1 there exist a network of L ReLU layers, each containing n neurons and a bottleneck ex- pansion of size pn such that it maps pnL input volumes (linearly isomorphic to [0,1]n) to the same output re- gion [0,1]n. Any complex possibly nonlinear function attached to the network output would thus effectively compute function values for pnL input linear regions. B. Semantic segmentation visualization re- sults Ee: a3 Oy 2 Oo Oo mt ret ooo baa S| Fl | Bl LD r poe > > = a> > -~ > = fd Be Bea at, ost EY 5 yan ro ES Be ase Best Image OS=32, S=1 OS=16, S=0.5 OS=16, $=0.75 OS=16, S=1 OS=8, MS+F Image OS=32, S=1 OS=16, S=0.5 OS=16, $=0.75 OS=16, S=1 OS=8, MS+F Figure 8: MobileNetv2 semantic segmentation visualization results on PASCAL VOC 2012 val set. OS: output stride. S: single scale input. MS+F: Multi-scale inputs with scales = {0.5,0.75,1,1.25,1.5,1.75} and left-right \ufb02ipped inputs. Employing output stride = 16 and single input scale = 1 attains a good trade-off between FLOPS and accuracy.",
    "raw_text": ">, Voldiag(o\u2019)B] = O,, Vo/[B] = vol S and there- fore Ep[V,(B)| = 2~\u2122 vol S. Substituting this and [En| = peo\u201d (7) into Eq. 3 concludes the proof. i k=0 k Notice that for sufficiently large expansion layers with m > n, the fraction of collapsed space Nin jn /2\u201d can be bounded by: Nm,n 2m \u2265 1\u2212 mn+1 2mn! \u2265 1\u22122(n+1) log m\u2212m \u2265 1\u22122\u2212m/2 and therefore ReLU(Bx) performs a nonlinear transfor- mation while preserving information with high proba- bility. We discussed how bottlenecks can prevent manifold collapse, but increasing the size of the bottleneck expan- sion may also make it possible for the network to repre- sent more complex functions. Following the main re- sults of [47], one can show, for example, that for any in- teger L \u2265 1 and p > 1 there exist a network of L ReLU layers, each containing n neurons and a bottleneck ex- pansion of size pn such that it maps pnL input volumes (linearly isomorphic to [0,1]n) to the same output re- gion [0,1]n. Any complex possibly nonlinear function attached to the network output would thus effectively compute function values for pnL input linear regions. B. Semantic segmentation visualization re- sults Ee: a3 Oy 2 Oo Oo mt ret ooo baa S| Fl | Bl LD r poe > > = a> > -~ > = fd Be Bea at, ost EY 5 yan ro ES Be ase Best Image OS=32, S=1 OS=16, S=0.5 OS=16, $=0.75 OS=16, S=1 OS=8, MS+F Image OS=32, S=1 OS=16, S=0.5 OS=16, $=0.75 OS=16, S=1 OS=8, MS+F Figure 8: MobileNetv2 semantic segmentation visualization results on PASCAL VOC 2012 val set. OS: output stride. S: single scale input. MS+F: Multi-scale inputs with scales = {0.5,0.75,1,1.25,1.5,1.75} and left-right \ufb02ipped inputs. Employing output stride = 16 and single input scale = 1 attains a good trade-off between FLOPS and accuracy."
  },
  {
    "chunk_id": "5fcaf8c8-a541-4ea4-8c5e-0e5cd7d78e03",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital composite that illustrates the process of regular convolution with an example of a 2D image and its corresponding convolution result. On the left side of the image, there's a visual representation of a 2D image with a red rectangle highlighting a specific area within it. This image appears to be a grayscale or low-contrast photograph of a scene that includes what looks like a blue box and some indistinct shapes.\n\nIn the center of the image, there's a text label \"regular convolution\" with an arrow pointing from the original image to the resultant image on the right. The right side shows the same 2D image but with a different visual effect applied. This effect is characterized by a red rectangle that has been convolved with another shape, which seems to be a Gaussian distribution or a similar kernel, resulting in a smoother and more uniform appearance of the highlighted area within the original image.\n\nThe bottom part of the image contains two smaller images. The top one shows a 2D array with a red rectangle highlighting a specific area, indicating that this is the region being convolved. The bottom image displays the same 2D array but with a different visual effect applied, which appears to be the result of the convolution process.\n\nThe style of the image suggests it's an educational or instructional graphic, possibly used in a technical or academic context to explain the concept of regular convolution in image processing or computer vision. ",
    "raw_text": null
  },
  {
    "chunk_id": "913e6f43-add9-4d45-b634-fa712e82b50d",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital composite that appears to be a step-by-step demonstration of a process related to computer vision or machine learning, specifically focusing on object detection and tracking. It shows a sequence of four images with accompanying text labels, each illustrating a different stage of the process.\n\n1. The first image at the top left is labeled \"Original Image\" and displays a photograph of a scene with multiple objects, including what appears to be a person and some boxes.\n\n2. The second image, labeled \"Separable Convolution Block,\" shows a convolutional neural network (CNN) architecture diagram with layers such as \"Conv1,\" \"Conv2,\" and \"Conv3.\" This suggests that the image is demonstrating how a CNN processes an input image to identify objects within it.\n\n3. The third image, labeled \"Feature Maps,\" shows a series of feature maps generated by the CNN at different stages of processing. These maps highlight areas of interest in the original image, which are likely regions where objects have been detected.\n\n4. The fourth image, labeled \"Final Output,\" displays the final output of the object detection process. It shows a set of bounding boxes around the objects in the original image, indicating where each object has been identified and tracked within the system.\n\nThe style of the image is informational and technical, with a focus on visualizing the computational steps involved in an automated object detection and tracking system. The text labels are clear and provide context for each stage of the process. ",
    "raw_text": null
  },
  {
    "chunk_id": "6f61ec94-b8c4-4ba3-b63a-1af0038e3790",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of several photographs arranged vertically, each showing different views of a three-dimensional object with a blue surface. At the top, there's a photograph labeled \"bottleneck convolution,\" which appears to be a close-up view of the object with a focus on its texture and structure. Below this, there are two photographs side by side: one is labeled \"convolution\" and shows a more detailed view of the same object, while the other is labeled \"deconvolution\" and displays a blurred version of the object, suggesting a process of image degradation or restoration.\n\nThe bottom part of the image features three photographs in sequence, each showing a different stage of an algorithmic process. The first photograph is labeled \"input,\" depicting the original blue surface with some indistinct shapes and textures. The second photograph is labeled \"convolution,\" which shows the same object as the top photographs but with a more pronounced texture and structure. The third photograph is labeled \"output,\" indicating the result of the convolution process, which appears to be a sharper and more defined version of the original input image.\n\nThe style of the image suggests it's an educational or technical illustration, possibly used in a presentation or article related to computer vision or image processing techniques. The text \"bottleneck convolution\" implies that this is a specific type of convolutional neural network (CNN) operation, which is commonly used in deep learning for feature extraction and classification tasks. ",
    "raw_text": null
  },
  {
    "chunk_id": "921b427a-fc6a-46a8-8649-fadeae64cc47",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital graphic that appears to be a screenshot of a computer program interface displaying a three-dimensional model with an overlay of a two-dimensional image. The 3D model consists of several cubes connected in a linear fashion, forming a chain or sequence. Each cube has a unique color and size, suggesting different types or properties.\n\nThe 2D image on the right side of the graphic shows a top-down view of the same structure as the 3D model, with each cube represented by a square block. The blocks are arranged in a horizontal line, and there is a red arrow pointing from one block to another, indicating a connection or relationship between them.\n\nAt the top of the image, there is text that reads \"expansion convolution block,\" which suggests that this graphic may be related to a field such as computer vision or machine learning, where \"convolution\" is a common term for a mathematical operation used in image processing and neural networks. The text implies that the image represents a conceptual model of a process involving expansion and convolution operations.\n\nThe overall style of the image is technical and schematic, with a focus on visualizing a computational or algorithmic process. The colors are simple and do not convey any specific information about the content of the cubes or the nature of the connections between them. ",
    "raw_text": null
  },
  {
    "chunk_id": "dff35e29-6023-4e01-bbac-9c9de2836e77",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image appears to be a digital graphic with an overlaid photograph of a physical object. The object is a three-dimensional model that resembles a building or architectural structure, composed of multiple blocks in shades of blue and white. These blocks are arranged in a way that suggests a layered construction, possibly representing different levels or sections of the structure.\n\nOverlaying this image is a digital interface with various graphical elements. There are two horizontal bars at the top of the image, one red and one green, which seem to be part of a user interface, perhaps indicating status or progress. Below these bars, there's a series of vertical lines that resemble a bar chart, with each line representing a different level or section of the model. The lines are color-coded in blue and white, matching the colors of the blocks in the model below.\n\nOn the right side of the image, there is a red arrow pointing downwards from the top to the bottom of the model, suggesting a path or sequence that might be related to the structure's design or construction process. The arrow seems to originate from the top-right corner and ends at the bottom-left corner of the model.\n\nThe image has a pixelated quality, especially noticeable in the areas where the digital interface overlays the photograph. This gives the impression that the image is a screenshot from a computer program or software application used for architectural design or modeling. The overall style of the image suggests it could be related to architecture, engineering, or construction (AEC) fields, possibly representing a 3D model of a building in a digital environment. ",
    "raw_text": null
  },
  {
    "chunk_id": "d6c95d38-a4ed-44ad-948f-eee26c3550c0",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image appears to be a screenshot of a computer-generated model or simulation, possibly from a software application used for architectural visualization or engineering design. It shows a 3D model with various elements that are typically associated with architectural design, such as walls, floors, and windows.\n\nThe model is composed of several blocks or sections, each representing different components of the structure. The colors in the image suggest different materials or textures, which could be used to represent different surfaces or finishes within the building.\n\nThere are annotations on the model that seem to indicate specific features or elements. For example, there is a label \"Nakli\" with a red arrow pointing at a particular section of the model, and another label \"Makli\" with a blue arrow pointing in a different direction. These labels might refer to specific parts of the design or could be placeholders for future modifications or annotations by the user.\n\nThe image also contains a grid overlay that seems to represent a 2D plane, possibly for measuring distances or aligning elements within the model. The grid lines are thin and evenly spaced, providing a reference for the model's dimensions.\n\nIn the bottom right corner of the image, there is a red arrow pointing towards a specific section of the model with the label \"Makli,\" which could indicate that this part of the design is being highlighted or selected for further editing or analysis.\n\nThe overall style of the image suggests it's a digital rendering or visualization tool used in architectural design, engineering, or construction planning. The image does not contain any text outside of the annotations on the model itself. ",
    "raw_text": null
  },
  {
    "chunk_id": "88de4c58-0e6b-4bac-b4e7-b819ee84f8c5",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of two separate images, each depicting a different type of network diagram. On the left side of the image, there is a flowchart with a series of nodes connected by directed edges, indicating a flow or process. Each node has a label that appears to be a combination of letters and numbers, such as \"A1,\" \"B2,\" \"C3,\" etc., suggesting a sequence or hierarchy within the system. The edges are labeled with numerical values ranging from 0 to 4, which could represent weights or distances between nodes in the network.\n\nOn the right side of the image, there is a more complex diagram that seems to be a neural network or a similar type of graphical representation. This diagram includes multiple layers of nodes, each labeled with a combination of letters and numbers, such as \"A1,\" \"B2,\" \"C3,\" etc., which could represent different layers in the network. The edges between these nodes are also labeled with numerical values ranging from 0 to 4, indicating the strength or weight of the connections between them.\n\nThe diagrams are presented against a white background and appear to be technical illustrations, possibly used for educational or research purposes to explain concepts related to neural networks, flow processes, or similar systems. The style of the image is informational and schematic, with a focus on clarity and precision in representing the connections and structures within each diagram. ",
    "raw_text": null
  },
  {
    "chunk_id": "a5877a18-57b1-4a4a-aa96-f17fed733f72",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image is a flowchart with a colorful layout that represents a process or algorithm. It consists of several rectangular boxes connected by arrows indicating the flow of data or steps in the process. Each box contains text and numbers, which are likely to be instructions or variables in the context of the flowchart.\n\nAt the top left corner, there is a label \"1\" with an arrow pointing to another labeled \"2,\" suggesting a sequence or step-by-step progression. The boxes are color-coded, with different colors used for different parts of the process. For example, one section uses red and blue colors, while another uses green and purple.\n\nThe text within the boxes includes phrases like \"1 GCONV,\" \"2 BN RELU,\" \"3 DW CONV,\" \"4 3x3 MAX POOL,\" \"5 3x3 AVG POOL,\" \"6 3x3 MAX POOL,\" and \"7 3x3 AVG POOL.\" These terms are likely to refer to specific operations in a neural network, such as convolutional layers with different parameters.\n\nThe flowchart also includes mathematical expressions like \"1 GCONV\" and \"2 BN RELU,\" which could be referring to the activation functions used in neural networks. The numbers \"3\" and \"4\" are associated with certain operations or steps, possibly indicating the number of repetitions or iterations within a loop or function.\n\nThe overall style of the image is technical and schematic, typical of diagrams used in computer science or engineering to illustrate complex processes or algorithms. The flowchart appears to be related to machine learning or artificial intelligence, specifically discussing the architecture of a neural network. ",
    "raw_text": null
  },
  {
    "chunk_id": "712ed437-e34c-4452-98ef-7e9efa0e73e7",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image is a screenshot of a computer screen displaying a scatter plot graph with various data points. The graph has a grid background and includes a title at the top that reads \"NASA/NESTAR.\" There are multiple axes on the graph, with the x-axis labeled \"Multiplicity\" ranging from 0 to 10, and the y-axis labeled \"M+,\" which likely stands for \"Mean plus standard deviation.\"\n\nThe data points are represented by different colored dots, each corresponding to a specific value of multiplicity. The colors of the dots vary, with some being blue, green, red, yellow, and others. Each dot is annotated with a label that includes a number, which could represent a specific value or index related to the data.\n\nThe graph also contains text labels at the bottom left corner, which are too small to read clearly in this image. The overall style of the image suggests it's a technical representation of statistical data, possibly from a scientific study or analysis. ",
    "raw_text": null
  },
  {
    "chunk_id": "0f0eb8be-2b1a-4e14-858d-018e6b60130d",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image is a screenshot of a computer-generated graph with a title \"Shortstop between two baselines\". It appears to be a type of radar chart, which is used to display multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point.\n\nThe chart has six axes labeled with the following text: \"Near baselines\", \"Shortstop between baselines\", and \"Far baselines\". Each axis is marked with numerical values ranging from 0 to 10, indicating a scale of measurement for each variable. The axes are color-coded, with different colors representing different variables or categories.\n\nThere are three lines on the chart, each corresponding to a different category: \"Start\", \"Stop\", and \"Shortstop\". These lines show the values of the variables at various points along their respective paths. The \"Start\" line begins at the origin (0,0) and extends towards the \"Near baselines\" axis, indicating an increase in value as it moves away from the origin. The \"Stop\" line starts at the origin and ends at a point on the \"Shortstop between baselines\" axis, suggesting a decrease in value along this path. The \"Shortstop\" line begins at the origin, extends towards the \"Near baselines\" axis, then curves upwards before ending at a point on the \"Far baselines\" axis, indicating an increase in value along its entire path.\n\nThe chart is overlaid with a grid that helps to visualize the data points more clearly. The background of the image is white, and the text and lines are in black, making it easy to distinguish between them. There are no visible texts other than the labels on the axes and the title of the chart. ",
    "raw_text": null
  },
  {
    "chunk_id": "45284b4b-569b-42ca-9caf-dc078fc5cd2b",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image appears to be a screenshot of a computer screen displaying a series of graphs and charts. The content is too small to read the specific details of each graph, but they seem to represent various types of data visualizations such as line graphs, bar graphs, and possibly scatter plots or histograms. Each graph has different axes with numerical values, suggesting that they are quantitative in nature.\n\nThe top left corner of the image shows a partial view of a window title, which includes the text \"Untitled - Step 0,\" indicating that this is likely a screenshot from a software application or a presentation tool where the user has not yet given a name to the document and is at the initial step.\n\nThe style of the image is digital and appears to be taken from a computer screen, as indicated by the pixelation and the window controls visible in the top left corner. The overall quality of the image is low resolution, making it difficult to discern specific details within each graph. ",
    "raw_text": null
  },
  {
    "chunk_id": "14dc1068-79a5-4f5a-a4ea-a545624c811f",
    "modality": "image",
    "source_pdf": "MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf",
    "page_number": null,
    "retrieval_text": " The image is a collage of various photographs arranged in a grid format. At the top of the collage, there are two rows of images: the first row shows a person riding a horse, followed by a dog standing on its hind legs, and then a person riding a motorcycle. The second row features a person riding a bicycle, another person riding a skateboard, and finally, a person riding a scooter.\n\nBelow these images, there are two rows of images that appear to be related to animals: the first row shows a horse, a dog, and a cat, while the second row displays a bird, a fish, and another animal whose identity is not clear due to the image's resolution.\n\nThe bottom row contains a series of images showing people riding different types of vehicles or engaging in various activities. From left to right, the first image shows a person on a horse, followed by a person on a bicycle, a person on a skateboard, and a person on a scooter. The last image in this row is not clearly visible due to the resolution.\n\nThe style of the collage suggests it may be used for educational or illustrative purposes, possibly to demonstrate different modes of transportation or activities involving animals. The images are arranged in a way that alternates between human and animal subjects, with each row transitioning from one type of activity to another. ",
    "raw_text": null
  },
  {
    "chunk_id": "717c1796-70f0-4947-8780-c1df7f9a3cc9",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 1,
    "retrieval_text": "5 2015 1 0 2 c e D 0 1 ] V C . s c [ 1 v 5 8 3 3 0 . 2 1 5 1 : v i X r a Deep Residual Learning for Image Recognition Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun Microsoft Research kahe, v-xiangz, v-shren, jiansun @microsoft.com { } Abstract Deeper neural networks are more dif\ufb01cult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148 deeper than VGG nets [41] but still having lower complex- \u00d7 ity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classi\ufb01cation task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28% relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation. 1. Introduction Deep convolutional neural networks [22, 21] have led to a series of breakthroughs for image classi\ufb01cation [21, 50, 40]. Deep networks naturally integrate low/mid/high- level features [50] and classi\ufb01ers in an end-to-end multi- layer fashion, and the \u201clevels\u201d of features can be enriched by the number of stacked layers (depth). Recent evidence [41, 44] reveals that network depth is of crucial importance, and the leading results [41, 44, 13, 16] on the challenging ImageNet dataset [36] all exploit \u201cvery deep\u201d [41] models, with a depth of sixteen [41] to thirty [16]. Many other non- trivial visual recognition tasks [8, 12, 7, 32, 27] have also 1http://image-net.org/challenges/LSVRC/2015/ http://mscoco.org/dataset/#detections-challenge2015. and 56-layer 20-layer 56-layer 20-layer * iter, (led) * ter. (1e4) Figure 1. Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer \u201cplain\u201d networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is presented in Fig. 4. greatly bene\ufb01ted from very deep models. Driven by the signi\ufb01cance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [1, 9], which hamper convergence from the beginning. This problem, however, has been largely addressed by normalized initial- ization [23, 9, 37, 13] and intermediate normalization layers [16], which enable networks with tens of layers to start con- verging for stochastic gradient descent (SGD) with back- propagation [22]. When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by over\ufb01tting, and adding more layers to a suitably deep model leads to higher train- ing error, as reported in [11, 42] and thoroughly veri\ufb01ed by our experiments. Fig. 1 shows a typical example. The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to \ufb01nd solutions that 1 x y weight layer F(x) V relu x weight layer identity Figure 2. Residual learning: a building block. are comparably good or better than the constructed solution (or unable to do so in feasible time). In this paper, we address the degradation problem by introducing a deep residual learning framework. In- stead of hoping each few stacked layers directly \ufb01t a desired underlying mapping, we explicitly let these lay- ers \ufb01t a residual mapping. Formally, denoting the desired underlying mapping as (x), we let the stacked nonlinear H layers \ufb01t another mapping of x. The orig- (x) := (x) F H \u2212 inal mapping is recast into (x)+x. We hypothesize that it F is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to \ufb01t an identity mapping by a stack of nonlinear layers. The formulation of (x)+x can be realized by feedfor- F ward neural networks with \u201cshortcut connections\u201d (Fig. 2). Shortcut connections [2, 34, 49] are those skipping one or more layers. In our case, the shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers (Fig. 2). Identity short- cut connections add neither extra parameter nor computa- tional complexity. The entire network can still be trained end-to-end by SGD with backpropagation, and can be eas- ily implemented using common libraries (e.g., Caffe [19]) without modifying the solvers. We present comprehensive experiments on ImageNet [36] to show the degradation problem and evaluate our method. We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart \u201cplain\u201d nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing re- sults substantially better than previous networks. Similar phenomena are also shown on the CIFAR-10 set [20], suggesting that the optimization dif\ufb01culties and the effects of our method are not just akin to a particular dataset. We present successfully trained models on this dataset with over 100 layers, and explore models with over 1000 layers. On the ImageNet classi\ufb01cation dataset [36], we obtain excellent results by extremely deep residual nets. Our 152- layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets [41]. Our ensemble has 3.57% top-5 error on the 2 ImageNet test set, and won the 1st place in the ILSVRC 2015 classi\ufb01cation competition. The extremely deep rep- resentations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO 2015 competitions. This strong evidence shows that the residual learning principle is generic, and we expect that it is applicable in other vision and non-vision problems.",
    "raw_text": "5 2015 1 0 2 c e D 0 1 ] V C . s c [ 1 v 5 8 3 3 0 . 2 1 5 1 : v i X r a Deep Residual Learning for Image Recognition Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun Microsoft Research kahe, v-xiangz, v-shren, jiansun @microsoft.com { } Abstract Deeper neural networks are more dif\ufb01cult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148 deeper than VGG nets [41] but still having lower complex- \u00d7 ity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classi\ufb01cation task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28% relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation. 1. Introduction Deep convolutional neural networks [22, 21] have led to a series of breakthroughs for image classi\ufb01cation [21, 50, 40]. Deep networks naturally integrate low/mid/high- level features [50] and classi\ufb01ers in an end-to-end multi- layer fashion, and the \u201clevels\u201d of features can be enriched by the number of stacked layers (depth). Recent evidence [41, 44] reveals that network depth is of crucial importance, and the leading results [41, 44, 13, 16] on the challenging ImageNet dataset [36] all exploit \u201cvery deep\u201d [41] models, with a depth of sixteen [41] to thirty [16]. Many other non- trivial visual recognition tasks [8, 12, 7, 32, 27] have also 1http://image-net.org/challenges/LSVRC/2015/ http://mscoco.org/dataset/#detections-challenge2015. and 56-layer 20-layer 56-layer 20-layer * iter, (led) * ter. (1e4) Figure 1. Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer \u201cplain\u201d networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is presented in Fig. 4. greatly bene\ufb01ted from very deep models. Driven by the signi\ufb01cance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [1, 9], which hamper convergence from the beginning. This problem, however, has been largely addressed by normalized initial- ization [23, 9, 37, 13] and intermediate normalization layers [16], which enable networks with tens of layers to start con- verging for stochastic gradient descent (SGD) with back- propagation [22]. When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by over\ufb01tting, and adding more layers to a suitably deep model leads to higher train- ing error, as reported in [11, 42] and thoroughly veri\ufb01ed by our experiments. Fig. 1 shows a typical example. The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to \ufb01nd solutions that 1 x y weight layer F(x) V relu x weight layer identity Figure 2. Residual learning: a building block. are comparably good or better than the constructed solution (or unable to do so in feasible time). In this paper, we address the degradation problem by introducing a deep residual learning framework. In- stead of hoping each few stacked layers directly \ufb01t a desired underlying mapping, we explicitly let these lay- ers \ufb01t a residual mapping. Formally, denoting the desired underlying mapping as (x), we let the stacked nonlinear H layers \ufb01t another mapping of x. The orig- (x) := (x) F H \u2212 inal mapping is recast into (x)+x. We hypothesize that it F is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to \ufb01t an identity mapping by a stack of nonlinear layers. The formulation of (x)+x can be realized by feedfor- F ward neural networks with \u201cshortcut connections\u201d (Fig. 2). Shortcut connections [2, 34, 49] are those skipping one or more layers. In our case, the shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers (Fig. 2). Identity short- cut connections add neither extra parameter nor computa- tional complexity. The entire network can still be trained end-to-end by SGD with backpropagation, and can be eas- ily implemented using common libraries (e.g., Caffe [19]) without modifying the solvers. We present comprehensive experiments on ImageNet [36] to show the degradation problem and evaluate our method. We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart \u201cplain\u201d nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing re- sults substantially better than previous networks. Similar phenomena are also shown on the CIFAR-10 set [20], suggesting that the optimization dif\ufb01culties and the effects of our method are not just akin to a particular dataset. We present successfully trained models on this dataset with over 100 layers, and explore models with over 1000 layers. On the ImageNet classi\ufb01cation dataset [36], we obtain excellent results by extremely deep residual nets. Our 152- layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets [41]. Our ensemble has 3.57% top-5 error on the 2 ImageNet test set, and won the 1st place in the ILSVRC 2015 classi\ufb01cation competition. The extremely deep rep- resentations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO 2015 competitions. This strong evidence shows that the residual learning principle is generic, and we expect that it is applicable in other vision and non-vision problems."
  },
  {
    "chunk_id": "d35b0d81-4ffd-4373-b4a7-4e66e31d80a1",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 2,
    "retrieval_text": "2. Related Work Residual Representations. In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD. Both of them are powerful shallow representations for image re- trieval and classi\ufb01cation [4, 48]. For vector quantization, encoding residual vectors [17] is shown to be more effec- tive than encoding original vectors. In low-level vision and computer graphics, for solv- ing Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subprob- lems at multiple scales, where each subproblem is respon- sible for the residual solution between a coarser and a \ufb01ner scale. An alternative to Multigrid is hierarchical basis pre- conditioning [45, 46], which relies on variables that repre- sent residual vectors between two scales. It has been shown [3, 45, 46] that these solvers converge much faster than stan- dard solvers that are unaware of the residual nature of the solutions. These methods suggest that a good reformulation or preconditioning can simplify the optimization. Shortcut Connections. Practices and theories that lead to shortcut connections [2, 34, 49] have been studied for a long time. An early practice of training multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output [34, 49]. In [44, 24], a few interme- diate layers are directly connected to auxiliary classi\ufb01ers for addressing vanishing/exploding gradients. The papers of [39, 38, 31, 47] propose methods for centering layer re- sponses, gradients, and propagated errors, implemented by shortcut connections. In [44], an \u201cinception\u201d layer is com- posed of a shortcut branch and a few deeper branches. Concurrent with our work, \u201chighway networks\u201d [42, 43] present shortcut connections with gating functions [15]. These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are parameter-free. When a gated shortcut is \u201cclosed\u201d (approaching zero), the layers in highway networks represent non-residual func- tions. On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, and all information is always passed through, with addi- tional residual functions to be learned. In addition, high- way networks have not demonstrated accuracy gains with extremely increased depth (e.g., over 100 layers).",
    "raw_text": "2. Related Work Residual Representations. In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD. Both of them are powerful shallow representations for image re- trieval and classi\ufb01cation [4, 48]. For vector quantization, encoding residual vectors [17] is shown to be more effec- tive than encoding original vectors. In low-level vision and computer graphics, for solv- ing Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subprob- lems at multiple scales, where each subproblem is respon- sible for the residual solution between a coarser and a \ufb01ner scale. An alternative to Multigrid is hierarchical basis pre- conditioning [45, 46], which relies on variables that repre- sent residual vectors between two scales. It has been shown [3, 45, 46] that these solvers converge much faster than stan- dard solvers that are unaware of the residual nature of the solutions. These methods suggest that a good reformulation or preconditioning can simplify the optimization. Shortcut Connections. Practices and theories that lead to shortcut connections [2, 34, 49] have been studied for a long time. An early practice of training multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output [34, 49]. In [44, 24], a few interme- diate layers are directly connected to auxiliary classi\ufb01ers for addressing vanishing/exploding gradients. The papers of [39, 38, 31, 47] propose methods for centering layer re- sponses, gradients, and propagated errors, implemented by shortcut connections. In [44], an \u201cinception\u201d layer is com- posed of a shortcut branch and a few deeper branches. Concurrent with our work, \u201chighway networks\u201d [42, 43] present shortcut connections with gating functions [15]. These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are parameter-free. When a gated shortcut is \u201cclosed\u201d (approaching zero), the layers in highway networks represent non-residual func- tions. On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, and all information is always passed through, with addi- tional residual functions to be learned. In addition, high- way networks have not demonstrated accuracy gains with extremely increased depth (e.g., over 100 layers)."
  },
  {
    "chunk_id": "d86df011-6249-4656-a6c5-2989f636bdf6",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 3,
    "retrieval_text": "3. Deep Residual Learning 3.1. Residual Learning Let us consider (x) as an underlying mapping to be H \ufb01t by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the \ufb01rst of these layers. If one hypothesizes that multiple nonlinear layers can asymptoti- cally approximate complicated functions2, then it is equiv- alent to hypothesize that they can asymptotically approxi- mate the residual functions, i.e., x (assuming that (x) H \u2212 the input and output are of the same dimensions). So rather than expect stacked layers to approximate (x), we H explicitly let these layers approximate a residual function x. The original function thus becomes (x) (x) := F \u2212 H (x)+x. Although both forms should be able to asymptot- F ically approximate the desired functions (as hypothesized), the ease of learning might be different. This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left). As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counter- part. The degradation problem suggests that the solvers might have dif\ufb01culties in approximating identity mappings by multiple nonlinear layers. With the residual learning re- formulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear lay- ers toward zero to approach identity mappings. In real cases, it is unlikely that identity mappings are op- timal, but our reformulation may help to precondition the problem. If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to \ufb01nd the perturbations with reference to an identity mapping, than to learn the function as a new one. We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity map- pings provide reasonable preconditioning.",
    "raw_text": "3. Deep Residual Learning 3.1. Residual Learning Let us consider (x) as an underlying mapping to be H \ufb01t by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the \ufb01rst of these layers. If one hypothesizes that multiple nonlinear layers can asymptoti- cally approximate complicated functions2, then it is equiv- alent to hypothesize that they can asymptotically approxi- mate the residual functions, i.e., x (assuming that (x) H \u2212 the input and output are of the same dimensions). So rather than expect stacked layers to approximate (x), we H explicitly let these layers approximate a residual function x. The original function thus becomes (x) (x) := F \u2212 H (x)+x. Although both forms should be able to asymptot- F ically approximate the desired functions (as hypothesized), the ease of learning might be different. This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left). As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counter- part. The degradation problem suggests that the solvers might have dif\ufb01culties in approximating identity mappings by multiple nonlinear layers. With the residual learning re- formulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear lay- ers toward zero to approach identity mappings. In real cases, it is unlikely that identity mappings are op- timal, but our reformulation may help to precondition the problem. If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to \ufb01nd the perturbations with reference to an identity mapping, than to learn the function as a new one. We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity map- pings provide reasonable preconditioning."
  },
  {
    "chunk_id": "c51eedd9-4e1a-4b33-9349-a6b978659fd0",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 3,
    "retrieval_text": "3.2. Identity Mapping by Shortcuts We adopt residual learning to every few stacked layers. A building block is shown in Fig. 2. Formally, in this paper we consider a building block de\ufb01ned as: y = (x, Wi ) + x. (1) F { } Here x and y are the input and output vectors of the lay- ers considered. The function (x, ) represents the Wi { F } residual mapping to be learned. For the example in Fig. 2 that has two layers, = W2\u03c3(W1x) in which \u03c3 denotes F 2This hypothesis, however, is still an open question. See [28]. 3 ReLU [29] and the biases are omitted for simplifying no- tations. The operation + x is performed by a shortcut F connection and element-wise addition. We adopt the sec- ond nonlinearity after the addition (i.e., \u03c3(y), see Fig. 2). The shortcut connections in Eqn.(1) introduce neither ex- tra parameter nor computation complexity. This is not only attractive in practice but also important in our comparisons between plain and residual networks. We can fairly com- pare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computa- tional cost (except for the negligible element-wise addition). The dimensions of x and must be equal in Eqn.(1). F If this is not the case (e.g., when changing the input/output channels), we can perform a linear projection Ws by the shortcut connections to match the dimensions: y = (x, Wi ) + Wsx. (2) F { } We can also use a square matrix Ws in Eqn.(1). But we will show by experiments that the identity mapping is suf\ufb01cient for addressing the degradation problem and is economical, and thus Ws is only used when matching dimensions. The form of the residual function is \ufb02exible. Exper- F iments in this paper involve a function that has two or F three layers (Fig. 5), while more layers are possible. But if has only a single layer, Eqn.(1) is similar to a linear layer: F y = W1x+x, for which we have not observed advantages. We also note that although the above notations are about fully-connected layers for simplicity, they are applicable to (x, Wi ) can repre- convolutional layers. The function { F } sent multiple convolutional layers. The element-wise addi- tion is performed on two feature maps, channel by channel.",
    "raw_text": "3.2. Identity Mapping by Shortcuts We adopt residual learning to every few stacked layers. A building block is shown in Fig. 2. Formally, in this paper we consider a building block de\ufb01ned as: y = (x, Wi ) + x. (1) F { } Here x and y are the input and output vectors of the lay- ers considered. The function (x, ) represents the Wi { F } residual mapping to be learned. For the example in Fig. 2 that has two layers, = W2\u03c3(W1x) in which \u03c3 denotes F 2This hypothesis, however, is still an open question. See [28]. 3 ReLU [29] and the biases are omitted for simplifying no- tations. The operation + x is performed by a shortcut F connection and element-wise addition. We adopt the sec- ond nonlinearity after the addition (i.e., \u03c3(y), see Fig. 2). The shortcut connections in Eqn.(1) introduce neither ex- tra parameter nor computation complexity. This is not only attractive in practice but also important in our comparisons between plain and residual networks. We can fairly com- pare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computa- tional cost (except for the negligible element-wise addition). The dimensions of x and must be equal in Eqn.(1). F If this is not the case (e.g., when changing the input/output channels), we can perform a linear projection Ws by the shortcut connections to match the dimensions: y = (x, Wi ) + Wsx. (2) F { } We can also use a square matrix Ws in Eqn.(1). But we will show by experiments that the identity mapping is suf\ufb01cient for addressing the degradation problem and is economical, and thus Ws is only used when matching dimensions. The form of the residual function is \ufb02exible. Exper- F iments in this paper involve a function that has two or F three layers (Fig. 5), while more layers are possible. But if has only a single layer, Eqn.(1) is similar to a linear layer: F y = W1x+x, for which we have not observed advantages. We also note that although the above notations are about fully-connected layers for simplicity, they are applicable to (x, Wi ) can repre- convolutional layers. The function { F } sent multiple convolutional layers. The element-wise addi- tion is performed on two feature maps, channel by channel."
  },
  {
    "chunk_id": "0563076e-0bc3-4200-bc74-63fef6670b3d",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 3,
    "retrieval_text": "3.3. Network Architectures We have tested various plain/residual nets, and have ob- served consistent phenomena. To provide instances for dis- cussion, we describe two models for ImageNet as follows. Plain Network. Our plain baselines (Fig. 3, middle) are mainly inspired by the philosophy of VGG nets [41] (Fig. 3, left). The convolutional layers mostly have 3 3 \ufb01lters and \u00d7 follow two simple design rules: (i) for the same output feature map size, the layers have the same number of \ufb01l- ters; and (ii) if the feature map size is halved, the num- ber of \ufb01lters is doubled so as to preserve the time com- plexity per layer. We perform downsampling directly by convolutional layers that have a stride of 2. The network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax. The total number of weighted layers is 34 in Fig. 3 (middle). It is worth noticing that our model has fewer \ufb01lters and lower complexity than VGG nets [41] (Fig. 3, left). Our 34- layer baseline has 3.6 billion FLOPs (multiply-adds), which is only 18% of VGG-19 (19.6 billion FLOPs). VGG-19 34-layer plain 34-layer residual image image output 3x3 conv, 64 size: 224 u \u00a5 3x3 conv, 64 pool, /2 output size: 112 3x3 conv, 128 \u00a5v 3x3 conv, 128 \u20187x7 conv, 64, /2 v v pool, /2 pool, /2 output see56 [333 ony 256 2 conw 64 \u00a5v \u00a5 Ba con 256 Ba con 6 38 conv, 56 Ba con 64 \u00a5 \u00a5 38 conv, 256 Ba con 64 v 3x3 conv, 64 Ba conv, 64 5 Za pool, /2 3x3 conv, 128, /2 output sue28 Tagen Bacon 28 \u00a5 Ba conv, 512 Bid conv, 128 \u00a5v \u00a5v Ba conv, S12 Bid conv, 198 Bd conv, S12 Bid conv, 128 v 3rd conv, 128 3d conv, 198 Bd conv, 128 output WOO size: 14 pool, /2 3x3 conv, 256, /2 3d conv, SD 3rd conv, 256 \u00a5v es Bacon Sia Bacon B56 \u00a5v \u00a5 Bacon 52 3a conv 256 Been sz 33 conv, 256 \u00a5 BS eon 256 3rd conv, 256 3x3 con, 256 Ba conv, 256 Bed eon 256 \u00a5 3rd conv, 256 3rd conv, 256 utput Vo cue pool, /2 30d conv, 512, 2 sive: 7 v 3d conv, 52 Bd conv, 512 \u00a5v Ba conv, 512 3rd conv, 52 3x3 conv, 52 \u00a5 output 7.4096 avg pool size:1 \u00a5v image \u20187x7 conv, 64, /2 v pool, /2 [3 conv 6a \u00a5v [acon 6a [scom6a \u00a5 [secon ea 3rd conv, 64 Bid conv, 64 c= [36 conv, 128,72 (arenas ee Bad conv, 128 \u00a5v Bd con, 128 Bd conv, 128 \u00a5 Bd eony, 28 Bid conv, 128 Bid conv, 128 EEE 3x3 conv, 256, /2 3a conv, 256 [amenase v [scones [pean \u00a5v [eens Bad conv, 256 3rd conv, 256 Bed conv, 256 [eens Bad conv, 256 3rd conv, 256 3xd conv, 512, 72 \u00a5v Wes = Bd conv, 51D Bd conv, 512 v Bd conv, 512 3rd conv, 512 3rd conv, S12 avg pool 64096 1000 %1000 7 1000 Figure 3. Example network architectures for ImageNet. Left: the VGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid- dle: a plain network with 34 parameter layers (3.6 billion FLOPs). Right: a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions. Table 1 shows more details and other variants. 4 Residual Network. Based on the above plain network, we insert shortcut connections (Fig. 3, right) which turn the network into its counterpart residual version. The identity shortcuts (Eqn.(1)) can be directly used when the input and output are of the same dimensions (solid line shortcuts in Fig. 3). When the dimensions increase (dotted line shortcuts in Fig. 3), we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) The projection shortcut in Eqn.(2) is used to match dimensions (done by 1 1 convolutions). For both \u00d7 options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2.",
    "raw_text": "3.3. Network Architectures We have tested various plain/residual nets, and have ob- served consistent phenomena. To provide instances for dis- cussion, we describe two models for ImageNet as follows. Plain Network. Our plain baselines (Fig. 3, middle) are mainly inspired by the philosophy of VGG nets [41] (Fig. 3, left). The convolutional layers mostly have 3 3 \ufb01lters and \u00d7 follow two simple design rules: (i) for the same output feature map size, the layers have the same number of \ufb01l- ters; and (ii) if the feature map size is halved, the num- ber of \ufb01lters is doubled so as to preserve the time com- plexity per layer. We perform downsampling directly by convolutional layers that have a stride of 2. The network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax. The total number of weighted layers is 34 in Fig. 3 (middle). It is worth noticing that our model has fewer \ufb01lters and lower complexity than VGG nets [41] (Fig. 3, left). Our 34- layer baseline has 3.6 billion FLOPs (multiply-adds), which is only 18% of VGG-19 (19.6 billion FLOPs). VGG-19 34-layer plain 34-layer residual image image output 3x3 conv, 64 size: 224 u \u00a5 3x3 conv, 64 pool, /2 output size: 112 3x3 conv, 128 \u00a5v 3x3 conv, 128 \u20187x7 conv, 64, /2 v v pool, /2 pool, /2 output see56 [333 ony 256 2 conw 64 \u00a5v \u00a5 Ba con 256 Ba con 6 38 conv, 56 Ba con 64 \u00a5 \u00a5 38 conv, 256 Ba con 64 v 3x3 conv, 64 Ba conv, 64 5 Za pool, /2 3x3 conv, 128, /2 output sue28 Tagen Bacon 28 \u00a5 Ba conv, 512 Bid conv, 128 \u00a5v \u00a5v Ba conv, S12 Bid conv, 198 Bd conv, S12 Bid conv, 128 v 3rd conv, 128 3d conv, 198 Bd conv, 128 output WOO size: 14 pool, /2 3x3 conv, 256, /2 3d conv, SD 3rd conv, 256 \u00a5v es Bacon Sia Bacon B56 \u00a5v \u00a5 Bacon 52 3a conv 256 Been sz 33 conv, 256 \u00a5 BS eon 256 3rd conv, 256 3x3 con, 256 Ba conv, 256 Bed eon 256 \u00a5 3rd conv, 256 3rd conv, 256 utput Vo cue pool, /2 30d conv, 512, 2 sive: 7 v 3d conv, 52 Bd conv, 512 \u00a5v Ba conv, 512 3rd conv, 52 3x3 conv, 52 \u00a5 output 7.4096 avg pool size:1 \u00a5v image \u20187x7 conv, 64, /2 v pool, /2 [3 conv 6a \u00a5v [acon 6a [scom6a \u00a5 [secon ea 3rd conv, 64 Bid conv, 64 c= [36 conv, 128,72 (arenas ee Bad conv, 128 \u00a5v Bd con, 128 Bd conv, 128 \u00a5 Bd eony, 28 Bid conv, 128 Bid conv, 128 EEE 3x3 conv, 256, /2 3a conv, 256 [amenase v [scones [pean \u00a5v [eens Bad conv, 256 3rd conv, 256 Bed conv, 256 [eens Bad conv, 256 3rd conv, 256 3xd conv, 512, 72 \u00a5v Wes = Bd conv, 51D Bd conv, 512 v Bd conv, 512 3rd conv, 512 3rd conv, S12 avg pool 64096 1000 %1000 7 1000 Figure 3. Example network architectures for ImageNet. Left: the VGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid- dle: a plain network with 34 parameter layers (3.6 billion FLOPs). Right: a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions. Table 1 shows more details and other variants. 4 Residual Network. Based on the above plain network, we insert shortcut connections (Fig. 3, right) which turn the network into its counterpart residual version. The identity shortcuts (Eqn.(1)) can be directly used when the input and output are of the same dimensions (solid line shortcuts in Fig. 3). When the dimensions increase (dotted line shortcuts in Fig. 3), we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) The projection shortcut in Eqn.(2) is used to match dimensions (done by 1 1 convolutions). For both \u00d7 options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2."
  },
  {
    "chunk_id": "3fda9877-5567-4f9b-b080-0a46913081ef",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 4,
    "retrieval_text": "3.4. Implementation Our implementation for ImageNet follows the practice in [21, 41]. The image is resized with its shorter side ran- domly sampled in [256,480] for scale augmentation [41]. A 224 224 crop is randomly sampled from an image or its \u00d7 horizontal \ufb02ip, with the per-pixel mean subtracted [21]. The standard color augmentation in [21] is used. We adopt batch normalization (BN) [16] right after each convolution and before activation, following [16]. We initialize the weights as in [13] and train all plain/residual nets from scratch. We use SGD with a mini-batch size of 256. The learning rate starts from 0.1 and is divided by 10 when the error plateaus, 104 iterations. We and the models are trained for up to 60 \u00d7 use a weight decay of 0.0001 and a momentum of 0.9. We do not use dropout [14], following the practice in [16]. In testing, for comparison studies we adopt the standard 10-crop testing [21]. For best results, we adopt the fully- convolutional form as in [41, 13], and average the scores at multiple scales (images are resized such that the shorter side is in 224,256,384,480,640 ). { } 4. Experiments 4.1. ImageNet Classi\ufb01cation We evaluate our method on the ImageNet 2012 classi\ufb01- cation dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images, and evalu- ated on the 50k validation images. We also obtain a \ufb01nal result on the 100k test images, reported by the test server. We evaluate both top-1 and top-5 error rates. Plain Networks. We \ufb01rst evaluate 18-layer and 34-layer plain nets. The 34-layer plain net is in Fig. 3 (middle). The 18-layer plain net is of a similar form. See Table 1 for de- tailed architectures. The results in Table 2 show that the deeper 34-layer plain net has higher validation error than the shallower 18-layer plain net. To reveal the reasons, in Fig. 4 (left) we com- pare their training/validation errors during the training pro- cedure. We have observed the degradation problem - the Tayer name | output size T8-layer 34-layer 30-layer 101-layer 152-layer convl | 112x112 7X7, 64, stride 2 3x3 max pool, stride 2 1x1, 64 1x1, 64 1x1, 64 conv2.x | 56x56 [ an | x2 [ et | 3 3x3,64 | x3 3x3,64 | x3 3x3, 64 | x3 a 7 1x1, 256 1x1, 256 1x1, 256 Ix, 128 1x1, 128 1x1, 128 cony3.x | 28x28 [ ae }2 [ a be | x4} | 3x3, 128 | x4 3x3, 128 | x4 3x3, 128 | x8 7 a 1x1, 512 1x1, 512 1x1, 512 1x1, 256 1x1, 256 1x1, 256 2 conv4.x | 14x14 [ poe }= [33 ze | 3x3,256 | x6]] 3x3,256 | x23 |] 3x3,256 | x36 79, 290 HO, 0 1x1, 1024 11, 1024 1x1, 1024 1x1, 512 1x1, 512 1x1, 512 2 convs.x | 7x7 [ Ra }= [ 3383 }s 3x3,512 | x3 | | 3x3,512 | x3 3x3,512 | x3 pone oe 1x1, 2048 1x1, 2048 1x1, 2048 xl average pool, 1000-d fe, softmax FLOPs 18x10\" 3.6x10\" 3.8x10\" 76x10\" 11.3x10\" layer name output size \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 Table 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down- sampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2. \u2014plain-34 l=. . A . : 0 10 30 iter. (1e4) 60} AMA DAM 30 - -----------j4.--=25> ae \u2014ResNet-18 NR aly \u2014ResNet-34 34-layer 205 10 20 30 40 50 iter. (1e4) Figure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts. plain ResNet 18 layers 27.94 27.88 34 layers 28.54 25.03 Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation. Here the ResNets have no extra parameter compared to their plain counterparts. Fig. 4 shows the training procedures. 34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one. We argue that this optimization dif\ufb01culty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero variances. We also verify that the backward propagated gradients exhibit healthy norms with BN. So neither forward nor backward signals vanish. In fact, the 34-layer plain net is still able to achieve compet- itive accuracy (Table 3), suggesting that the solver works to some extent. We conjecture that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error3. The reason for such opti- mization dif\ufb01culties will be studied in the future. Residual Networks. Next we evaluate 18-layer and 34- layer residual nets (ResNets). The baseline architectures are the same as the above plain nets, expect that a shortcut connection is added to each pair of 3 3 \ufb01lters as in Fig. 3 \u00d7 (right). In the \ufb01rst comparison (Table 2 and Fig. 4 right), we use identity mapping for all shortcuts and zero-padding for increasing dimensions (option A). So they have no extra parameter compared to the plain counterparts. We have three major observations from Table 2 and Fig. 4. First, the situation is reversed with residual learn- ing \u2013 the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data. This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth. Second, compared to its plain counterpart, the 34-layer 3We have experimented with more training iterations (3\u00d7) and still ob- served the degradation problem, suggesting that this problem cannot be feasibly addressed by simply using more iterations. 5 model top-1 err. top-5 err. VGG-16 [41] 28.07 9.33 GoogLeNet [44] - 9.15 PReLU-net [13] 24.27 7.38 plain-34 28.54 10.02 ResNet-34 A 25.03 7.76 ResNet-34 B 24.52 7.46 ResNet-34 C 24.19 7.40 ResNet-50 22.85 6.71 ResNet-101 21.75 6.05 ResNet-152 21.43 5.71 Table 3. Error rates (%, 10-crop testing) on ImageNet validation. VGG-16 is based on our test. ResNet-50/101/152 are of option B that only uses projections for increasing dimensions. method top-1 err. top-5 err. VGG [41] (ILSVRC\u201914) - 8.43\u2020 GoogLeNet [44] (ILSVRC\u201914) - 7.89 VGG [41] (v5) 24.4 7.1 PReLU-net [13] 21.59 5.71 BN-inception [16] 21.99 5.81 ResNet-34 B 21.84 5.71 ResNet-34 C 21.53 5.60 ResNet-50 20.74 5.25 ResNet-101 19.87 4.60 ResNet-152 19.38 4.49 Table 4. Error rates (%) of single-model results on the ImageNet validation set (except \u2020 reported on the test set). method top-5 err. (test) VGG [41] (ILSVRC\u201914) 7.32 GoogLeNet [44] (ILSVRC\u201914) 6.66 VGG [41] (v5) 6.8 PReLU-net [13] 4.94 BN-inception [16] 4.82 ResNet (ILSVRC\u201915) 3.57 Table 5. Error rates (%) of ensembles. The top-5 error is on the test set of ImageNet and reported by the test server. ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced training error (Fig. 4 right vs. left). This comparison veri\ufb01es the effectiveness of residual learning on extremely deep systems. Last, we also note that the 18-layer plain/residual nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left). When the net is \u201cnot overly deep\u201d (18 layers here), the current SGD solver is still able to \ufb01nd good solutions to the plain net. In this case, the ResNet eases the optimization by providing faster conver- gence at the early stage.",
    "raw_text": "3.4. Implementation Our implementation for ImageNet follows the practice in [21, 41]. The image is resized with its shorter side ran- domly sampled in [256,480] for scale augmentation [41]. A 224 224 crop is randomly sampled from an image or its \u00d7 horizontal \ufb02ip, with the per-pixel mean subtracted [21]. The standard color augmentation in [21] is used. We adopt batch normalization (BN) [16] right after each convolution and before activation, following [16]. We initialize the weights as in [13] and train all plain/residual nets from scratch. We use SGD with a mini-batch size of 256. The learning rate starts from 0.1 and is divided by 10 when the error plateaus, 104 iterations. We and the models are trained for up to 60 \u00d7 use a weight decay of 0.0001 and a momentum of 0.9. We do not use dropout [14], following the practice in [16]. In testing, for comparison studies we adopt the standard 10-crop testing [21]. For best results, we adopt the fully- convolutional form as in [41, 13], and average the scores at multiple scales (images are resized such that the shorter side is in 224,256,384,480,640 ). { } 4. Experiments 4.1. ImageNet Classi\ufb01cation We evaluate our method on the ImageNet 2012 classi\ufb01- cation dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images, and evalu- ated on the 50k validation images. We also obtain a \ufb01nal result on the 100k test images, reported by the test server. We evaluate both top-1 and top-5 error rates. Plain Networks. We \ufb01rst evaluate 18-layer and 34-layer plain nets. The 34-layer plain net is in Fig. 3 (middle). The 18-layer plain net is of a similar form. See Table 1 for de- tailed architectures. The results in Table 2 show that the deeper 34-layer plain net has higher validation error than the shallower 18-layer plain net. To reveal the reasons, in Fig. 4 (left) we com- pare their training/validation errors during the training pro- cedure. We have observed the degradation problem - the Tayer name | output size T8-layer 34-layer 30-layer 101-layer 152-layer convl | 112x112 7X7, 64, stride 2 3x3 max pool, stride 2 1x1, 64 1x1, 64 1x1, 64 conv2.x | 56x56 [ an | x2 [ et | 3 3x3,64 | x3 3x3,64 | x3 3x3, 64 | x3 a 7 1x1, 256 1x1, 256 1x1, 256 Ix, 128 1x1, 128 1x1, 128 cony3.x | 28x28 [ ae }2 [ a be | x4} | 3x3, 128 | x4 3x3, 128 | x4 3x3, 128 | x8 7 a 1x1, 512 1x1, 512 1x1, 512 1x1, 256 1x1, 256 1x1, 256 2 conv4.x | 14x14 [ poe }= [33 ze | 3x3,256 | x6]] 3x3,256 | x23 |] 3x3,256 | x36 79, 290 HO, 0 1x1, 1024 11, 1024 1x1, 1024 1x1, 512 1x1, 512 1x1, 512 2 convs.x | 7x7 [ Ra }= [ 3383 }s 3x3,512 | x3 | | 3x3,512 | x3 3x3,512 | x3 pone oe 1x1, 2048 1x1, 2048 1x1, 2048 xl average pool, 1000-d fe, softmax FLOPs 18x10\" 3.6x10\" 3.8x10\" 76x10\" 11.3x10\" layer name output size \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 Table 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down- sampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2. \u2014plain-34 l=. . A . : 0 10 30 iter. (1e4) 60} AMA DAM 30 - -----------j4.--=25> ae \u2014ResNet-18 NR aly \u2014ResNet-34 34-layer 205 10 20 30 40 50 iter. (1e4) Figure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts. plain ResNet 18 layers 27.94 27.88 34 layers 28.54 25.03 Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation. Here the ResNets have no extra parameter compared to their plain counterparts. Fig. 4 shows the training procedures. 34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one. We argue that this optimization dif\ufb01culty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero variances. We also verify that the backward propagated gradients exhibit healthy norms with BN. So neither forward nor backward signals vanish. In fact, the 34-layer plain net is still able to achieve compet- itive accuracy (Table 3), suggesting that the solver works to some extent. We conjecture that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error3. The reason for such opti- mization dif\ufb01culties will be studied in the future. Residual Networks. Next we evaluate 18-layer and 34- layer residual nets (ResNets). The baseline architectures are the same as the above plain nets, expect that a shortcut connection is added to each pair of 3 3 \ufb01lters as in Fig. 3 \u00d7 (right). In the \ufb01rst comparison (Table 2 and Fig. 4 right), we use identity mapping for all shortcuts and zero-padding for increasing dimensions (option A). So they have no extra parameter compared to the plain counterparts. We have three major observations from Table 2 and Fig. 4. First, the situation is reversed with residual learn- ing \u2013 the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data. This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth. Second, compared to its plain counterpart, the 34-layer 3We have experimented with more training iterations (3\u00d7) and still ob- served the degradation problem, suggesting that this problem cannot be feasibly addressed by simply using more iterations. 5 model top-1 err. top-5 err. VGG-16 [41] 28.07 9.33 GoogLeNet [44] - 9.15 PReLU-net [13] 24.27 7.38 plain-34 28.54 10.02 ResNet-34 A 25.03 7.76 ResNet-34 B 24.52 7.46 ResNet-34 C 24.19 7.40 ResNet-50 22.85 6.71 ResNet-101 21.75 6.05 ResNet-152 21.43 5.71 Table 3. Error rates (%, 10-crop testing) on ImageNet validation. VGG-16 is based on our test. ResNet-50/101/152 are of option B that only uses projections for increasing dimensions. method top-1 err. top-5 err. VGG [41] (ILSVRC\u201914) - 8.43\u2020 GoogLeNet [44] (ILSVRC\u201914) - 7.89 VGG [41] (v5) 24.4 7.1 PReLU-net [13] 21.59 5.71 BN-inception [16] 21.99 5.81 ResNet-34 B 21.84 5.71 ResNet-34 C 21.53 5.60 ResNet-50 20.74 5.25 ResNet-101 19.87 4.60 ResNet-152 19.38 4.49 Table 4. Error rates (%) of single-model results on the ImageNet validation set (except \u2020 reported on the test set). method top-5 err. (test) VGG [41] (ILSVRC\u201914) 7.32 GoogLeNet [44] (ILSVRC\u201914) 6.66 VGG [41] (v5) 6.8 PReLU-net [13] 4.94 BN-inception [16] 4.82 ResNet (ILSVRC\u201915) 3.57 Table 5. Error rates (%) of ensembles. The top-5 error is on the test set of ImageNet and reported by the test server. ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced training error (Fig. 4 right vs. left). This comparison veri\ufb01es the effectiveness of residual learning on extremely deep systems. Last, we also note that the 18-layer plain/residual nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left). When the net is \u201cnot overly deep\u201d (18 layers here), the current SGD solver is still able to \ufb01nd good solutions to the plain net. In this case, the ResNet eases the optimization by providing faster conver- gence at the early stage."
  },
  {
    "chunk_id": "a2b00f74-dff1-4039-a9a9-523b8af9be02",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 6,
    "retrieval_text": "Identity vs. Projection Shortcuts. We have shown that 6 256-d ix, 64 yeu 3x3, 64 rela 1x1, 256 Figure 5. A deeper residual function F for ImageNet. Left: a building block (on 56\u00d756 feature maps) as in Fig. 3 for ResNet- 34. Right: a \u201cbottleneck\u201d building block for ResNet-50/101/152. parameter-free, identity shortcuts help with training. Next we investigate projection shortcuts (Eqn.(2)). In Table 3 we compare three options: (A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter- free (the same as Table 2 and Fig. 4 right); (B) projec- tion shortcuts are used for increasing dimensions, and other shortcuts are identity; and (C) all shortcuts are projections. Table 3 shows that all three options are considerably bet- ter than the plain counterpart. B is slightly better than A. We argue that this is because the zero-padded dimensions in A indeed have no residual learning. C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts. But the small dif- ferences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. So we do not use option C in the rest of this paper, to reduce mem- ory/time complexity and model sizes. Identity shortcuts are particularly important for not increasing the complexity of the bottleneck architectures that are introduced below. Deeper Bottleneck Architectures. Next we describe our deeper nets for ImageNet. Because of concerns on the train- ing time that we can afford, we modify the building block as a bottleneck design4. For each residual function F use a stack of 3 layers instead of 2 (Fig. 5). The three layers , we are 1 1, 3 1 convolutions, where the 1 3, and 1 1 layers \u00d7 \u00d7 \u00d7 \u00d7 are responsible for reducing and then increasing (restoring) dimensions, leaving the 3 3 layer a bottleneck with smaller \u00d7 input/output dimensions. Fig. 5 shows an example, where both designs have similar time complexity. The parameter-free identity shortcuts are particularly im- portant for the bottleneck architectures. If the identity short- cut in Fig. 5 (right) is replaced with projection, one can show that the time complexity and model size are doubled, as the shortcut is connected to the two high-dimensional ends. So identity shortcuts lead to more ef\ufb01cient models for the bottleneck designs. 50-layer ResNet: We replace each 2-layer block in the 4Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy from increased depth (as shown on CIFAR-10), but are not as economical as the bottleneck ResNets. So the usage of bottleneck designs is mainly due to practical considerations. We further note that the degradation problem of plain nets is also witnessed for the bottleneck designs. 34-layer net with this 3-layer bottleneck block, resulting in a 50-layer ResNet (Table 1). We use option B for increasing dimensions. This model has 3.8 billion FLOPs. 101-layer and 152-layer ResNets: We construct 101- layer and 152-layer ResNets by using more 3-layer blocks (Table 1). Remarkably, although the depth is signi\ufb01cantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 bil- lion FLOPs). The 50/101/152-layer ResNets are more accurate than the 34-layer ones by considerable margins (Table 3 and 4). We do not observe the degradation problem and thus en- joy signi\ufb01cant accuracy gains from considerably increased depth. The bene\ufb01ts of depth are witnessed for all evaluation metrics (Table 3 and 4). Comparisons with State-of-the-art Methods. In Table 4 we compare with the previous best single-model results. Our baseline 34-layer ResNets have achieved very compet- itive accuracy. Our 152-layer ResNet has a single-model top-5 validation error of 4.49%. This single-model result outperforms all previous ensemble results (Table 5). We combine six models of different depth to form an ensemble (only with two 152-layer ones at the time of submitting). This leads to 3.57% top-5 error on the test set (Table 5). This entry won the 1st place in ILSVRC 2015.",
    "raw_text": "Identity vs. Projection Shortcuts. We have shown that 6 256-d ix, 64 yeu 3x3, 64 rela 1x1, 256 Figure 5. A deeper residual function F for ImageNet. Left: a building block (on 56\u00d756 feature maps) as in Fig. 3 for ResNet- 34. Right: a \u201cbottleneck\u201d building block for ResNet-50/101/152. parameter-free, identity shortcuts help with training. Next we investigate projection shortcuts (Eqn.(2)). In Table 3 we compare three options: (A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter- free (the same as Table 2 and Fig. 4 right); (B) projec- tion shortcuts are used for increasing dimensions, and other shortcuts are identity; and (C) all shortcuts are projections. Table 3 shows that all three options are considerably bet- ter than the plain counterpart. B is slightly better than A. We argue that this is because the zero-padded dimensions in A indeed have no residual learning. C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts. But the small dif- ferences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. So we do not use option C in the rest of this paper, to reduce mem- ory/time complexity and model sizes. Identity shortcuts are particularly important for not increasing the complexity of the bottleneck architectures that are introduced below. Deeper Bottleneck Architectures. Next we describe our deeper nets for ImageNet. Because of concerns on the train- ing time that we can afford, we modify the building block as a bottleneck design4. For each residual function F use a stack of 3 layers instead of 2 (Fig. 5). The three layers , we are 1 1, 3 1 convolutions, where the 1 3, and 1 1 layers \u00d7 \u00d7 \u00d7 \u00d7 are responsible for reducing and then increasing (restoring) dimensions, leaving the 3 3 layer a bottleneck with smaller \u00d7 input/output dimensions. Fig. 5 shows an example, where both designs have similar time complexity. The parameter-free identity shortcuts are particularly im- portant for the bottleneck architectures. If the identity short- cut in Fig. 5 (right) is replaced with projection, one can show that the time complexity and model size are doubled, as the shortcut is connected to the two high-dimensional ends. So identity shortcuts lead to more ef\ufb01cient models for the bottleneck designs. 50-layer ResNet: We replace each 2-layer block in the 4Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy from increased depth (as shown on CIFAR-10), but are not as economical as the bottleneck ResNets. So the usage of bottleneck designs is mainly due to practical considerations. We further note that the degradation problem of plain nets is also witnessed for the bottleneck designs. 34-layer net with this 3-layer bottleneck block, resulting in a 50-layer ResNet (Table 1). We use option B for increasing dimensions. This model has 3.8 billion FLOPs. 101-layer and 152-layer ResNets: We construct 101- layer and 152-layer ResNets by using more 3-layer blocks (Table 1). Remarkably, although the depth is signi\ufb01cantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 bil- lion FLOPs). The 50/101/152-layer ResNets are more accurate than the 34-layer ones by considerable margins (Table 3 and 4). We do not observe the degradation problem and thus en- joy signi\ufb01cant accuracy gains from considerably increased depth. The bene\ufb01ts of depth are witnessed for all evaluation metrics (Table 3 and 4). Comparisons with State-of-the-art Methods. In Table 4 we compare with the previous best single-model results. Our baseline 34-layer ResNets have achieved very compet- itive accuracy. Our 152-layer ResNet has a single-model top-5 validation error of 4.49%. This single-model result outperforms all previous ensemble results (Table 5). We combine six models of different depth to form an ensemble (only with two 152-layer ones at the time of submitting). This leads to 3.57% top-5 error on the test set (Table 5). This entry won the 1st place in ILSVRC 2015."
  },
  {
    "chunk_id": "4f56dfed-afa9-4039-8a34-402365e2b2ea",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 7,
    "retrieval_text": "4.2. CIFAR-10 and Analysis We conducted more studies on the CIFAR-10 dataset [20], which consists of 50k training images and 10k test- ing images in 10 classes. We present experiments trained on the training set and evaluated on the test set. Our focus is on the behaviors of extremely deep networks, but not on pushing the state-of-the-art results, so we intentionally use simple architectures as follows. The plain/residual architectures follow the form in Fig. 3 (middle/right). The network inputs are 32 32 images, with \u00d7 the per-pixel mean subtracted. The \ufb01rst layer is 3 3 convo- \u00d7 lutions. Then we use a stack of 6n layers with 3 3 convo- \u00d7 32,16,8 lutions on the feature maps of sizes respectively, } { with 2n layers for each feature map size. The numbers of \ufb01lters are 16,32,64 respectively. The subsampling is per- { } formed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. There are totally 6n+2 stacked weighted layers. The following table summarizes the architecture: output map size 32\u00d732 16\u00d716 8\u00d78 # layers 1+2n 2n 2n # \ufb01lters 16 32 64 When shortcut connections are used, they are connected 3 layers (totally 3n shortcuts). On this to the pairs of 3 \u00d7 dataset we use identity shortcuts in all cases (i.e., option A), 7 method error (%) Maxout [10] 9.38 NIN [25] 8.81 DSN [24] 8.22 # layers # params FitNet [35] 19 2.5M 8.39 Highway [42, 43] 19 2.3M 7.54 (7.72\u00b10.16) Highway [42, 43] 32 1.25M 8.80 ResNet 20 0.27M 8.75 ResNet 32 0.46M 7.51 ResNet 44 0.66M 7.17 ResNet 56 0.85M 6.97 ResNet 110 1.7M 6.43 (6.61\u00b10.16) ResNet 1202 19.4M 7.93 Table 6. Classi\ufb01cation error on the CIFAR-10 test set. All meth- ods are with data augmentation. For ResNet-110, we run it 5 times and show \u201cbest (mean\u00b1std)\u201d as in [43]. so our residual models have exactly the same depth, width, and number of parameters as the plain counterparts. We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in [13] and BN [16] but with no dropout. These models are trained with a mini- batch size of 128 on two GPUs. We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split. We follow the simple data augmen- tation in [24] for training: 4 pixels are padded on each side, and a 32 32 crop is randomly sampled from the padded \u00d7 image or its horizontal \ufb02ip. For testing, we only evaluate the single view of the original 32 32 image. \u00d7 3,5,7,9 , leading to 20, 32, 44, and We compare n = } { 56-layer networks. Fig. 6 (left) shows the behaviors of the plain nets. The deep plain nets suffer from increased depth, and exhibit higher training error when going deeper. This phenomenon is similar to that on ImageNet (Fig. 4, left) and on MNIST (see [42]), suggesting that such an optimization dif\ufb01culty is a fundamental problem. Fig. 6 (middle) shows the behaviors of ResNets. Also similar to the ImageNet cases (Fig. 4, right), our ResNets manage to overcome the optimization dif\ufb01culty and demon- strate accuracy gains when the depth increases. We further explore n = 18 that leads to a 110-layer ResNet. In this case, we \ufb01nd that the initial learning rate of 0.1 is slightly too large to start converging5. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and con- tinue training. The rest of the learning schedule is as done previously. This 110-layer network converges well (Fig. 6, middle). It has fewer parameters than other deep and thin 5With an initial learning rate of 0.1, it starts converging (<90% error) after several epochs, but still reaches similar accuracy. S6-layer 20-layer T 2 3 \u201c+ iter. (Le) iter. (1e4) iter. (Let) Figure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error of plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers. F=-plain-20 F=-plain-20 plain-56 Net-20 I\u2014ResNet-56 I ResNet-110 0 20 layer index (sorted by magnitude) \" Figure 7. Standard deviations (std) of layer responses on CIFAR- 10. The responses are the outputs of each 3\u00d73 layer, after BN and before nonlinearity. Top: the layers are shown in their original order. Bottom: the responses are ranked in descending order. training data 07+12 07++12 test data VOC 07 test VOC 12 test VGG-16 73.2 70.4 ResNet-101 76.4 73.8 Table 7. Object detection mAP (%) on the PASCAL VOC 2007/2012 test sets using baseline Faster R-CNN. See also Ta- ble 10 and 11 for better results. metric mAP@.5 mAP@[.5, .95] VGG-16 41.5 21.2 ResNet-101 48.4 27.2 Table 8. Object detection mAP (%) on the COCO validation set using baseline Faster R-CNN. See also Table 9 for better results. networks such as FitNet [35] and Highway [42] (Table 6), yet is among the state-of-the-art results (6.43%, Table 6). Analysis of Layer Responses. Fig. 7 shows the standard deviations (std) of the layer responses. The responses are the outputs of each 3 3 layer, after BN and before other \u00d7 nonlinearity (ReLU/addition). For ResNets, this analy- sis reveals the response strength of the residual functions. Fig. 7 shows that ResNets have generally smaller responses than their plain counterparts. These results support our ba- sic motivation (Sec.3.1) that the residual functions might be generally closer to zero than the non-residual functions. We also notice that the deeper ResNet has smaller magni- tudes of responses, as evidenced by the comparisons among ResNet-20, 56, and 110 in Fig. 7. When there are more layers, an individual layer of ResNets tends to modify the signal less. Exploring Over 1000 layers. We explore an aggressively deep model of over 1000 layers. We set n = 200 that leads to a 1202-layer network, which is trained as described above. Our method shows no optimization dif\ufb01culty, and this 103-layer network is able to achieve training error <0.1% (Fig. 6, right). Its test error is still fairly good (7.93%, Table 6).",
    "raw_text": "4.2. CIFAR-10 and Analysis We conducted more studies on the CIFAR-10 dataset [20], which consists of 50k training images and 10k test- ing images in 10 classes. We present experiments trained on the training set and evaluated on the test set. Our focus is on the behaviors of extremely deep networks, but not on pushing the state-of-the-art results, so we intentionally use simple architectures as follows. The plain/residual architectures follow the form in Fig. 3 (middle/right). The network inputs are 32 32 images, with \u00d7 the per-pixel mean subtracted. The \ufb01rst layer is 3 3 convo- \u00d7 lutions. Then we use a stack of 6n layers with 3 3 convo- \u00d7 32,16,8 lutions on the feature maps of sizes respectively, } { with 2n layers for each feature map size. The numbers of \ufb01lters are 16,32,64 respectively. The subsampling is per- { } formed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. There are totally 6n+2 stacked weighted layers. The following table summarizes the architecture: output map size 32\u00d732 16\u00d716 8\u00d78 # layers 1+2n 2n 2n # \ufb01lters 16 32 64 When shortcut connections are used, they are connected 3 layers (totally 3n shortcuts). On this to the pairs of 3 \u00d7 dataset we use identity shortcuts in all cases (i.e., option A), 7 method error (%) Maxout [10] 9.38 NIN [25] 8.81 DSN [24] 8.22 # layers # params FitNet [35] 19 2.5M 8.39 Highway [42, 43] 19 2.3M 7.54 (7.72\u00b10.16) Highway [42, 43] 32 1.25M 8.80 ResNet 20 0.27M 8.75 ResNet 32 0.46M 7.51 ResNet 44 0.66M 7.17 ResNet 56 0.85M 6.97 ResNet 110 1.7M 6.43 (6.61\u00b10.16) ResNet 1202 19.4M 7.93 Table 6. Classi\ufb01cation error on the CIFAR-10 test set. All meth- ods are with data augmentation. For ResNet-110, we run it 5 times and show \u201cbest (mean\u00b1std)\u201d as in [43]. so our residual models have exactly the same depth, width, and number of parameters as the plain counterparts. We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in [13] and BN [16] but with no dropout. These models are trained with a mini- batch size of 128 on two GPUs. We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split. We follow the simple data augmen- tation in [24] for training: 4 pixels are padded on each side, and a 32 32 crop is randomly sampled from the padded \u00d7 image or its horizontal \ufb02ip. For testing, we only evaluate the single view of the original 32 32 image. \u00d7 3,5,7,9 , leading to 20, 32, 44, and We compare n = } { 56-layer networks. Fig. 6 (left) shows the behaviors of the plain nets. The deep plain nets suffer from increased depth, and exhibit higher training error when going deeper. This phenomenon is similar to that on ImageNet (Fig. 4, left) and on MNIST (see [42]), suggesting that such an optimization dif\ufb01culty is a fundamental problem. Fig. 6 (middle) shows the behaviors of ResNets. Also similar to the ImageNet cases (Fig. 4, right), our ResNets manage to overcome the optimization dif\ufb01culty and demon- strate accuracy gains when the depth increases. We further explore n = 18 that leads to a 110-layer ResNet. In this case, we \ufb01nd that the initial learning rate of 0.1 is slightly too large to start converging5. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and con- tinue training. The rest of the learning schedule is as done previously. This 110-layer network converges well (Fig. 6, middle). It has fewer parameters than other deep and thin 5With an initial learning rate of 0.1, it starts converging (<90% error) after several epochs, but still reaches similar accuracy. S6-layer 20-layer T 2 3 \u201c+ iter. (Le) iter. (1e4) iter. (Let) Figure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error of plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers. F=-plain-20 F=-plain-20 plain-56 Net-20 I\u2014ResNet-56 I ResNet-110 0 20 layer index (sorted by magnitude) \" Figure 7. Standard deviations (std) of layer responses on CIFAR- 10. The responses are the outputs of each 3\u00d73 layer, after BN and before nonlinearity. Top: the layers are shown in their original order. Bottom: the responses are ranked in descending order. training data 07+12 07++12 test data VOC 07 test VOC 12 test VGG-16 73.2 70.4 ResNet-101 76.4 73.8 Table 7. Object detection mAP (%) on the PASCAL VOC 2007/2012 test sets using baseline Faster R-CNN. See also Ta- ble 10 and 11 for better results. metric mAP@.5 mAP@[.5, .95] VGG-16 41.5 21.2 ResNet-101 48.4 27.2 Table 8. Object detection mAP (%) on the COCO validation set using baseline Faster R-CNN. See also Table 9 for better results. networks such as FitNet [35] and Highway [42] (Table 6), yet is among the state-of-the-art results (6.43%, Table 6). Analysis of Layer Responses. Fig. 7 shows the standard deviations (std) of the layer responses. The responses are the outputs of each 3 3 layer, after BN and before other \u00d7 nonlinearity (ReLU/addition). For ResNets, this analy- sis reveals the response strength of the residual functions. Fig. 7 shows that ResNets have generally smaller responses than their plain counterparts. These results support our ba- sic motivation (Sec.3.1) that the residual functions might be generally closer to zero than the non-residual functions. We also notice that the deeper ResNet has smaller magni- tudes of responses, as evidenced by the comparisons among ResNet-20, 56, and 110 in Fig. 7. When there are more layers, an individual layer of ResNets tends to modify the signal less. Exploring Over 1000 layers. We explore an aggressively deep model of over 1000 layers. We set n = 200 that leads to a 1202-layer network, which is trained as described above. Our method shows no optimization dif\ufb01culty, and this 103-layer network is able to achieve training error <0.1% (Fig. 6, right). Its test error is still fairly good (7.93%, Table 6)."
  },
  {
    "chunk_id": "796cad07-187c-48cd-b1ef-c5ce948b7277",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 8,
    "retrieval_text": "But there are still open problems on such aggressively deep models. The testing result of this 1202-layer network is worse than that of our 110-layer network, although both have similar training error. We argue that this is because of over\ufb01tting. The 1202-layer network may be unnecessarily large (19.4M) for this small dataset. Strong regularization such as maxout [10] or dropout [14] is applied to obtain the best results ([10, 25, 24, 35]) on this dataset. In this paper, we use no maxout/dropout and just simply impose regular- ization via deep and thin architectures by design, without distracting from the focus on the dif\ufb01culties of optimiza- tion. But combining with stronger regularization may im- prove results, which we will study in the future. 4.3. Object Detection on PASCAL and MS COCO Our method has good generalization performance on other recognition tasks. Table 7 and 8 show the object de- tection baseline results on PASCAL VOC 2007 and 2012 [5] and COCO [26]. We adopt Faster R-CNN [32] as the de- tection method. Here we are interested in the improvements of replacing VGG-16 [41] with ResNet-101. The detection implementation (see appendix) of using both models is the same, so the gains can only be attributed to better networks. Most remarkably, on the challenging COCO dataset we ob- tain a 6.0% increase in COCO\u2019s standard metric (mAP@[.5, .95]), which is a 28% relative improvement. This gain is solely due to the learned representations. Based on deep residual nets, we won the 1st places in several tracks in ILSVRC & COCO 2015 competitions: Im- ageNet detection, ImageNet localization, COCO detection, and COCO segmentation. The details are in the appendix. 8 References [1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen- cies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994. [2] C. M. Bishop. Neural networks for pattern recognition. Oxford university press, 1995. [3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam, 2000. [4] K. Chat\ufb01eld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil is in the details: an evaluation of recent feature encoding methods. In BMVC, 2011. [5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis- serman. The Pascal Visual Object Classes (VOC) Challenge. IJCV, pages 303\u2013338, 2010. [6] S. Gidaris and N. Komodakis. Object detection via a multi-region & semantic segmentation-aware cnn model. In ICCV, 2015. [7] R. Girshick. Fast R-CNN. In ICCV, 2015. [8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier- archies for accurate object detection and semantic segmentation. In CVPR, 2014. [9] X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training deep feedforward neural networks. In AISTATS, 2010. [10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. arXiv:1302.4389, 2013. [11] K. He and J. Sun. Convolutional neural networks at constrained time cost. In CVPR, 2015. [12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. [13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into recti\ufb01ers: Surpassing human-level performance on imagenet classi\ufb01cation. In ICCV, 2015. [14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co- adaptation of feature detectors. arXiv:1207.0580, 2012. [15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. [16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. [17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. TPAMI, 33, 2011. [18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid. Aggregating local image descriptors into compact codes. TPAMI, 2012. [19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014. [20] A. Krizhevsky. Learning multiple layers of features from tiny im- ages. Tech Report, 2009. [21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation with deep convolutional neural networks. In NIPS, 2012. [22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to hand- written zip code recognition. Neural computation, 1989. [23] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00a8uller. Ef\ufb01cient backprop. In Neural Networks: Tricks of the Trade, pages 9\u201350. Springer, 1998. [24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply- supervised nets. arXiv:1409.5185, 2014. [25] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400, 2013. [26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV. 2014. [27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 9 [28] G. Mont\u00b4ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In NIPS, 2014. [29] V. Nair and G. E. Hinton. Recti\ufb01ed linear units improve restricted boltzmann machines. In ICML, 2010. [30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for image categorization. In CVPR, 2007. [31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by linear transformations in perceptrons. In AISTATS, 2012. [32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015. [33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. arXiv:1504.06066, 2015. [34] B. D. Ripley. Pattern recognition and neural networks. Cambridge university press, 1996. [35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015. [36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014. [37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120, 2013. [38] N. N. Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical report, 1998. [39] N. N. Schraudolph. Centering neural network gradient factors. In Neural Networks: Tricks of the Trade, pages 207\u2013226. Springer, 1998. [40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le- Cun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. [41] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. [42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv:1505.00387, 2015. [43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. 1507.06228, 2015. [44] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er- han, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu- tions. In CVPR, 2015. [45] R. Szeliski. Fast surface interpolation using hierarchical basis func- tions. TPAMI, 1990. [46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In SIGGRAPH, 2006. [47] T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochas- tic gradient towards second-order methods\u2013backpropagation learn- ing with transformations in nonlinearities. In Neural Information Processing, 2013.",
    "raw_text": "But there are still open problems on such aggressively deep models. The testing result of this 1202-layer network is worse than that of our 110-layer network, although both have similar training error. We argue that this is because of over\ufb01tting. The 1202-layer network may be unnecessarily large (19.4M) for this small dataset. Strong regularization such as maxout [10] or dropout [14] is applied to obtain the best results ([10, 25, 24, 35]) on this dataset. In this paper, we use no maxout/dropout and just simply impose regular- ization via deep and thin architectures by design, without distracting from the focus on the dif\ufb01culties of optimiza- tion. But combining with stronger regularization may im- prove results, which we will study in the future. 4.3. Object Detection on PASCAL and MS COCO Our method has good generalization performance on other recognition tasks. Table 7 and 8 show the object de- tection baseline results on PASCAL VOC 2007 and 2012 [5] and COCO [26]. We adopt Faster R-CNN [32] as the de- tection method. Here we are interested in the improvements of replacing VGG-16 [41] with ResNet-101. The detection implementation (see appendix) of using both models is the same, so the gains can only be attributed to better networks. Most remarkably, on the challenging COCO dataset we ob- tain a 6.0% increase in COCO\u2019s standard metric (mAP@[.5, .95]), which is a 28% relative improvement. This gain is solely due to the learned representations. Based on deep residual nets, we won the 1st places in several tracks in ILSVRC & COCO 2015 competitions: Im- ageNet detection, ImageNet localization, COCO detection, and COCO segmentation. The details are in the appendix. 8 References [1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen- cies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994. [2] C. M. Bishop. Neural networks for pattern recognition. Oxford university press, 1995. [3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam, 2000. [4] K. Chat\ufb01eld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil is in the details: an evaluation of recent feature encoding methods. In BMVC, 2011. [5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis- serman. The Pascal Visual Object Classes (VOC) Challenge. IJCV, pages 303\u2013338, 2010. [6] S. Gidaris and N. Komodakis. Object detection via a multi-region & semantic segmentation-aware cnn model. In ICCV, 2015. [7] R. Girshick. Fast R-CNN. In ICCV, 2015. [8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier- archies for accurate object detection and semantic segmentation. In CVPR, 2014. [9] X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training deep feedforward neural networks. In AISTATS, 2010. [10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. arXiv:1302.4389, 2013. [11] K. He and J. Sun. Convolutional neural networks at constrained time cost. In CVPR, 2015. [12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. [13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into recti\ufb01ers: Surpassing human-level performance on imagenet classi\ufb01cation. In ICCV, 2015. [14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co- adaptation of feature detectors. arXiv:1207.0580, 2012. [15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. [16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. [17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. TPAMI, 33, 2011. [18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid. Aggregating local image descriptors into compact codes. TPAMI, 2012. [19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014. [20] A. Krizhevsky. Learning multiple layers of features from tiny im- ages. Tech Report, 2009. [21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation with deep convolutional neural networks. In NIPS, 2012. [22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to hand- written zip code recognition. Neural computation, 1989. [23] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00a8uller. Ef\ufb01cient backprop. In Neural Networks: Tricks of the Trade, pages 9\u201350. Springer, 1998. [24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply- supervised nets. arXiv:1409.5185, 2014. [25] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400, 2013. [26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV. 2014. [27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 9 [28] G. Mont\u00b4ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In NIPS, 2014. [29] V. Nair and G. E. Hinton. Recti\ufb01ed linear units improve restricted boltzmann machines. In ICML, 2010. [30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for image categorization. In CVPR, 2007. [31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by linear transformations in perceptrons. In AISTATS, 2012. [32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015. [33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. arXiv:1504.06066, 2015. [34] B. D. Ripley. Pattern recognition and neural networks. Cambridge university press, 1996. [35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015. [36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014. [37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120, 2013. [38] N. N. Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical report, 1998. [39] N. N. Schraudolph. Centering neural network gradient factors. In Neural Networks: Tricks of the Trade, pages 207\u2013226. Springer, 1998. [40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le- Cun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. [41] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. [42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv:1505.00387, 2015. [43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. 1507.06228, 2015. [44] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er- han, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu- tions. In CVPR, 2015. [45] R. Szeliski. Fast surface interpolation using hierarchical basis func- tions. TPAMI, 1990. [46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In SIGGRAPH, 2006. [47] T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochas- tic gradient towards second-order methods\u2013backpropagation learn- ing with transformations in nonlinearities. In Neural Information Processing, 2013."
  },
  {
    "chunk_id": "b7930999-edb1-4586-a7c8-c44bf43b372c",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 9,
    "retrieval_text": "[48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms, 2008. [49] W. Venables and B. Ripley. Modern applied statistics with s-plus. 1999. [50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu- tional neural networks. In ECCV, 2014. A. Object Detection Baselines In this section we introduce our detection method based on the baseline Faster R-CNN [32] system. The models are initialized by the ImageNet classi\ufb01cation models, and then \ufb01ne-tuned on the object detection data. We have experi- mented with ResNet-50/101 at the time of the ILSVRC & COCO 2015 detection competitions. Unlike VGG-16 used in [32], our ResNet has no hidden fc layers. We adopt the idea of \u201cNetworks on Conv fea- ture maps\u201d (NoC) [33] to address this issue. We compute the full-image shared conv feature maps using those lay- ers whose strides on the image are no greater than 16 pixels (i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv layers in ResNet-101; Table 1). We consider these layers as analogous to the 13 conv layers in VGG-16, and by doing so, both ResNet and VGG-16 have conv feature maps of the same total stride (16 pixels). These layers are shared by a region proposal network (RPN, generating 300 proposals) [32] and a Fast R-CNN detection network [7]. RoI pool- ing [7] is performed before conv5 1. On this RoI-pooled feature, all layers of conv5 x and up are adopted for each region, playing the roles of VGG-16\u2019s fc layers. The \ufb01nal classi\ufb01cation layer is replaced by two sibling layers (classi- \ufb01cation and box regression [7]). For the usage of BN layers, after pre-training, we com- pute the BN statistics (means and variances) for each layer on the ImageNet training set. Then the BN layers are \ufb01xed during \ufb01ne-tuning for object detection. As such, the BN layers become linear activations with constant offsets and scales, and BN statistics are not updated by \ufb01ne-tuning. We \ufb01x the BN layers mainly for reducing memory consumption in Faster R-CNN training.",
    "raw_text": "[48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms, 2008. [49] W. Venables and B. Ripley. Modern applied statistics with s-plus. 1999. [50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu- tional neural networks. In ECCV, 2014. A. Object Detection Baselines In this section we introduce our detection method based on the baseline Faster R-CNN [32] system. The models are initialized by the ImageNet classi\ufb01cation models, and then \ufb01ne-tuned on the object detection data. We have experi- mented with ResNet-50/101 at the time of the ILSVRC & COCO 2015 detection competitions. Unlike VGG-16 used in [32], our ResNet has no hidden fc layers. We adopt the idea of \u201cNetworks on Conv fea- ture maps\u201d (NoC) [33] to address this issue. We compute the full-image shared conv feature maps using those lay- ers whose strides on the image are no greater than 16 pixels (i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv layers in ResNet-101; Table 1). We consider these layers as analogous to the 13 conv layers in VGG-16, and by doing so, both ResNet and VGG-16 have conv feature maps of the same total stride (16 pixels). These layers are shared by a region proposal network (RPN, generating 300 proposals) [32] and a Fast R-CNN detection network [7]. RoI pool- ing [7] is performed before conv5 1. On this RoI-pooled feature, all layers of conv5 x and up are adopted for each region, playing the roles of VGG-16\u2019s fc layers. The \ufb01nal classi\ufb01cation layer is replaced by two sibling layers (classi- \ufb01cation and box regression [7]). For the usage of BN layers, after pre-training, we com- pute the BN statistics (means and variances) for each layer on the ImageNet training set. Then the BN layers are \ufb01xed during \ufb01ne-tuning for object detection. As such, the BN layers become linear activations with constant offsets and scales, and BN statistics are not updated by \ufb01ne-tuning. We \ufb01x the BN layers mainly for reducing memory consumption in Faster R-CNN training."
  },
  {
    "chunk_id": "3b9be6ed-f94f-4f75-8cdb-5fc18523737d",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 10,
    "retrieval_text": "PASCAL VOC Following [7, 32], for the PASCAL VOC 2007 test set, we use the 5k trainval images in VOC 2007 and 16k train- val images in VOC 2012 for training (\u201c07+12\u201d). For the PASCAL VOC 2012 test set, we use the 10k trainval+test images in VOC 2007 and 16k trainval images in VOC 2012 for training (\u201c07++12\u201d). The hyper-parameters for train- ing Faster R-CNN are the same as in [32]. Table 7 shows the results. ResNet-101 improves the mAP by >3% over VGG-16. This gain is solely because of the improved fea- tures learned by ResNet. MS COCO The MS COCO dataset [26] involves 80 object cate- gories. We evaluate the PASCAL VOC metric (mAP @ IoU = 0.5) and the standard COCO metric (mAP @ IoU = .5:.05:.95). We use the 80k images on the train set for train- ing and the 40k images on the val set for evaluation. Our detection system for COCO is similar to that for PASCAL VOC. We train the COCO models with an 8-GPU imple- mentation, and thus the RPN step has a mini-batch size of 10 8 images (i.e., 1 per GPU) and the Fast R-CNN step has a mini-batch size of 16 images. The RPN step and Fast R- CNN step are both trained for 240k iterations with a learn- ing rate of 0.001 and then for 80k iterations with 0.0001. Table 8 shows the results on the MS COCO validation set. ResNet-101 has a 6% increase of mAP@[.5, .95] over VGG-16, which is a 28% relative improvement, solely con- tributed by the features learned by the better network. Re- markably, the mAP@[.5, .95]\u2019s absolute increase (6.0%) is nearly as big as mAP@.5\u2019s (6.9%). This suggests that a deeper network can improve both recognition and localiza- tion. B. Object Detection Improvements For completeness, we report the improvements made for the competitions. These improvements are based on deep features and thus should bene\ufb01t from residual learning. MS COCO Box re\ufb01nement. Our box re\ufb01nement partially follows the it- erative localization in [6]. In Faster R-CNN, the \ufb01nal output is a regressed box that is different from its proposal box. So for inference, we pool a new feature from the regressed box and obtain a new classi\ufb01cation score and a new regressed box. We combine these 300 new predictions with the orig- inal 300 predictions. Non-maximum suppression (NMS) is applied on the union set of predicted boxes using an IoU threshold of 0.3 [8], followed by box voting [6]. Box re- \ufb01nement improves mAP by about 2 points (Table 9). Global context. We combine global context in the Fast R-CNN step. Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a \u201csingle-level\u201d pyramid) which can be implemented as \u201cRoI\u201d pooling using the entire image\u2019s bounding box as the RoI. This pooled feature is fed into the post-RoI layers to obtain a global context feature. This global feature is con- catenated with the original per-region feature, followed by the sibling classi\ufb01cation and box regression layers. This new structure is trained end-to-end. Global context im- proves mAP@.5 by about 1 point (Table 9). Multi-scale testing. In the above, all results are obtained by single-scale training/testing as in [32], where the image\u2019s shorter side is s = 600 pixels. Multi-scale training/testing has been developed in [12, 7] by selecting a scale from a feature pyramid, and in [33] by using maxout layers. our current implementation, we have performed multi-scale testing following [33]; we have not performed multi-scale training because of limited time. In addition, we have per- formed multi-scale testing only for the Fast R-CNN step (but not yet for the RPN step). With a trained model, we compute conv feature maps on an image pyramid, where the image\u2019s shorter sides are s 200,400,600,800,1000 In . \u2208 { } training data COCO train COCO trainval test data COCO val COCO test-dev mAP @.5 @[.5, .95] @.5 @[.5, .95] baseline Faster R-CNN (VGG-16) 41.5 21.2 baseline Faster R-CNN (ResNet-101) 48.4 27.2 +box re\ufb01nement 49.9 29.9 +context 51.1 30.0 53.3 32.2 +multi-scale testing 53.8 32.5 55.7 34.9 ensemble 59.0 37.4 Table 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101. system net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train baseline VGG-16 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 baseline ResNet-101 07+12 76.4 79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0 baseline+++ ResNet-101 COCO+07+12 85.6 90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8 tv Table 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d include box re\ufb01nement, context, and multi-scale testing in Table 9. system net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train baseline VGG-16 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 baseline ResNet-101 07++12 73.8 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6 baseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0 Table 11. Detection results on the PASCAL VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/ displaylb.php?challengeid=11&compid=4). The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d include box re\ufb01nement, context, and multi-scale testing in Table 9. tv We select two adjacent scales from the pyramid following [33]. RoI pooling and subsequent layers are performed on the feature maps of these two scales [33], which are merged by maxout as in [33]. Multi-scale testing improves the mAP by over 2 points (Table 9). val2 test GoogLeNet [44] (ILSVRC\u201914) - 43.9 our single model (ILSVRC\u201915) 60.5 58.8 our ensemble (ILSVRC\u201915) 63.6 62.1 Using validation data. Next we use the 80k+40k trainval set for training and the 20k test-dev set for evaluation. The test- dev set has no publicly available ground truth and the result is reported by the evaluation server. Under this setting, the results are an mAP@.5 of 55.7% and an mAP@[.5, .95] of 34.9% (Table 9). This is our single-model result. Ensemble. In Faster R-CNN, the system is designed to learn region proposals and also object classi\ufb01ers, so an ensemble can be used to boost both tasks. We use an ensemble for proposing regions, and the union set of proposals are pro- cessed by an ensemble of per-region classi\ufb01ers. Table 9 shows our result based on an ensemble of 3 networks. The mAP is 59.0% and 37.4% on the test-dev set. This result won the 1st place in the detection task in COCO 2015.",
    "raw_text": "PASCAL VOC Following [7, 32], for the PASCAL VOC 2007 test set, we use the 5k trainval images in VOC 2007 and 16k train- val images in VOC 2012 for training (\u201c07+12\u201d). For the PASCAL VOC 2012 test set, we use the 10k trainval+test images in VOC 2007 and 16k trainval images in VOC 2012 for training (\u201c07++12\u201d). The hyper-parameters for train- ing Faster R-CNN are the same as in [32]. Table 7 shows the results. ResNet-101 improves the mAP by >3% over VGG-16. This gain is solely because of the improved fea- tures learned by ResNet. MS COCO The MS COCO dataset [26] involves 80 object cate- gories. We evaluate the PASCAL VOC metric (mAP @ IoU = 0.5) and the standard COCO metric (mAP @ IoU = .5:.05:.95). We use the 80k images on the train set for train- ing and the 40k images on the val set for evaluation. Our detection system for COCO is similar to that for PASCAL VOC. We train the COCO models with an 8-GPU imple- mentation, and thus the RPN step has a mini-batch size of 10 8 images (i.e., 1 per GPU) and the Fast R-CNN step has a mini-batch size of 16 images. The RPN step and Fast R- CNN step are both trained for 240k iterations with a learn- ing rate of 0.001 and then for 80k iterations with 0.0001. Table 8 shows the results on the MS COCO validation set. ResNet-101 has a 6% increase of mAP@[.5, .95] over VGG-16, which is a 28% relative improvement, solely con- tributed by the features learned by the better network. Re- markably, the mAP@[.5, .95]\u2019s absolute increase (6.0%) is nearly as big as mAP@.5\u2019s (6.9%). This suggests that a deeper network can improve both recognition and localiza- tion. B. Object Detection Improvements For completeness, we report the improvements made for the competitions. These improvements are based on deep features and thus should bene\ufb01t from residual learning. MS COCO Box re\ufb01nement. Our box re\ufb01nement partially follows the it- erative localization in [6]. In Faster R-CNN, the \ufb01nal output is a regressed box that is different from its proposal box. So for inference, we pool a new feature from the regressed box and obtain a new classi\ufb01cation score and a new regressed box. We combine these 300 new predictions with the orig- inal 300 predictions. Non-maximum suppression (NMS) is applied on the union set of predicted boxes using an IoU threshold of 0.3 [8], followed by box voting [6]. Box re- \ufb01nement improves mAP by about 2 points (Table 9). Global context. We combine global context in the Fast R-CNN step. Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a \u201csingle-level\u201d pyramid) which can be implemented as \u201cRoI\u201d pooling using the entire image\u2019s bounding box as the RoI. This pooled feature is fed into the post-RoI layers to obtain a global context feature. This global feature is con- catenated with the original per-region feature, followed by the sibling classi\ufb01cation and box regression layers. This new structure is trained end-to-end. Global context im- proves mAP@.5 by about 1 point (Table 9). Multi-scale testing. In the above, all results are obtained by single-scale training/testing as in [32], where the image\u2019s shorter side is s = 600 pixels. Multi-scale training/testing has been developed in [12, 7] by selecting a scale from a feature pyramid, and in [33] by using maxout layers. our current implementation, we have performed multi-scale testing following [33]; we have not performed multi-scale training because of limited time. In addition, we have per- formed multi-scale testing only for the Fast R-CNN step (but not yet for the RPN step). With a trained model, we compute conv feature maps on an image pyramid, where the image\u2019s shorter sides are s 200,400,600,800,1000 In . \u2208 { } training data COCO train COCO trainval test data COCO val COCO test-dev mAP @.5 @[.5, .95] @.5 @[.5, .95] baseline Faster R-CNN (VGG-16) 41.5 21.2 baseline Faster R-CNN (ResNet-101) 48.4 27.2 +box re\ufb01nement 49.9 29.9 +context 51.1 30.0 53.3 32.2 +multi-scale testing 53.8 32.5 55.7 34.9 ensemble 59.0 37.4 Table 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101. system net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train baseline VGG-16 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 baseline ResNet-101 07+12 76.4 79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0 baseline+++ ResNet-101 COCO+07+12 85.6 90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8 tv Table 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d include box re\ufb01nement, context, and multi-scale testing in Table 9. system net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train baseline VGG-16 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 baseline ResNet-101 07++12 73.8 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6 baseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0 Table 11. Detection results on the PASCAL VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/ displaylb.php?challengeid=11&compid=4). The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d include box re\ufb01nement, context, and multi-scale testing in Table 9. tv We select two adjacent scales from the pyramid following [33]. RoI pooling and subsequent layers are performed on the feature maps of these two scales [33], which are merged by maxout as in [33]. Multi-scale testing improves the mAP by over 2 points (Table 9). val2 test GoogLeNet [44] (ILSVRC\u201914) - 43.9 our single model (ILSVRC\u201915) 60.5 58.8 our ensemble (ILSVRC\u201915) 63.6 62.1 Using validation data. Next we use the 80k+40k trainval set for training and the 20k test-dev set for evaluation. The test- dev set has no publicly available ground truth and the result is reported by the evaluation server. Under this setting, the results are an mAP@.5 of 55.7% and an mAP@[.5, .95] of 34.9% (Table 9). This is our single-model result. Ensemble. In Faster R-CNN, the system is designed to learn region proposals and also object classi\ufb01ers, so an ensemble can be used to boost both tasks. We use an ensemble for proposing regions, and the union set of proposals are pro- cessed by an ensemble of per-region classi\ufb01ers. Table 9 shows our result based on an ensemble of 3 networks. The mAP is 59.0% and 37.4% on the test-dev set. This result won the 1st place in the detection task in COCO 2015."
  },
  {
    "chunk_id": "e60f00c0-8243-426c-ba00-b0fa11a6019c",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 11,
    "retrieval_text": "PASCAL VOC We revisit the PASCAL VOC dataset based on the above model. With the single model on the COCO dataset (55.7% mAP@.5 in Table 9), we \ufb01ne-tune this model on the PAS- CAL VOC sets. The improvements of box re\ufb01nement, con- text, and multi-scale testing are also adopted. By doing so Table 12. Our results (mAP, %) on the ImageNet detection dataset. Our detection system is Faster R-CNN [32] with the improvements in Table 9, using ResNet-101. we achieve 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11)6. The result on PASCAL VOC 2012 is 10 points higher than the previ- ous state-of-the-art result [6]. ImageNet Detection The ImageNet Detection (DET) task involves 200 object categories. The accuracy is evaluated by mAP@.5. Our object detection algorithm for ImageNet DET is the same as that for MS COCO in Table 9. The networks are pre- trained on the 1000-class ImageNet classi\ufb01cation set, and are \ufb01ne-tuned on the DET data. We split the validation set into two parts (val1/val2) following [8]. We \ufb01ne-tune the detection models using the DET training set and the val1 set. The val2 set is used for validation. We do not use other ILSVRC 2015 data. Our single model with ResNet-101 has 6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html, submitted on 2015-11-26. 11 LOC method LOC network testing LOC error on GT CLS classi\ufb01cation network top-5 LOC error on predicted CLS VGG\u2019s [41] VGG-16 1-crop 33.1 [41] RPN ResNet-101 1-crop 13.3 RPN ResNet-101 dense 11.7 RPN ResNet-101 dense ResNet-101 14.4 RPN+RCNN ResNet-101 dense ResNet-101 10.6 RPN+RCNN ensemble dense ensemble 8.9 Table 13. Localization error (%) on the ImageNet validation. In the column of \u201cLOC error on GT class\u201d ([41]), the ground truth class is used. In the \u201ctesting\u201d column, \u201c1-crop\u201d denotes testing on a center crop of 224\u00d7224 pixels, \u201cdense\u201d denotes dense (fully convolutional) and multi-scale testing. 58.8% mAP and our ensemble of 3 models has 62.1% mAP on the DET test set (Table 12). This result won the 1st place in the ImageNet detection task in ILSVRC 2015, surpassing the second place by 8.5 points (absolute).",
    "raw_text": "PASCAL VOC We revisit the PASCAL VOC dataset based on the above model. With the single model on the COCO dataset (55.7% mAP@.5 in Table 9), we \ufb01ne-tune this model on the PAS- CAL VOC sets. The improvements of box re\ufb01nement, con- text, and multi-scale testing are also adopted. By doing so Table 12. Our results (mAP, %) on the ImageNet detection dataset. Our detection system is Faster R-CNN [32] with the improvements in Table 9, using ResNet-101. we achieve 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11)6. The result on PASCAL VOC 2012 is 10 points higher than the previ- ous state-of-the-art result [6]. ImageNet Detection The ImageNet Detection (DET) task involves 200 object categories. The accuracy is evaluated by mAP@.5. Our object detection algorithm for ImageNet DET is the same as that for MS COCO in Table 9. The networks are pre- trained on the 1000-class ImageNet classi\ufb01cation set, and are \ufb01ne-tuned on the DET data. We split the validation set into two parts (val1/val2) following [8]. We \ufb01ne-tune the detection models using the DET training set and the val1 set. The val2 set is used for validation. We do not use other ILSVRC 2015 data. Our single model with ResNet-101 has 6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html, submitted on 2015-11-26. 11 LOC method LOC network testing LOC error on GT CLS classi\ufb01cation network top-5 LOC error on predicted CLS VGG\u2019s [41] VGG-16 1-crop 33.1 [41] RPN ResNet-101 1-crop 13.3 RPN ResNet-101 dense 11.7 RPN ResNet-101 dense ResNet-101 14.4 RPN+RCNN ResNet-101 dense ResNet-101 10.6 RPN+RCNN ensemble dense ensemble 8.9 Table 13. Localization error (%) on the ImageNet validation. In the column of \u201cLOC error on GT class\u201d ([41]), the ground truth class is used. In the \u201ctesting\u201d column, \u201c1-crop\u201d denotes testing on a center crop of 224\u00d7224 pixels, \u201cdense\u201d denotes dense (fully convolutional) and multi-scale testing. 58.8% mAP and our ensemble of 3 models has 62.1% mAP on the DET test set (Table 12). This result won the 1st place in the ImageNet detection task in ILSVRC 2015, surpassing the second place by 8.5 points (absolute)."
  },
  {
    "chunk_id": "03ed308f-50ba-4f9e-b49b-04ed1a1ba0bb",
    "modality": "text",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": 12,
    "retrieval_text": "C. ImageNet Localization The ImageNet Localization (LOC) task [36] requires to classify and localize the objects. Following [40, 41], we assume that the image-level classi\ufb01ers are \ufb01rst adopted for predicting the class labels of an image, and the localiza- tion algorithm only accounts for predicting bounding boxes based on the predicted classes. We adopt the \u201cper-class re- gression\u201d (PCR) strategy [40, 41], learning a bounding box regressor for each class. We pre-train the networks for Im- ageNet classi\ufb01cation and then \ufb01ne-tune them for localiza- tion. We train networks on the provided 1000-class Ima- geNet training set. Our localization algorithm is based on the RPN frame- work of [32] with a few modi\ufb01cations. Unlike the way in [32] that is category-agnostic, our RPN for localization is designed in a per-class form. This RPN ends with two sib- ling 1 1 convolutional layers for binary classi\ufb01cation (cls) \u00d7 and box regression (reg), as in [32]. The cls and reg layers are both in a per-class from, in contrast to [32]. Speci\ufb01- cally, the cls layer has a 1000-d output, and each dimension is binary logistic regression for predicting being or not be- ing an object class; the reg layer has a 1000 4-d output \u00d7 consisting of box regressors for 1000 classes. As in [32], our bounding box regression is with reference to multiple translation-invariant \u201canchor\u201d boxes at each position. As in our ImageNet classi\ufb01cation training (Sec. 3.4), we randomly sample 224 224 crops for data augmentation. \u00d7 We use a mini-batch size of 256 images for \ufb01ne-tuning. To avoid negative samples being dominate, 8 anchors are ran- domly sampled for each image, where the sampled positive and negative anchors have a ratio of 1:1 [32]. For testing, the network is applied on the image fully-convolutionally. Table 13 compares the localization results. Following [41], we \ufb01rst perform \u201coracle\u201d testing using the ground truth class as the classi\ufb01cation prediction. VGG\u2019s paper [41] re- 12 method top-5 localization err val test OverFeat [40] (ILSVRC\u201913) 30.0 29.9 GoogLeNet [44] (ILSVRC\u201914) - 26.7 VGG [41] (ILSVRC\u201914) 26.9 25.3 ours (ILSVRC\u201915) 8.9 9.0 Table 14. Comparisons of localization error (%) on the ImageNet dataset with state-of-the-art methods. ports a center-crop error of 33.1% (Table 13) using ground truth classes. Under the same setting, our RPN method us- ing ResNet-101 net signi\ufb01cantly reduces the center-crop er- ror to 13.3%. This comparison demonstrates the excellent performance of our framework. With dense (fully convolu- tional) and multi-scale testing, our ResNet-101 has an error of 11.7% using ground truth classes. Using ResNet-101 for predicting classes (4.6% top-5 classi\ufb01cation error, Table 4), the top-5 localization error is 14.4%. The above results are only based on the proposal network (RPN) in Faster R-CNN [32]. One may use the detection network (Fast R-CNN [7]) in Faster R-CNN to improve the results. But we notice that on this dataset, one image usually contains a single dominate object, and the proposal regions highly overlap with each other and thus have very similar RoI-pooled features. As a result, the image-centric training of Fast R-CNN [7] generates samples of small variations, which may not be desired for stochastic training. Motivated by this, in our current experiment we use the original R- CNN [8] that is RoI-centric, in place of Fast R-CNN. Our R-CNN implementation is as follows. We apply the per-class RPN trained as above on the training images to predict bounding boxes for the ground truth class. These predicted boxes play a role of class-dependent proposals. For each training image, the highest scored 200 proposals are extracted as training samples to train an R-CNN classi- \ufb01er. The image region is cropped from a proposal, warped to 224 224 pixels, and fed into the classi\ufb01cation network \u00d7 as in R-CNN [8]. The outputs of this network consist of two sibling fc layers for cls and reg, also in a per-class form. This R-CNN network is \ufb01ne-tuned on the training set us- ing a mini-batch size of 256 in the RoI-centric fashion. For testing, the RPN generates the highest scored 200 proposals for each predicted class, and the R-CNN network is used to update these proposals\u2019 scores and box positions. This method reduces the top-5 localization error to 10.6% (Table 13). This is our single-model result on the validation set. Using an ensemble of networks for both clas- si\ufb01cation and localization, we achieve a top-5 localization error of 9.0% on the test set. This number signi\ufb01cantly out- performs the ILSVRC 14 results (Table 14), showing a 64% relative reduction of error. This result won the 1st place in the ImageNet localization task in ILSVRC 2015.",
    "raw_text": "C. ImageNet Localization The ImageNet Localization (LOC) task [36] requires to classify and localize the objects. Following [40, 41], we assume that the image-level classi\ufb01ers are \ufb01rst adopted for predicting the class labels of an image, and the localiza- tion algorithm only accounts for predicting bounding boxes based on the predicted classes. We adopt the \u201cper-class re- gression\u201d (PCR) strategy [40, 41], learning a bounding box regressor for each class. We pre-train the networks for Im- ageNet classi\ufb01cation and then \ufb01ne-tune them for localiza- tion. We train networks on the provided 1000-class Ima- geNet training set. Our localization algorithm is based on the RPN frame- work of [32] with a few modi\ufb01cations. Unlike the way in [32] that is category-agnostic, our RPN for localization is designed in a per-class form. This RPN ends with two sib- ling 1 1 convolutional layers for binary classi\ufb01cation (cls) \u00d7 and box regression (reg), as in [32]. The cls and reg layers are both in a per-class from, in contrast to [32]. Speci\ufb01- cally, the cls layer has a 1000-d output, and each dimension is binary logistic regression for predicting being or not be- ing an object class; the reg layer has a 1000 4-d output \u00d7 consisting of box regressors for 1000 classes. As in [32], our bounding box regression is with reference to multiple translation-invariant \u201canchor\u201d boxes at each position. As in our ImageNet classi\ufb01cation training (Sec. 3.4), we randomly sample 224 224 crops for data augmentation. \u00d7 We use a mini-batch size of 256 images for \ufb01ne-tuning. To avoid negative samples being dominate, 8 anchors are ran- domly sampled for each image, where the sampled positive and negative anchors have a ratio of 1:1 [32]. For testing, the network is applied on the image fully-convolutionally. Table 13 compares the localization results. Following [41], we \ufb01rst perform \u201coracle\u201d testing using the ground truth class as the classi\ufb01cation prediction. VGG\u2019s paper [41] re- 12 method top-5 localization err val test OverFeat [40] (ILSVRC\u201913) 30.0 29.9 GoogLeNet [44] (ILSVRC\u201914) - 26.7 VGG [41] (ILSVRC\u201914) 26.9 25.3 ours (ILSVRC\u201915) 8.9 9.0 Table 14. Comparisons of localization error (%) on the ImageNet dataset with state-of-the-art methods. ports a center-crop error of 33.1% (Table 13) using ground truth classes. Under the same setting, our RPN method us- ing ResNet-101 net signi\ufb01cantly reduces the center-crop er- ror to 13.3%. This comparison demonstrates the excellent performance of our framework. With dense (fully convolu- tional) and multi-scale testing, our ResNet-101 has an error of 11.7% using ground truth classes. Using ResNet-101 for predicting classes (4.6% top-5 classi\ufb01cation error, Table 4), the top-5 localization error is 14.4%. The above results are only based on the proposal network (RPN) in Faster R-CNN [32]. One may use the detection network (Fast R-CNN [7]) in Faster R-CNN to improve the results. But we notice that on this dataset, one image usually contains a single dominate object, and the proposal regions highly overlap with each other and thus have very similar RoI-pooled features. As a result, the image-centric training of Fast R-CNN [7] generates samples of small variations, which may not be desired for stochastic training. Motivated by this, in our current experiment we use the original R- CNN [8] that is RoI-centric, in place of Fast R-CNN. Our R-CNN implementation is as follows. We apply the per-class RPN trained as above on the training images to predict bounding boxes for the ground truth class. These predicted boxes play a role of class-dependent proposals. For each training image, the highest scored 200 proposals are extracted as training samples to train an R-CNN classi- \ufb01er. The image region is cropped from a proposal, warped to 224 224 pixels, and fed into the classi\ufb01cation network \u00d7 as in R-CNN [8]. The outputs of this network consist of two sibling fc layers for cls and reg, also in a per-class form. This R-CNN network is \ufb01ne-tuned on the training set us- ing a mini-batch size of 256 in the RoI-centric fashion. For testing, the RPN generates the highest scored 200 proposals for each predicted class, and the R-CNN network is used to update these proposals\u2019 scores and box positions. This method reduces the top-5 localization error to 10.6% (Table 13). This is our single-model result on the validation set. Using an ensemble of networks for both clas- si\ufb01cation and localization, we achieve a top-5 localization error of 9.0% on the test set. This number signi\ufb01cantly out- performs the ILSVRC 14 results (Table 14), showing a 64% relative reduction of error. This result won the 1st place in the ImageNet localization task in ILSVRC 2015."
  },
  {
    "chunk_id": "460a973a-8ddf-43e8-88b6-1d6f1ba01b21",
    "modality": "image",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of three separate plots, each representing different data sets, displayed side by side against a white background. The style of the image suggests it may be from a scientific or technical presentation, possibly related to engineering or physics.\n\nOn the left, there is a line graph with two lines: one in red and one in black. Both lines represent some form of measurement over time, as indicated by the horizontal axis labeled \"Time (s)\" which ranges from 0 to 10 seconds. The vertical axis on the left side of this graph is labeled \"Layer (m)\", ranging from 0 to 20 meters.\n\nThe middle plot is a bar chart with three bars, each representing a different layer: \"5-layer\", \"6-layer\", and \"7-layer\". Each bar has a height that corresponds to the value on the vertical axis, which ranges from 0 to 100. The horizontal axis of this graph is labeled \"Time (s)\" and also spans from 0 to 10 seconds.\n\nThe right plot is another line graph with two lines: one in red and one in black. This graph has a vertical axis on the left side, labeled \"Temperature (\u00b0C)\", ranging from -50 to 20 degrees Celsius. The horizontal axis of this graph is also labeled \"Time (s)\" and ranges from 0 to 10 seconds.\n\nThe image does not contain any text or additional annotations that provide further context about the data being presented. The specific nature of the data, such as what the layers represent in the middle plot or the significance of the temperature measurements in the right plot, is not explained within the image itself. ",
    "raw_text": null
  },
  {
    "chunk_id": "67dc946e-5ab4-4a82-93ae-732063a31c47",
    "modality": "image",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": null,
    "retrieval_text": " The image is a grayscale photograph of a hand-drawn diagram with mathematical and conceptual elements. At the top left corner, there's a label \"weight layer\" connected to a box labeled \"relu,\" which stands for Rectified Linear Unit, a common activation function in neural networks. Below this, another label \"weight layer\" is connected to a box labeled \"relu.\" To the right of these two boxes, there's a third box labeled \"identity,\" indicating that the output from the second weight layer is not processed by an activation function and is passed directly through.\n\nThe diagram also includes mathematical equations and symbols. There are three equations:\n\n1. The first equation on the left side of the image shows a summation symbol (\u2211) followed by \"weight layer\" and then \"relu.\" This suggests that the weight layer is being processed through a rectified linear unit activation function.\n2. The second equation in the middle of the image reads \"x,\" which could represent an input or a variable, followed by \"relu\" and then \"weight layer.\" This implies that the output from the first rectified linear unit activation function is passed to another weight layer for further processing.\n3. The third equation on the right side of the image shows \"x\" again, followed by \"relu,\" and then \"weight layer.\" This indicates a similar process as in the second equation, where the output from the first rectified linear unit activation function is passed through another weight layer without being processed by an activation function.\n4. The fourth equation at the bottom right corner of the image shows \"x\" followed by \"relu,\" and then \"weight layer.\" This suggests a final processing step for the input \"x\" through a rectified linear unit activation function and then through a weight layer, which is likely the output layer in this context.\n\nThe diagram appears to be illustrating a simplified neural network architecture with multiple layers of weighted connections and rectified linear unit activation functions. The use of mathematical notation and the visual representation of the neural network suggest that this image may be used for educational purposes or as part of a technical explanation related to machine learning, specifically focusing on neural networks. ",
    "raw_text": null
  },
  {
    "chunk_id": "fe98c051-787c-4604-9f98-f0189d777068",
    "modality": "image",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of two separate photographs placed side by side. On the left side, there is a photograph of a graph with multiple lines representing different data sets. Each line has a label at the top right corner indicating the name of the dataset it represents. The graph shows a series of peaks and valleys, suggesting fluctuations in the values being measured over time.\n\nOn the right side, there is a photograph of a table with columns and rows. The table appears to be a data table with numerical values in each cell, which are likely related to the data represented by the graph on the left. The numbers in the cells are too small to read clearly due to the resolution of the image.\n\nThe style of the image is informational, typically used for scientific or technical presentations to illustrate data trends and correlations between different variables. ",
    "raw_text": null
  },
  {
    "chunk_id": "b2ba4aba-f453-4487-be81-e8ebfefcff4c",
    "modality": "image",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": null,
    "retrieval_text": " The image displays two flowcharts with a series of decision points and numerical values. Each flowchart has a starting point labeled \"Start\" and an ending point labeled \"End.\"\n\nThe left flowchart begins at \"Start,\" proceeds through a series of decisions, and ends at \"End.\" Along the way, it includes decision points such as \"If 1 then 2,\" \"If 3 then 4,\" and \"If 5 then 6.\" Each decision point is connected to one or more numerical values, which are represented by boxes with numbers inside them. The numbers in the left flowchart are 7, 8, 9, 10, 11, 12, 13, 14, and 15.\n\nThe right flowchart also starts at \"Start\" and ends at \"End,\" with decision points such as \"If 1 then 7,\" \"If 2 then 8,\" \"If 3 then 9,\" \"If 4 then 10,\" \"If 5 then 11,\" \"If 6 then 12,\" \"If 7 then 13,\" and \"If 8 then 14.\" The numbers in the right flowchart are 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19.\n\nThe style of the image is a simple line drawing with boxes representing decision points and numerical values. The flowcharts are presented in a way that suggests they represent a sequence of operations or steps, where each decision point leads to one of several possible outcomes represented by the numbers. There are no texts other than the labels on the decision points and the numerical values. ",
    "raw_text": null
  },
  {
    "chunk_id": "fb757bcf-62a4-48d6-bac4-199d4e5cf817",
    "modality": "image",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": null,
    "retrieval_text": " The image appears to be a screenshot of a computer screen displaying a series of line graphs with various data sets plotted against time. Each graph has multiple lines representing different data points, which are color-coded and labeled with what seems to be numerical values along the y-axis. The x-axis represents time, as indicated by the labels at the bottom of each graph.\n\nThe topmost graph shows a line that is relatively flat, with only minor fluctuations over time. Below it, there are two graphs with more pronounced peaks and valleys, suggesting they represent data sets with greater variability. The third graph from the top has a series of sharp spikes, indicating rapid changes in the data being plotted.\n\nThe bottommost graph is partially obscured by the others, but it seems to have a similar structure with multiple lines representing different data sets. The colors and labels for this graph are not fully visible due to the overlapping nature of the image.\n\nThe style of the image is digital and appears to be a screenshot from a software application used for data analysis or visualization, such as a spreadsheet or graphing tool. There are no texts providing additional context or information about the data being presented. ",
    "raw_text": null
  },
  {
    "chunk_id": "6d530b8d-8a27-4bb2-b33d-0376cb454c94",
    "modality": "image",
    "source_pdf": "ResNet Deep Residual Learning for Image Recognition (He et al 2016).pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital representation of a scientific graph with multiple plots and annotations. It appears to be a screenshot from a presentation or a research paper, as indicated by the watermark text \"kodansha\" in the bottom right corner.\n\nThe main feature of the image is a line graph with three lines representing different data sets: \"Original,\" \"Reduced,\" and \"Residual.\" Each line represents a different color-coded dataset, with the original being in black, reduced in red, and residual in blue. The x-axis of this graph is labeled \"Layer number (original)\" and ranges from 0 to 100, while the y-axis is labeled \"Layer index (original)\" and ranges from 0 to 60.\n\nBelow the line graph, there are two bar graphs with different color schemes. The left bar graph has a red and yellow color scheme, while the right one uses blue and green. Both bar graphs have numerical values on their y-axes, but the specific numbers are not clearly visible due to the image's resolution.\n\nThe title of the graph is \"Layer index (original)\" and is placed at the top left corner. The overall style of the image suggests it is a technical or scientific visualization used for data analysis or comparison. ",
    "raw_text": null
  },
  {
    "chunk_id": "c9dfc4e6-d2c5-4988-9cc4-8160cb503040",
    "modality": "text",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": 1,
    "retrieval_text": "5 2015 1 0 2 y a M 8 1 ] V C . s c [ 1 v 7 9 5 4 0 . 5 0 5 1 : v arXiv i X r a U-Net: Convolutional Networks for Biomedical Image Segmentation Olaf Ronneberger, Philipp Fischer, and Thomas Brox Computer Science Department and BIOSS Centre for Biological Signalling Studies, University of Freiburg, Germany ronneber@informatik.uni-freiburg.de, WWW home page: http://lmb.informatik.uni-freiburg.de/ Abstract. There is large consent that successful training of deep net- works requires many thousand annotated training samples. In this pa- per, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more e\ufb03ciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localiza- tion. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neu- ronal structures in electron microscopic stacks. Using the same net- work trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these cate- gories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Ca\ufb00e) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net. 1 Introduction In the last two years, deep convolutional networks have outperformed the state of the art in many visual recognition tasks, e.g. [7,3]. While convolutional networks have already existed for a long time [8], their success was limited due to the size of the available training sets and the size of the considered networks. The breakthrough by Krizhevsky et al. [7] was due to supervised training of a large network with 8 layers and millions of parameters on the ImageNet dataset with 1 million training images. Since then, even larger and deeper networks have been trained [12]. The typical use of convolutional networks is on classi\ufb01cation tasks, where the output to an image is a single class label. However, in many visual tasks, especially in biomedical image processing, the desired output should include localization, i.e., a class label is supposed to be assigned to each pixel. More- over, thousands of training images are usually beyond reach in biomedical tasks. Hence, Ciresan et al. [1] trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel 2 . 164 64 128 64 64 2 input output image - tle i al >|> *| segmentation =| map v 128 128 256 128 \u00a5 max pool 2x2 4 up-conv 2x2 => conv 1x1 => conv 3x3, ReLU copy and crop Fig. 1. U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the di\ufb00erent operations. as input. First, this network can localize. Secondly, the training data in terms of patches is much larger than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a large margin. Obviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it is quite slow because the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-o\ufb00 between localization accuracy and the use of context. Larger patches require more max-pooling layers that reduce the localization accuracy, while small patches allow the network to see only little context. More recent approaches [11,4] proposed a classi\ufb01er output that takes into account the features from multiple layers. Good localization and the use of context are possible at the same time. In this paper, we build upon a more elegant architecture, the so-called \u201cfully convolutional network\u201d [9]. We modify and extend this architecture such that it works with very few training images and yields more precise segmentations; see Figure 1. The main idea in [9] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence, these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled Fig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring output. A successive convolution layer can then learn to assemble a more precise output based on this information. One important modi\ufb01cation in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see Figure 2). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory. As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training im- ages. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simu- lated e\ufb03ciently. The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning. Another challenge in many cell segmentation tasks is the separation of touch- ing objects of the same class; see Figure 3. To this end, we propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function. The resulting network is applicable to various biomedical segmentation prob- lems. In this paper, we show results on the segmentation of neuronal structures in EM stacks (an ongoing competition started at ISBI 2012), where we out- 3 4 performed the network of Ciresan et al. [1]. Furthermore, we show results for cell segmentation in light microscopy images from the ISBI cell tracking chal- lenge 2015. Here we won with a large margin on the two most challenging 2D transmitted light datasets.",
    "raw_text": "5 2015 1 0 2 y a M 8 1 ] V C . s c [ 1 v 7 9 5 4 0 . 5 0 5 1 : v arXiv i X r a U-Net: Convolutional Networks for Biomedical Image Segmentation Olaf Ronneberger, Philipp Fischer, and Thomas Brox Computer Science Department and BIOSS Centre for Biological Signalling Studies, University of Freiburg, Germany ronneber@informatik.uni-freiburg.de, WWW home page: http://lmb.informatik.uni-freiburg.de/ Abstract. There is large consent that successful training of deep net- works requires many thousand annotated training samples. In this pa- per, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more e\ufb03ciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localiza- tion. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neu- ronal structures in electron microscopic stacks. Using the same net- work trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these cate- gories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Ca\ufb00e) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net. 1 Introduction In the last two years, deep convolutional networks have outperformed the state of the art in many visual recognition tasks, e.g. [7,3]. While convolutional networks have already existed for a long time [8], their success was limited due to the size of the available training sets and the size of the considered networks. The breakthrough by Krizhevsky et al. [7] was due to supervised training of a large network with 8 layers and millions of parameters on the ImageNet dataset with 1 million training images. Since then, even larger and deeper networks have been trained [12]. The typical use of convolutional networks is on classi\ufb01cation tasks, where the output to an image is a single class label. However, in many visual tasks, especially in biomedical image processing, the desired output should include localization, i.e., a class label is supposed to be assigned to each pixel. More- over, thousands of training images are usually beyond reach in biomedical tasks. Hence, Ciresan et al. [1] trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel 2 . 164 64 128 64 64 2 input output image - tle i al >|> *| segmentation =| map v 128 128 256 128 \u00a5 max pool 2x2 4 up-conv 2x2 => conv 1x1 => conv 3x3, ReLU copy and crop Fig. 1. U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the di\ufb00erent operations. as input. First, this network can localize. Secondly, the training data in terms of patches is much larger than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a large margin. Obviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it is quite slow because the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-o\ufb00 between localization accuracy and the use of context. Larger patches require more max-pooling layers that reduce the localization accuracy, while small patches allow the network to see only little context. More recent approaches [11,4] proposed a classi\ufb01er output that takes into account the features from multiple layers. Good localization and the use of context are possible at the same time. In this paper, we build upon a more elegant architecture, the so-called \u201cfully convolutional network\u201d [9]. We modify and extend this architecture such that it works with very few training images and yields more precise segmentations; see Figure 1. The main idea in [9] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence, these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled Fig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring output. A successive convolution layer can then learn to assemble a more precise output based on this information. One important modi\ufb01cation in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see Figure 2). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory. As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training im- ages. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simu- lated e\ufb03ciently. The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning. Another challenge in many cell segmentation tasks is the separation of touch- ing objects of the same class; see Figure 3. To this end, we propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function. The resulting network is applicable to various biomedical segmentation prob- lems. In this paper, we show results on the segmentation of neuronal structures in EM stacks (an ongoing competition started at ISBI 2012), where we out- 3 4 performed the network of Ciresan et al. [1]. Furthermore, we show results for cell segmentation in light microscopy images from the ISBI cell tracking chal- lenge 2015. Here we won with a large margin on the two most challenging 2D transmitted light datasets."
  },
  {
    "chunk_id": "415deebd-45f2-40b5-a4d6-7e71e017a799",
    "modality": "text",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": 4,
    "retrieval_text": "2 Network Architecture The network architecture is illustrated in Figure 1. It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a recti\ufb01ed linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (\u201cup-convolution\u201d) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each fol- lowed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the \ufb01nal layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes. In total the network has 23 convolutional layers. To allow a seamless tiling of the output segmentation map (see Figure 2), it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size. 3 Training The input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Ca\ufb00e [6]. Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image. Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step. The energy function is computed by a pixel-wise soft-max over the final feature map combined with the cross entropy loss function. The soft-max is defined as px(x) = exp(ax(x))/ (ch, exp(ax(x))) where a,(x) denotes the activation in feature channel k at the pixel position x \u20ac Q with Q C Z?. K is the number of classes and p,(x) is the approximated maximum-function. Le. pr(x) & 1 for the k that has the maximum activation a;(x) and p,(x) \u00a9 0 for all other k. The cross entropy then penalizes at each position the deviation of Pex) (x) from 1 using B= Ss w(x) log (Pex) (X)) ) xEQ a b c \\ \u2018 d : . Fig. 3. HeLa cells on glass recorded with DIC (di\ufb00erential interference contrast) mi- croscopy. (a) raw image. (b) overlay with ground truth segmentation. Di\ufb00erent colors indicate di\ufb00erent instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels. where \u00a2: Q \u2014 {1,...,K} is the true label of each pixel and w : 2 > R is a weight map that we introduced to give some pixels more importance in the training. We pre-compute the weight map for each ground truth segmentation to com- pensate the di\ufb00erent frequency of pixels from a certain class in the training data set, and to force the network to learn the small separation borders that we introduce between touching cells (See Figure 3c and d). The separation border is computed using morphological operations. The weight map is then computed as w(x) = wc(x) + w0 \u00b7 exp \u2212 (d1(x) + d2(x))2 2\u03c32 (2) where wc : \u2126 \u2192 R is the weight map to balance the class frequencies, d1 : \u2126 \u2192 R denotes the distance to the border of the nearest cell and d2 : \u2126 \u2192 R the distance to the border of the second nearest cell. In our experiments we set w0 = 10 and \u03c3 \u2248 5 pixels. In deep networks with many convolutional layers and different paths through the network, a good initialization of the weights is extremely important. Oth- erwise, parts of the network might give excessive activations, while other parts never contribute. Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. For a network with our architecture (alternating convolution and ReLU layers) this can be achieved by drawing the initial weights from a Gaussian distribution with a standard deviation of \\/2/N, where N denotes the number of incoming nodes of one neu- ron [5]. E.g. for a 3x3 convolution and 64 feature channels in the previous layer N=9-64=576.",
    "raw_text": "2 Network Architecture The network architecture is illustrated in Figure 1. It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a recti\ufb01ed linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (\u201cup-convolution\u201d) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each fol- lowed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the \ufb01nal layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes. In total the network has 23 convolutional layers. To allow a seamless tiling of the output segmentation map (see Figure 2), it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size. 3 Training The input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Ca\ufb00e [6]. Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image. Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step. The energy function is computed by a pixel-wise soft-max over the final feature map combined with the cross entropy loss function. The soft-max is defined as px(x) = exp(ax(x))/ (ch, exp(ax(x))) where a,(x) denotes the activation in feature channel k at the pixel position x \u20ac Q with Q C Z?. K is the number of classes and p,(x) is the approximated maximum-function. Le. pr(x) & 1 for the k that has the maximum activation a;(x) and p,(x) \u00a9 0 for all other k. The cross entropy then penalizes at each position the deviation of Pex) (x) from 1 using B= Ss w(x) log (Pex) (X)) ) xEQ a b c \\ \u2018 d : . Fig. 3. HeLa cells on glass recorded with DIC (di\ufb00erential interference contrast) mi- croscopy. (a) raw image. (b) overlay with ground truth segmentation. Di\ufb00erent colors indicate di\ufb00erent instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels. where \u00a2: Q \u2014 {1,...,K} is the true label of each pixel and w : 2 > R is a weight map that we introduced to give some pixels more importance in the training. We pre-compute the weight map for each ground truth segmentation to com- pensate the di\ufb00erent frequency of pixels from a certain class in the training data set, and to force the network to learn the small separation borders that we introduce between touching cells (See Figure 3c and d). The separation border is computed using morphological operations. The weight map is then computed as w(x) = wc(x) + w0 \u00b7 exp \u2212 (d1(x) + d2(x))2 2\u03c32 (2) where wc : \u2126 \u2192 R is the weight map to balance the class frequencies, d1 : \u2126 \u2192 R denotes the distance to the border of the nearest cell and d2 : \u2126 \u2192 R the distance to the border of the second nearest cell. In our experiments we set w0 = 10 and \u03c3 \u2248 5 pixels. In deep networks with many convolutional layers and different paths through the network, a good initialization of the weights is extremely important. Oth- erwise, parts of the network might give excessive activations, while other parts never contribute. Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. For a network with our architecture (alternating convolution and ReLU layers) this can be achieved by drawing the initial weights from a Gaussian distribution with a standard deviation of \\/2/N, where N denotes the number of incoming nodes of one neu- ron [5]. E.g. for a 3x3 convolution and 64 feature channels in the previous layer N=9-64=576."
  },
  {
    "chunk_id": "ed06f0c4-dbb5-4525-aa67-9818dd667f3b",
    "modality": "text",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": 5,
    "retrieval_text": "3.1 Data Augmentation Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of 5 6 microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elas- tic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images. We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpola- tion. Drop-out layers at the end of the contracting path perform further implicit data augmentation. 4 Experiments We demonstrate the application of the u-net to three di\ufb00erent segmentation tasks. The \ufb01rst task is the segmentation of neuronal structures in electron mi- croscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions. The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila \ufb01rst instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its seg- mentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 di\ufb00erent levels and computation of the \u201cwarping error\u201d, the \u201cRand error\u201d and the \u201cpixel error\u201d [14]. The u-net (averaged over 7 rotated versions of the input data) achieves with- out any further pre- or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a rand-error of 0.0382. This is signi\ufb01cantly better than the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing Table 1. Ranking on the EM segmentation challenge [14] (march 6th, 2015), sorted by warping error. Warping Error Rand Error Pixel Error ** human values ** 0.000005 0.0021 0.0010 1. u-net 0.000353 0.0382 0.0611 2. DIVE-SCI 0.000355 0.0305 0.0584 3. IDSIA [1] 0.000420 0.0504 0.0613 4. DIVE 0.000430 0.0545 0.0582 ... 10. IDSIA-SCI 0.000653 0.0189 0.1027 Rank Group name a Fig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the \u201cPhC-U373\u201d data set. (b) Segmentation result (cyan mask) with manual ground truth (yellow border) (c) input image of the \u201cDIC-HeLa\u201d data set. (d) Segmentation result (random colored masks) with manual ground truth (yellow border). Table 2. Segmentation results (IOU) on the ISBI cell tracking challenge 2015. Name PhC-U373 DIC-HeLa IMCB-SG (2014) 0.2669 0.2935 KTH-SE (2014) 0.7953 0.4607 HOUS-US (2014) 0.5323 - second-best 2015 0.83 0.46 u-net (2015) 0.9203 0.7756 algorithms on this data set use highly data set speci\ufb01c post-processing methods1 applied to the probability map of Ciresan et al. [1]. We also applied the u-net to a cell segmentation task in light microscopic im- ages. This segmenation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13]. The \ufb01rst data set \u201cPhC-U373\u201d2 contains Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy (see Figure 4a,b and Supp. Material). It contains 35 partially annotated train- ing images. Here we achieve an average IOU (\u201cintersection over union\u201d) of 92%, which is signi\ufb01cantly better than the second best algorithm with 83% (see Ta- ble 2). The second data set \u201cDIC-HeLa\u201d3 are HeLa cells on a \ufb02at glass recorded by di\ufb00erential interference contrast (DIC) microscopy (see Figure 3, Figure 4c,d and Supp. Material). It contains 20 partially annotated training images. Here we achieve an average IOU of 77.5% which is signi\ufb01cantly better than the second best algorithm with 46%.",
    "raw_text": "3.1 Data Augmentation Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of 5 6 microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elas- tic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images. We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpola- tion. Drop-out layers at the end of the contracting path perform further implicit data augmentation. 4 Experiments We demonstrate the application of the u-net to three di\ufb00erent segmentation tasks. The \ufb01rst task is the segmentation of neuronal structures in electron mi- croscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions. The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila \ufb01rst instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its seg- mentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 di\ufb00erent levels and computation of the \u201cwarping error\u201d, the \u201cRand error\u201d and the \u201cpixel error\u201d [14]. The u-net (averaged over 7 rotated versions of the input data) achieves with- out any further pre- or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a rand-error of 0.0382. This is signi\ufb01cantly better than the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing Table 1. Ranking on the EM segmentation challenge [14] (march 6th, 2015), sorted by warping error. Warping Error Rand Error Pixel Error ** human values ** 0.000005 0.0021 0.0010 1. u-net 0.000353 0.0382 0.0611 2. DIVE-SCI 0.000355 0.0305 0.0584 3. IDSIA [1] 0.000420 0.0504 0.0613 4. DIVE 0.000430 0.0545 0.0582 ... 10. IDSIA-SCI 0.000653 0.0189 0.1027 Rank Group name a Fig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the \u201cPhC-U373\u201d data set. (b) Segmentation result (cyan mask) with manual ground truth (yellow border) (c) input image of the \u201cDIC-HeLa\u201d data set. (d) Segmentation result (random colored masks) with manual ground truth (yellow border). Table 2. Segmentation results (IOU) on the ISBI cell tracking challenge 2015. Name PhC-U373 DIC-HeLa IMCB-SG (2014) 0.2669 0.2935 KTH-SE (2014) 0.7953 0.4607 HOUS-US (2014) 0.5323 - second-best 2015 0.83 0.46 u-net (2015) 0.9203 0.7756 algorithms on this data set use highly data set speci\ufb01c post-processing methods1 applied to the probability map of Ciresan et al. [1]. We also applied the u-net to a cell segmentation task in light microscopic im- ages. This segmenation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13]. The \ufb01rst data set \u201cPhC-U373\u201d2 contains Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy (see Figure 4a,b and Supp. Material). It contains 35 partially annotated train- ing images. Here we achieve an average IOU (\u201cintersection over union\u201d) of 92%, which is signi\ufb01cantly better than the second best algorithm with 83% (see Ta- ble 2). The second data set \u201cDIC-HeLa\u201d3 are HeLa cells on a \ufb02at glass recorded by di\ufb00erential interference contrast (DIC) microscopy (see Figure 3, Figure 4c,d and Supp. Material). It contains 20 partially annotated training images. Here we achieve an average IOU of 77.5% which is signi\ufb01cantly better than the second best algorithm with 46%."
  },
  {
    "chunk_id": "c5d9bb21-74c1-438d-8c8c-cbf397e54639",
    "modality": "text",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": 7,
    "retrieval_text": "5 Conclusion The u-net architecture achieves very good performance on very di\ufb00erent biomed- ical segmentation applications. Thanks to data augmentation with elastic defor- 1 The authors of this algorithm have submitted 78 di\ufb00erent solutions to achieve this result. 2 Data set provided by Dr. Sanjay Kumar. Department of Bioengineering University of California at Berkeley. Berkeley CA (USA) 3 Data set provided by Dr. Gert van Cappellen Erasmus Medical Center. Rotterdam. The Netherlands 7 8 mations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the full Ca\ufb00e[6]-based implementation and the trained networks4. We are sure that the u-net architecture can be applied easily to many more tasks. Acknowlegements This study was supported by the Excellence Initiative of the German Federal and State governments (EXC 294) and by the BMBF (Fkz 0316185B). References 1. Ciresan, D.C., Gambardella, L.M., Giusti, A., Schmidhuber, J.: Deep neural net- works segment neuronal membranes in electron microscopy images. In: NIPS. pp. 2852\u20132860 (2012) 2. Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative un- supervised feature learning with convolutional neural networks. In: NIPS (2014) 3. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for ac- curate object detection and semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014) 4. Hariharan, B., Arbelez, P., Girshick, R., Malik, J.: Hypercolumns for object seg- mentation and \ufb01ne-grained localization (2014), arXiv:1411.5752 [cs.CV] 5. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into recti\ufb01ers: Surpassing human- level performance on imagenet classi\ufb01cation (2015), arXiv:1502.01852 [cs.CV] 6. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar- rama, S., Darrell, T.: Ca\ufb00e: Convolutional architecture for fast feature embedding (2014), arXiv:1408.5093 [cs.CV] 7. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classi\ufb01cation with deep con- volutional neural networks. In: NIPS. pp. 1106\u20131114 (2012) 8. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural Computation 1(4), 541\u2013551 (1989) 9. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation (2014), arXiv:1411.4038 [cs.CV] 10. Maska, M., (...), de Solorzano, C.O.: A benchmark for comparison of cell tracking algorithms. Bioinformatics 30, 1609\u20131617 (2014) 11. Seyedhosseini, M., Sajjadi, M., Tasdizen, T.: Image segmentation with cascaded hierarchical models and logistic disjunctive normal networks. In: Computer Vision (ICCV), 2013 IEEE International Conference on. pp. 2168\u20132175 (2013) 12. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2014), arXiv:1409.1556 [cs.CV] 13. WWW: Web page of the cell tracking challenge, http://www.codesolorzano.com/ celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html 14. WWW: Web page of the em segmentation challenge, http://brainiac2.mit.edu/ isbi_challenge/ 4 U-net implementation, trained networks and supplementary material available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net",
    "raw_text": "5 Conclusion The u-net architecture achieves very good performance on very di\ufb00erent biomed- ical segmentation applications. Thanks to data augmentation with elastic defor- 1 The authors of this algorithm have submitted 78 di\ufb00erent solutions to achieve this result. 2 Data set provided by Dr. Sanjay Kumar. Department of Bioengineering University of California at Berkeley. Berkeley CA (USA) 3 Data set provided by Dr. Gert van Cappellen Erasmus Medical Center. Rotterdam. The Netherlands 7 8 mations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the full Ca\ufb00e[6]-based implementation and the trained networks4. We are sure that the u-net architecture can be applied easily to many more tasks. Acknowlegements This study was supported by the Excellence Initiative of the German Federal and State governments (EXC 294) and by the BMBF (Fkz 0316185B). References 1. Ciresan, D.C., Gambardella, L.M., Giusti, A., Schmidhuber, J.: Deep neural net- works segment neuronal membranes in electron microscopy images. In: NIPS. pp. 2852\u20132860 (2012) 2. Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative un- supervised feature learning with convolutional neural networks. In: NIPS (2014) 3. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for ac- curate object detection and semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014) 4. Hariharan, B., Arbelez, P., Girshick, R., Malik, J.: Hypercolumns for object seg- mentation and \ufb01ne-grained localization (2014), arXiv:1411.5752 [cs.CV] 5. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into recti\ufb01ers: Surpassing human- level performance on imagenet classi\ufb01cation (2015), arXiv:1502.01852 [cs.CV] 6. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar- rama, S., Darrell, T.: Ca\ufb00e: Convolutional architecture for fast feature embedding (2014), arXiv:1408.5093 [cs.CV] 7. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classi\ufb01cation with deep con- volutional neural networks. In: NIPS. pp. 1106\u20131114 (2012) 8. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural Computation 1(4), 541\u2013551 (1989) 9. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation (2014), arXiv:1411.4038 [cs.CV] 10. Maska, M., (...), de Solorzano, C.O.: A benchmark for comparison of cell tracking algorithms. Bioinformatics 30, 1609\u20131617 (2014) 11. Seyedhosseini, M., Sajjadi, M., Tasdizen, T.: Image segmentation with cascaded hierarchical models and logistic disjunctive normal networks. In: Computer Vision (ICCV), 2013 IEEE International Conference on. pp. 2168\u20132175 (2013) 12. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2014), arXiv:1409.1556 [cs.CV] 13. WWW: Web page of the cell tracking challenge, http://www.codesolorzano.com/ celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html 14. WWW: Web page of the em segmentation challenge, http://brainiac2.mit.edu/ isbi_challenge/ 4 U-net implementation, trained networks and supplementary material available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net"
  },
  {
    "chunk_id": "93b111dc-9393-464d-9004-155fb4c6ce18",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital graphic that appears to be a screenshot of a computer interface displaying a sequence of images or frames, each labeled with a number from 1 to 8. These images are arranged in two rows, with the top row showing four images and the bottom row showing four more. Each image contains a series of vertical bars of varying heights, which seem to represent some form of data or measurements.\n\nThe leftmost column of numbers is labeled \"Nagarjuna,\" indicating that these are likely the names of the frames or images being displayed. The rightmost column of numbers is labeled \"Auto,\" suggesting that these are the corresponding automatic segmentation results for each frame. There's also a column with the label \"Segmentation,\" which might represent the manual segmentation results, and another column labeled \"CORR,\" which could be a measure of correlation or similarity between the auto and manual segmentations.\n\nThe images themselves are too small to discern specific details, but they all appear to have a similar structure with multiple vertical bars in each frame. The numbers next to the bars increase from left to right, suggesting that the values associated with these bars also increase as you move across the frames.\n\nAt the bottom of the image, there's a legend or key that includes three different icons: one for \"Nagarjuna,\" one for \"Auto,\" and one for \"Segmentation.\" Each icon is accompanied by a label, but the text is too small to read clearly. The overall style of the image suggests it might be from a scientific paper or presentation related to image processing or computer vision, specifically discussing the comparison between manual and automatic segmentation methods. ",
    "raw_text": null
  },
  {
    "chunk_id": "1c7a5d48-326a-4a4b-b447-3a74112443fd",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital composite that appears to be a microscopic view of cells with various stains and markers. It shows three distinct sections:\n\n1. On the left side, there's a magnified view of what seems to be a cellular structure with multiple nuclei, each labeled with a green marker. The cells are densely packed together, suggesting they might be in a tissue sample or a culture dish.\n\n2. In the center, there is a zoomed-in section where individual cells are more clearly visible. These cells have a darker staining, possibly indicating a different type of cellular component or a different stain. The green marker is present here as well, highlighting specific areas within the cells.\n\n3. On the right side, there's an overlay of a yellow box with a black outline, which seems to be a digital annotation tool used for scientific analysis. This box is placed on top of the central image, indicating that it has been selected or is being analyzed. The box contains a magnified view of the cellular structure within the center image, providing a closer look at the details of the cells.\n\nThe background of the image is dark and indistinct, which helps to emphasize the stained cells and the annotation tool. There are no texts or labels visible in the image that provide additional context or information about the cells or the analysis being performed. The style of the image suggests it could be from a scientific study or a research project related to cell biology or histology. ",
    "raw_text": null
  },
  {
    "chunk_id": "572073a0-cab2-497a-a473-d5b03bbc8ab6",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a black and white photograph of a microscopic view of a cellular structure, possibly a section of a plant or animal tissue. It shows a complex network of cells with various shapes and sizes, interconnected by thin strands that could be part of a cytoskeleton or extracellular matrix. The image is taken from an angle where the top right corner is cut off, leaving only the lower left portion visible. There are no texts or labels present in the image. ",
    "raw_text": null
  },
  {
    "chunk_id": "7834d184-53fb-47f5-a29f-82b31ffac46b",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a microscopic view of cells under a light microscope. It appears to be a fluorescently labeled cell culture, likely from a tissue sample, given the presence of nuclei and cytoplasmic structures. The cells are densely packed with a granular appearance, indicative of a dense matrix or organelles within the cytoplasm. There is no text present in the image to provide additional context or information about the cells. The style of the image is scientific and microscopic, typically used in biological research to study cellular structures and functions. ",
    "raw_text": null
  },
  {
    "chunk_id": "91ae02a4-5f4c-4fd5-a86e-404c13698ddd",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a close-up photograph of a microscopic view of cells with variously colored markers on them. These markers are likely fluorescent dyes used for cell labeling in scientific research. The colors include shades of blue, green, red, yellow, and purple, each representing different groups or types of cells. The cells appear to be densely packed together, suggesting a high concentration or density within the sample. There is no text present on the image, and the style of the photograph is typical for scientific microscopy, with a focus on the cellular details rather than artistic representation. ",
    "raw_text": null
  },
  {
    "chunk_id": "439c5034-6515-4a4a-b0ad-562f687c002f",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a black and white photograph that features a close-up of what appears to be a map or a geographical representation of an area. The central focus is on a section of the map where there are distinct shapes resembling leaves or petals, which are likely representing different regions or territories. These shapes are outlined in white against a dark background, creating a stark contrast that highlights their details.\n\nThe map includes various lines and borders that define the boundaries between these shapes. The lines are also white, providing a clear demarcation of the territories. There is no visible text on the image to provide additional context or information about the location being depicted.\n\nThe style of the image is simplistic and schematic, with an emphasis on the geographical features rather than realistic representation. The absence of color and the use of white lines and shapes give it a minimalist aesthetic. ",
    "raw_text": null
  },
  {
    "chunk_id": "eda04e5d-4dae-4615-b5ed-0f785bf39d8f",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital graphic with a blue background that appears to be a screenshot of a scientific or medical data visualization. It features two main panels: on the left side, there's a color-coded heat map overlaid on a satellite image of what seems to be a geographical area, possibly a city or region, with roads and landmarks visible. The heat map uses a scale from 0 to 6, with different colors representing varying levels of intensity. There are also two lines that seem to represent some form of data or measurement, one in red and the other in blue, which could be related to temperature or another variable.\n\nOn the right side, there's a line graph with a title \"Temperature\" at the top, indicating that the graph is likely representing temperature data over time. The x-axis of the graph is labeled \"Time (hours)\" and ranges from 0 to 12 hours, while the y-axis is labeled \"Temperature (\u00b0C)\" and ranges from -5 to 30 degrees Celsius. There are two lines on this graph as well, one in red and the other in blue, which correspond to the same colors used in the heat map.\n\nThe image has a technical appearance, suggesting it is related to scientific research or analysis, possibly in fields such as meteorology, climatology, or environmental studies. The style of the image is informational and analytical, with a focus on data representation and visualization. ",
    "raw_text": null
  },
  {
    "chunk_id": "1041125c-7055-43e5-a5c7-c4371b6274ee",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a microscopic view of cells, likely taken through a microscope. It shows three distinct cellular structures: two larger cells with visible nuclei and cytoplasmic details, and one smaller cell that appears to be in the process of dividing or undergoing some form of transformation. The cells are stained, which is common in microscopy to enhance their features for observation.\n\nThe image has a watermark text overlay at the bottom right corner that reads \"Kittens,\" which is likely an unrelated and humorous addition to the image, as it does not provide any context or information about the content of the image. The style of the image is scientific, with a focus on biological microscopy, and it is presented in a way that suggests it might be used for educational or research purposes. ",
    "raw_text": null
  },
  {
    "chunk_id": "4ae7b737-5fbf-468e-8932-034adfe6908c",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a digital composite that appears to be a scientific or medical illustration. It features three distinct images of what seem to be microscopic cells or organisms, each with a different shape and size. The cells are stained in shades of blue and green, suggesting they may have been prepared for microscopy using a technique such as fluorescence microscopy.\n\nThe top cell is elongated and has a pointed end, resembling a sperm or a similar type of motile organism. The middle cell is round with a smooth surface, which could indicate it's a type of bacteria or a protozoan. The bottom cell is irregularly shaped with multiple protrusions, possibly indicating a complex structure or a type of eukaryotic cell with many extensions.\n\nThe cells are set against a dark background that provides contrast, highlighting their shapes and colors. There are no visible texts or labels within the image to provide additional context or information about the cells. The style of the image is clinical and informative, typical of scientific documentation used in research or educational materials. ",
    "raw_text": null
  },
  {
    "chunk_id": "599f9bef-5a2d-4352-9e67-8c2334d93d49",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a microscopic view of a cellular structure, specifically a section of a cell with various organelles visible. At the center of the image, there is a large, spherical organelle that appears to be a nucleus, characterized by its dense, granular texture and central position within the cell. Surrounding the nucleus are smaller, irregularly shaped structures that could be mitochondria or other membrane-bound organelles, with some having a more rounded shape and others being more elongated. The background is a uniform, dark color, providing contrast to the lighter, more structured appearance of the cellular components. There are no texts or labels present in the image. The style of the image is a scientific microscopic photograph, commonly used in biological research to study cellular structures and functions. ",
    "raw_text": null
  },
  {
    "chunk_id": "e40506ea-d606-4f74-a03e-3d29049f8d28",
    "modality": "image",
    "source_pdf": "U-Net Convolutional Networks for Biomedical Image Segmentation.pdf",
    "page_number": null,
    "retrieval_text": " The image is a microscopic view of cells with variously colored markers attached to them. These markers are likely fluorescent dyes used for cell labeling in scientific research. The colors include shades of purple, green, red, yellow, blue, and orange. The cells appear to be in close proximity to each other, suggesting they may be interacting or part of a larger structure. There is no text present in the image. The style of the image is a scientific microscopic photograph with a focus on cellular biology or immunohistochemistry. ",
    "raw_text": null
  },
  {
    "chunk_id": "25ed4e7e-ed7a-4c24-9ae2-c983b99cb236",
    "modality": "text",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": 1,
    "retrieval_text": "8 2018 1 0 2 r p A 8 ] V C . s c [ 1 v 7 6 7 2 0 . 4 0 8 1 : v i X r a YOLOv3: An Incremental Improvement Joseph Redmon Ali Farhadi University of Washington Abstract We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that\u2019s pretty swell. It\u2019s a little bigger than last time but more accurate. It\u2019s still fast though, don\u2019t worry. At 320 \u00d7 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP50 in 51 ms on a Titan X, com- pared to 57.5 AP50 in 198 ms by RetinaNet, similar perfor- mance but 3.8\u00d7 faster. As always, all the code is online at https://pjreddie.com/yolo/. 38 W yoLovs ~@ RetinaNet-50 S RetinaNet-101 36 Ig] Method mAP_ time [B] SSD321 28.0 61 [C] DSSD321 28.0 85 o 34 [D] R-FCN 299 85 fo) [F] [E] SSD513 31.2 125 re) [F] DSSD513 33.2 156 [G] FPN FRCN 36.2 172 El RetinaNet-50-500 32.5 73 RetinaNet-101-500 34.4 90 30 1} RetinaNet-101-800 37.8 198 YOLOv3-320 28.2 22 YOLOv3-416 31.0 29 28 (c] YOLOv3-608 33.0 51 50 100 150 200 250 inference time (ms) 1. Introduction Figure 1. We adapt this \ufb01gure from the Focal Loss paper [9]. YOLOv3 runs signi\ufb01cantly faster than other detection methods with comparable performance. Times from either an M40 or Titan X, they are basically the same GPU. Sometimes you just kinda phone it in for a year, you know? I didn\u2019t do a whole lot of research this year. Spent a lot of time on Twitter. Played around with GANs a little. I had a little momentum left over from last year [12] [1]; I managed to make some improvements to YOLO. But, hon- estly, nothing like super interesting, just a bunch of small changes that make it better. I also helped out with other people\u2019s research a little. 2.1. Bounding Box Prediction Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes [15]. The network predicts 4 coordinates for each bounding box, tx, ty, tw, th. If the cell is offset from the top left corner of the image by (cx,cy) and the bounding box prior has width and height pw, ph, then the predictions correspond to: Actually, that\u2019s what brings us here today. We have a camera-ready deadline [4] and we need to cite some of the random updates I made to YOLO but we don\u2019t have a source. So get ready for a TECH REPORT! The great thing about tech reports is that they don\u2019t need intros, y\u2019all know why we\u2019re here. So the end of this intro- duction will signpost for the rest of the paper. First we\u2019ll tell you what the deal is with YOLOv3. Then we\u2019ll tell you how we do. We\u2019ll also tell you about some things we tried that didn\u2019t work. Finally we\u2019ll contemplate what this all means.",
    "raw_text": "8 2018 1 0 2 r p A 8 ] V C . s c [ 1 v 7 6 7 2 0 . 4 0 8 1 : v i X r a YOLOv3: An Incremental Improvement Joseph Redmon Ali Farhadi University of Washington Abstract We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that\u2019s pretty swell. It\u2019s a little bigger than last time but more accurate. It\u2019s still fast though, don\u2019t worry. At 320 \u00d7 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP50 in 51 ms on a Titan X, com- pared to 57.5 AP50 in 198 ms by RetinaNet, similar perfor- mance but 3.8\u00d7 faster. As always, all the code is online at https://pjreddie.com/yolo/. 38 W yoLovs ~@ RetinaNet-50 S RetinaNet-101 36 Ig] Method mAP_ time [B] SSD321 28.0 61 [C] DSSD321 28.0 85 o 34 [D] R-FCN 299 85 fo) [F] [E] SSD513 31.2 125 re) [F] DSSD513 33.2 156 [G] FPN FRCN 36.2 172 El RetinaNet-50-500 32.5 73 RetinaNet-101-500 34.4 90 30 1} RetinaNet-101-800 37.8 198 YOLOv3-320 28.2 22 YOLOv3-416 31.0 29 28 (c] YOLOv3-608 33.0 51 50 100 150 200 250 inference time (ms) 1. Introduction Figure 1. We adapt this \ufb01gure from the Focal Loss paper [9]. YOLOv3 runs signi\ufb01cantly faster than other detection methods with comparable performance. Times from either an M40 or Titan X, they are basically the same GPU. Sometimes you just kinda phone it in for a year, you know? I didn\u2019t do a whole lot of research this year. Spent a lot of time on Twitter. Played around with GANs a little. I had a little momentum left over from last year [12] [1]; I managed to make some improvements to YOLO. But, hon- estly, nothing like super interesting, just a bunch of small changes that make it better. I also helped out with other people\u2019s research a little. 2.1. Bounding Box Prediction Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes [15]. The network predicts 4 coordinates for each bounding box, tx, ty, tw, th. If the cell is offset from the top left corner of the image by (cx,cy) and the bounding box prior has width and height pw, ph, then the predictions correspond to: Actually, that\u2019s what brings us here today. We have a camera-ready deadline [4] and we need to cite some of the random updates I made to YOLO but we don\u2019t have a source. So get ready for a TECH REPORT! The great thing about tech reports is that they don\u2019t need intros, y\u2019all know why we\u2019re here. So the end of this intro- duction will signpost for the rest of the paper. First we\u2019ll tell you what the deal is with YOLOv3. Then we\u2019ll tell you how we do. We\u2019ll also tell you about some things we tried that didn\u2019t work. Finally we\u2019ll contemplate what this all means."
  },
  {
    "chunk_id": "9e11a751-b2c7-474a-adb2-4d2b1fe82e78",
    "modality": "text",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": 1,
    "retrieval_text": "2. The Deal So here\u2019s the deal with YOLOv3: We mostly took good ideas from other people. We also trained a new classi\ufb01er network that\u2019s better than the other ones. We\u2019ll just take you through the whole system from scratch so you can un- derstand it all. bx = \u03c3(tx) + cx by = \u03c3(ty) + cy bw = pwetw bh = pheth During training we use sum of squared error loss. If the ground truth for some coordinate prediction is \u02c6t* our gra- dient is the ground truth value (computed from the ground truth box) minus our prediction: \u02c6t* \u2212 t*. This ground truth value can be easily computed by inverting the equations above. YOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bound- ing box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior 1 eeneeeeeee \u00ab \u00ab< \u00ab Figure 2. Bounding boxes with dimension priors and location prediction. We predict the width and height of the box as offsets from cluster centroids. We predict the center coordinates of the box relative to the location of \ufb01lter application using a sigmoid function. This \ufb01gure blatantly self-plagiarized from [15]. is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, follow- ing [17]. We use the threshold of .5. Unlike [17] our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predic- tions, only objectness. 2.2. Class Prediction Each box predicts the classes the bounding box may con- tain using multilabel classi\ufb01cation. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classi\ufb01ers. Dur- ing training we use binary cross-entropy loss for the class predictions. This formulation helps when we move to more complex domains like the Open Images Dataset [7]. In this dataset there are many overlapping labels (i.e. Woman and Person). Using a softmax imposes the assumption that each box has exactly one class which is often not the case. A multilabel approach better models the data.",
    "raw_text": "2. The Deal So here\u2019s the deal with YOLOv3: We mostly took good ideas from other people. We also trained a new classi\ufb01er network that\u2019s better than the other ones. We\u2019ll just take you through the whole system from scratch so you can un- derstand it all. bx = \u03c3(tx) + cx by = \u03c3(ty) + cy bw = pwetw bh = pheth During training we use sum of squared error loss. If the ground truth for some coordinate prediction is \u02c6t* our gra- dient is the ground truth value (computed from the ground truth box) minus our prediction: \u02c6t* \u2212 t*. This ground truth value can be easily computed by inverting the equations above. YOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bound- ing box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior 1 eeneeeeeee \u00ab \u00ab< \u00ab Figure 2. Bounding boxes with dimension priors and location prediction. We predict the width and height of the box as offsets from cluster centroids. We predict the center coordinates of the box relative to the location of \ufb01lter application using a sigmoid function. This \ufb01gure blatantly self-plagiarized from [15]. is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, follow- ing [17]. We use the threshold of .5. Unlike [17] our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predic- tions, only objectness. 2.2. Class Prediction Each box predicts the classes the bounding box may con- tain using multilabel classi\ufb01cation. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classi\ufb01ers. Dur- ing training we use binary cross-entropy loss for the class predictions. This formulation helps when we move to more complex domains like the Open Images Dataset [7]. In this dataset there are many overlapping labels (i.e. Woman and Person). Using a softmax imposes the assumption that each box has exactly one class which is often not the case. A multilabel approach better models the data."
  },
  {
    "chunk_id": "56d058be-f4ba-4098-b62b-c1ee81d3bce5",
    "modality": "text",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": 2,
    "retrieval_text": "2.3. Predictions Across Scales YOLOv3 predicts boxes at 3 different scales. Our sys- tem extracts features from those scales using a similar con- cept to feature pyramid networks [8]. From our base fea- ture extractor we add several convolutional layers. The last of these predicts a 3-d tensor encoding bounding box, ob- jectness, and class predictions. In our experiments with COCO [10] we predict 3 boxes at each scale so the tensor is N \u00d7 N \u00d7 [3 \u2217 (4 + 1 + 80)] for the 4 bounding box offsets, 1 objectness prediction, and 80 class predictions. Next we take the feature map from 2 layers previous and upsample it by 2\u00d7. We also take a feature map from earlier in the network and merge it with our upsampled features using concatenation. This method allows us to get more meaningful semantic information from the upsampled fea- tures and \ufb01ner-grained information from the earlier feature map. We then add a few more convolutional layers to pro- cess this combined feature map, and eventually predict a similar tensor, although now twice the size. We perform the same design one more time to predict boxes for the \ufb01nal scale. Thus our predictions for the 3rd scale bene\ufb01t from all the prior computation as well as \ufb01ne- grained features from early on in the network. We still use k-means clustering to determine our bound- ing box priors. We just sort of chose 9 clusters and 3 scales arbitrarily and then divide up the clusters evenly across scales. On the COCO dataset the 9 clusters were: (10\u00d713),(16\u00d730),(33\u00d723),(30\u00d761),(62\u00d745),(59\u00d7 119),(116 \u00d7 90),(156 \u00d7 198),(373 \u00d7 326). 2.4. Feature Extractor We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3\u00d73 and 1\u00d71 convolutional layers but now has some shortcut connections as well and is signi\ufb01cantly larger. It has 53 convolutional layers so we call it.... wait for it..... Darknet-53! Type Filters Size Output Convolutional 32 3x3 256 x 256 Convolutional 64 3x3/2 128x128 Convolutional 32 1x1 1x] Convolutional 64 3x3 Residua 128 x 128 Convolutional 128 3x3/2 64x64 Convolutional 64 1x1 2x} Convolutional 128 3x3 Residua 64 x 64 Convolutional 256 3x3/2 32x32 Convolutional 128 1x1 8x| Convolutional 256 3x3 Residua 32 x 32 Convolutional 512 3x3/2 16x16 Convolutional 256 1x1 8x} Convolutional 512 3x3 Residua 16 x 16 Convolutional 1024 3x3/2 8x8 Convolutional 512 1x1 4x} Convolutional 1024 3x3 Residua 8x8 Avgpool Global Connected 1000 Softmax Table 1. Darknet-53. This new network is much more powerful than Darknet- 19 but still more ef\ufb01cient than ResNet-101 or ResNet-152. Here are some ImageNet results: Backbone Top-1 Top-5 Bn Ops BFLOP/s Darknet-19 [15] 74.1 91.8 7.29 1246 ResNet-101[5] 77.1 93.7 19.7 1039 ResNet-152 [5] Darknet-53 77.6 77.2 93.8 93.8 29.4 18.7 1090 1457 FPS 171 53 37 78 models like RetinaNet in this metric though. However, when we look at the \u201cold\u201d detection metric of mAP at IOU= .5 (or AP50 in the chart) YOLOv3 is very strong. It is almost on par with RetinaNet and far above the SSD variants. This indicates that YOLOv3 is a very strong detector that excels at producing decent boxes for ob- jects. However, performance drops signi\ufb01cantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object. Table 2. Comparison of backbones. Accuracy, billions of oper- ations, billion \ufb02oating point operations per second, and FPS for various networks. Each network is trained with identical settings and tested at 256\u00d7256, single crop accuracy. Run times are measured on a Titan X at 256 \u00d7 256. Thus Darknet-53 performs on par with state-of-the-art classi\ufb01ers but with fewer \ufb02oating point operations and more speed. Darknet-53 is better than ResNet-101 and 1.5\u00d7 faster. Darknet-53 has similar perfor- mance to ResNet-152 and is 2\u00d7 faster. In the past YOLO struggled with small objects. How- ever, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high APS performance. However, it has comparatively worse performance on medium and larger size objects. More in- vestigation is needed to get to the bottom of this. When we plot accuracy vs speed on the AP50 metric (see \ufb01gure 5) we see YOLOv3 has signi\ufb01cant bene\ufb01ts over other detection systems. Namely, it\u2019s faster and better.",
    "raw_text": "2.3. Predictions Across Scales YOLOv3 predicts boxes at 3 different scales. Our sys- tem extracts features from those scales using a similar con- cept to feature pyramid networks [8]. From our base fea- ture extractor we add several convolutional layers. The last of these predicts a 3-d tensor encoding bounding box, ob- jectness, and class predictions. In our experiments with COCO [10] we predict 3 boxes at each scale so the tensor is N \u00d7 N \u00d7 [3 \u2217 (4 + 1 + 80)] for the 4 bounding box offsets, 1 objectness prediction, and 80 class predictions. Next we take the feature map from 2 layers previous and upsample it by 2\u00d7. We also take a feature map from earlier in the network and merge it with our upsampled features using concatenation. This method allows us to get more meaningful semantic information from the upsampled fea- tures and \ufb01ner-grained information from the earlier feature map. We then add a few more convolutional layers to pro- cess this combined feature map, and eventually predict a similar tensor, although now twice the size. We perform the same design one more time to predict boxes for the \ufb01nal scale. Thus our predictions for the 3rd scale bene\ufb01t from all the prior computation as well as \ufb01ne- grained features from early on in the network. We still use k-means clustering to determine our bound- ing box priors. We just sort of chose 9 clusters and 3 scales arbitrarily and then divide up the clusters evenly across scales. On the COCO dataset the 9 clusters were: (10\u00d713),(16\u00d730),(33\u00d723),(30\u00d761),(62\u00d745),(59\u00d7 119),(116 \u00d7 90),(156 \u00d7 198),(373 \u00d7 326). 2.4. Feature Extractor We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3\u00d73 and 1\u00d71 convolutional layers but now has some shortcut connections as well and is signi\ufb01cantly larger. It has 53 convolutional layers so we call it.... wait for it..... Darknet-53! Type Filters Size Output Convolutional 32 3x3 256 x 256 Convolutional 64 3x3/2 128x128 Convolutional 32 1x1 1x] Convolutional 64 3x3 Residua 128 x 128 Convolutional 128 3x3/2 64x64 Convolutional 64 1x1 2x} Convolutional 128 3x3 Residua 64 x 64 Convolutional 256 3x3/2 32x32 Convolutional 128 1x1 8x| Convolutional 256 3x3 Residua 32 x 32 Convolutional 512 3x3/2 16x16 Convolutional 256 1x1 8x} Convolutional 512 3x3 Residua 16 x 16 Convolutional 1024 3x3/2 8x8 Convolutional 512 1x1 4x} Convolutional 1024 3x3 Residua 8x8 Avgpool Global Connected 1000 Softmax Table 1. Darknet-53. This new network is much more powerful than Darknet- 19 but still more ef\ufb01cient than ResNet-101 or ResNet-152. Here are some ImageNet results: Backbone Top-1 Top-5 Bn Ops BFLOP/s Darknet-19 [15] 74.1 91.8 7.29 1246 ResNet-101[5] 77.1 93.7 19.7 1039 ResNet-152 [5] Darknet-53 77.6 77.2 93.8 93.8 29.4 18.7 1090 1457 FPS 171 53 37 78 models like RetinaNet in this metric though. However, when we look at the \u201cold\u201d detection metric of mAP at IOU= .5 (or AP50 in the chart) YOLOv3 is very strong. It is almost on par with RetinaNet and far above the SSD variants. This indicates that YOLOv3 is a very strong detector that excels at producing decent boxes for ob- jects. However, performance drops signi\ufb01cantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object. Table 2. Comparison of backbones. Accuracy, billions of oper- ations, billion \ufb02oating point operations per second, and FPS for various networks. Each network is trained with identical settings and tested at 256\u00d7256, single crop accuracy. Run times are measured on a Titan X at 256 \u00d7 256. Thus Darknet-53 performs on par with state-of-the-art classi\ufb01ers but with fewer \ufb02oating point operations and more speed. Darknet-53 is better than ResNet-101 and 1.5\u00d7 faster. Darknet-53 has similar perfor- mance to ResNet-152 and is 2\u00d7 faster. In the past YOLO struggled with small objects. How- ever, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high APS performance. However, it has comparatively worse performance on medium and larger size objects. More in- vestigation is needed to get to the bottom of this. When we plot accuracy vs speed on the AP50 metric (see \ufb01gure 5) we see YOLOv3 has signi\ufb01cant bene\ufb01ts over other detection systems. Namely, it\u2019s faster and better."
  },
  {
    "chunk_id": "4effc140-1cd5-4617-90de-24b4c743e67b",
    "modality": "text",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": 3,
    "retrieval_text": "4. Things We Tried That Didn\u2019t Work Darknet-53 also achieves the highest measured \ufb02oating point operations per second. This means the network struc- ture better utilizes the GPU, making it more ef\ufb01cient to eval- uate and thus faster. That\u2019s mostly because ResNets have just way too many layers and aren\u2019t very ef\ufb01cient. 2.5. Training We still train on full images with no hard negative mining or any of that stuff. We use multi-scale training, lots of data augmentation, batch normalization, all the standard stuff. We use the Darknet neural network framework for training and testing [14]. We tried lots of stuff while we were working on YOLOv3. A lot of it didn\u2019t work. Here\u2019s the stuff we can remember. Anchor box x,y offset predictions. We tried using the normal anchor box prediction mechanism where you pre- dict the x,y offset as a multiple of the box width or height using a linear activation. We found this formulation de- creased model stability and didn\u2019t work very well. Linear x,y predictions instead of logistic. We tried using a linear activation to directly predict the x,y offset instead of the logistic activation. This led to a couple point drop in mAP. 3. How We Do YOLOv3 is pretty good! See table 3. In terms of COCOs weird average mean AP metric it is on par with the SSD variants but is 3\u00d7 faster. It is still quite a bit behind other Focal loss. We tried using focal loss. It dropped our mAP about 2 points. YOLOv3 may already be robust to the problem focal loss is trying to solve because it has sep- arate objectness predictions and conditional class predic- tions. Thus for most examples there is no loss from the class predictions? Or something? We aren\u2019t totally sure. backbone AP AP50 AP75 APS APM APL Two-stage methods Faster R-CNN+++ [5] ResNet-101-C4 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w FPN [8] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [6] Inception-ResNet-v2 [21] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w TDM [20] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods YOLOv2 [15] DarkNet-19 [15] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [11, 3] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 DSSD513 [3] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [9] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 RetinaNet [9] ResNeXt-101-FPN 40.8 61.1 44.1 24.1 44.2 51.2 YOLOv3 608 \u00d7 608 Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 Table 3. I\u2019m seriously just stealing all these tables from [9] they take soooo long to make from scratch. Ok, YOLOv3 is doing alright. Keep in mind that RetinaNet has like 3.8\u00d7 longer to process an image. YOLOv3 is much better than SSD variants and comparable to state-of-the-art models on the AP50 metric. K lee) T W yotovs Method \u2014~@- RetinaNet-50 oo RetinaNet-101 mAP-50 time [B] SSD321 454. 61 [C] DSSD321 46.1 85 [D] R-FCN 51.9 85 [E] SSD513 50.4 125 [F] DSSD513 53.3 156 [G] FPN FRCN 59.1 172 RetinaNet-50-500 50.9 73 RetinaNet-101-500 53.1 90 RetinaNet-101-800 57.5 198 YOLOv3-320 51.5 22 YOLOv3-416 55.3 29 51 YOLOv3-608 57.9 \u00b0\u00b0TB] [c] 100 150 200 250 inference time (ms) Figure 3. Again adapted from the [9], this time displaying speed/accuracy tradeoff on the mAP at .5 IOU metric. You can tell YOLOv3 is good because it\u2019s very high and far to the left. Can you cite your own paper? Guess who\u2019s going to try, this guy \u2192 [16]. Oh, I forgot, we also \ufb01x a data loading bug in YOLOv2, that helped by like 2 mAP. Just sneaking this in here to not throw off layout. Dual IOU thresholds and truth assignment. Faster R- CNN uses two IOU thresholds during training. If a predic- tion overlaps the ground truth by .7 it is as a positive exam- ple, by [.3\u2212.7] it is ignored, less than .3 for all ground truth objects it is a negative example. We tried a similar strategy but couldn\u2019t get good results. We quite like our current formulation, it seems to be at a local optima at least. It is possible that some of these techniques could eventually produce good results, perhaps they just need some tuning to stabilize the training.",
    "raw_text": "4. Things We Tried That Didn\u2019t Work Darknet-53 also achieves the highest measured \ufb02oating point operations per second. This means the network struc- ture better utilizes the GPU, making it more ef\ufb01cient to eval- uate and thus faster. That\u2019s mostly because ResNets have just way too many layers and aren\u2019t very ef\ufb01cient. 2.5. Training We still train on full images with no hard negative mining or any of that stuff. We use multi-scale training, lots of data augmentation, batch normalization, all the standard stuff. We use the Darknet neural network framework for training and testing [14]. We tried lots of stuff while we were working on YOLOv3. A lot of it didn\u2019t work. Here\u2019s the stuff we can remember. Anchor box x,y offset predictions. We tried using the normal anchor box prediction mechanism where you pre- dict the x,y offset as a multiple of the box width or height using a linear activation. We found this formulation de- creased model stability and didn\u2019t work very well. Linear x,y predictions instead of logistic. We tried using a linear activation to directly predict the x,y offset instead of the logistic activation. This led to a couple point drop in mAP. 3. How We Do YOLOv3 is pretty good! See table 3. In terms of COCOs weird average mean AP metric it is on par with the SSD variants but is 3\u00d7 faster. It is still quite a bit behind other Focal loss. We tried using focal loss. It dropped our mAP about 2 points. YOLOv3 may already be robust to the problem focal loss is trying to solve because it has sep- arate objectness predictions and conditional class predic- tions. Thus for most examples there is no loss from the class predictions? Or something? We aren\u2019t totally sure. backbone AP AP50 AP75 APS APM APL Two-stage methods Faster R-CNN+++ [5] ResNet-101-C4 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w FPN [8] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [6] Inception-ResNet-v2 [21] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w TDM [20] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods YOLOv2 [15] DarkNet-19 [15] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [11, 3] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 DSSD513 [3] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [9] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 RetinaNet [9] ResNeXt-101-FPN 40.8 61.1 44.1 24.1 44.2 51.2 YOLOv3 608 \u00d7 608 Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 Table 3. I\u2019m seriously just stealing all these tables from [9] they take soooo long to make from scratch. Ok, YOLOv3 is doing alright. Keep in mind that RetinaNet has like 3.8\u00d7 longer to process an image. YOLOv3 is much better than SSD variants and comparable to state-of-the-art models on the AP50 metric. K lee) T W yotovs Method \u2014~@- RetinaNet-50 oo RetinaNet-101 mAP-50 time [B] SSD321 454. 61 [C] DSSD321 46.1 85 [D] R-FCN 51.9 85 [E] SSD513 50.4 125 [F] DSSD513 53.3 156 [G] FPN FRCN 59.1 172 RetinaNet-50-500 50.9 73 RetinaNet-101-500 53.1 90 RetinaNet-101-800 57.5 198 YOLOv3-320 51.5 22 YOLOv3-416 55.3 29 51 YOLOv3-608 57.9 \u00b0\u00b0TB] [c] 100 150 200 250 inference time (ms) Figure 3. Again adapted from the [9], this time displaying speed/accuracy tradeoff on the mAP at .5 IOU metric. You can tell YOLOv3 is good because it\u2019s very high and far to the left. Can you cite your own paper? Guess who\u2019s going to try, this guy \u2192 [16]. Oh, I forgot, we also \ufb01x a data loading bug in YOLOv2, that helped by like 2 mAP. Just sneaking this in here to not throw off layout. Dual IOU thresholds and truth assignment. Faster R- CNN uses two IOU thresholds during training. If a predic- tion overlaps the ground truth by .7 it is as a positive exam- ple, by [.3\u2212.7] it is ignored, less than .3 for all ground truth objects it is a negative example. We tried a similar strategy but couldn\u2019t get good results. We quite like our current formulation, it seems to be at a local optima at least. It is possible that some of these techniques could eventually produce good results, perhaps they just need some tuning to stabilize the training."
  },
  {
    "chunk_id": "9630ea96-e754-4d9f-a9d1-a16d1ab3aa30",
    "modality": "text",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": 4,
    "retrieval_text": "5. What This All Means prisingly dif\ufb01cult.\u201d [18] If humans have a hard time telling the difference, how much does it matter? But maybe a better question is: \u201cWhat are we going to do with these detectors now that we have them?\u201d A lot of the people doing this research are at Google and Facebook. I guess at least we know the technology is in good hands and de\ufb01nitely won\u2019t be used to harvest your personal infor- mation and sell it to.... wait, you\u2019re saying that\u2019s exactly what it will be used for?? Oh. Well the other people heavily funding vision research are the military and they\u2019ve never done anything horrible like killing lots of people with new technology oh wait.....1 YOLOv3 is a good detector. It\u2019s fast, it\u2019s accurate. It\u2019s not as great on the COCO average AP between .5 and .95 IOU metric. But it\u2019s very good on the old detection metric of .5 IOU. Why did we switch metrics anyway? The original COCO paper just has this cryptic sentence: \u201cA full discus- sion of evaluation metrics will be added once the evaluation server is complete\u201d. Russakovsky et al report that that hu- mans have a hard time distinguishing an IOU of .3 from .5! \u201cTraining humans to visually inspect a bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is sur- I have a lot of hope that most of the people using com- puter vision are just doing happy, good stuff with it, like counting the number of zebras in a national park [13], or tracking their cat as it wanders around their house [19]. But computer vision is already being put to questionable use and as researchers we have a responsibility to at least consider the harm our work might be doing and think of ways to mit- igate it. We owe the world that much. In closing, do not @ me. (Because I \ufb01nally quit Twitter). 1The author is funded by the Of\ufb01ce of Naval Research and Google. References [1] Analogy. Wikipedia, Mar 2018. 1 [2] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) chal- lenge. International journal of computer vision, 88(2):303\u2013 338, 2010. 6 [3] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg. Dssd: Deconvolutional single shot detector. arXiv preprint arXiv:1701.06659, 2017. 3 [4] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi. Iqa: Visual question answering in interactive environments. arXiv preprint arXiv:1712.03316, 2017. 1 [5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 770\u2013778, 2016. 3 [6] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. 3 [7] I. Krasin, T. Duerig, N. Alldrin, V. Ferrari, S. Abu-El-Haija, A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit, S. Belongie, V. Gomes, A. Gupta, C. Sun, G. Chechik, D. Cai, Z. Feng, D. Narayanan, and K. Murphy. Open- images: A public dataset for large-scale multi-label and multi-class image classi\ufb01cation. Dataset available from https://github.com/openimages, 2017. 2 [8] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2117\u20132125, 2017. 2, 3 [9] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00b4ar. Focal loss for dense object detection. arXiv preprint arXiv:1708.02002, 2017. 1, 3, 4 [10] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft coco: Com- mon objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014. 2 [11] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.- Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21\u201337. Springer, 2016. 3 [12] I. Newton. Philosophiae naturalis principia mathematica. William Dawson & Sons Ltd., London, 1687. 1 [13] J. Parham, J. Crall, C. Stewart, T. Berger-Wolf, and D. Rubenstein. Animal population censusing at scale with citizen science and photographic identi\ufb01cation. 2017. 4 [14] J. Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/darknet/, 2013\u20132016. 3 [15] J. Redmon and A. Farhadi. Yolo9000: Better, faster, stronger. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 6517\u20136525. IEEE, 2017. 1, 2, 3 [16] J. Redmon and A. Farhadi. Yolov3: An incremental improve- ment. arXiv, 2018. 4 [17] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To- wards real-time object detection with region proposal net- works. arXiv preprint arXiv:1506.01497, 2015. 2 [18] O. Russakovsky, L.-J. Li, and L. Fei-Fei. Best of both worlds: human-machine collaboration for object annotation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2121\u20132131, 2015. 4 [19] M. Scott. Smart camera gimbal bot scanlime:027, Dec 2017. 4 [20] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be- yond skip connections: Top-down modulation for object de- tection. arXiv preprint arXiv:1612.06851, 2016. 3 [21] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. 2017. 3 00 75 o YOLOv3 All the other slow ones \u00a9 * ao =? i 25 t) oO 50 100 150 200 Execution time (ms) 3 75 All the other slow ones \u00b0 YOLOv3 re) A we * \u00a3 FPS Figure 4. Zero-axis charts are probably more intellectually honest... and we can still screw with the variables to make ourselves look good!",
    "raw_text": "5. What This All Means prisingly dif\ufb01cult.\u201d [18] If humans have a hard time telling the difference, how much does it matter? But maybe a better question is: \u201cWhat are we going to do with these detectors now that we have them?\u201d A lot of the people doing this research are at Google and Facebook. I guess at least we know the technology is in good hands and de\ufb01nitely won\u2019t be used to harvest your personal infor- mation and sell it to.... wait, you\u2019re saying that\u2019s exactly what it will be used for?? Oh. Well the other people heavily funding vision research are the military and they\u2019ve never done anything horrible like killing lots of people with new technology oh wait.....1 YOLOv3 is a good detector. It\u2019s fast, it\u2019s accurate. It\u2019s not as great on the COCO average AP between .5 and .95 IOU metric. But it\u2019s very good on the old detection metric of .5 IOU. Why did we switch metrics anyway? The original COCO paper just has this cryptic sentence: \u201cA full discus- sion of evaluation metrics will be added once the evaluation server is complete\u201d. Russakovsky et al report that that hu- mans have a hard time distinguishing an IOU of .3 from .5! \u201cTraining humans to visually inspect a bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is sur- I have a lot of hope that most of the people using com- puter vision are just doing happy, good stuff with it, like counting the number of zebras in a national park [13], or tracking their cat as it wanders around their house [19]. But computer vision is already being put to questionable use and as researchers we have a responsibility to at least consider the harm our work might be doing and think of ways to mit- igate it. We owe the world that much. In closing, do not @ me. (Because I \ufb01nally quit Twitter). 1The author is funded by the Of\ufb01ce of Naval Research and Google. References [1] Analogy. Wikipedia, Mar 2018. 1 [2] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) chal- lenge. International journal of computer vision, 88(2):303\u2013 338, 2010. 6 [3] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg. Dssd: Deconvolutional single shot detector. arXiv preprint arXiv:1701.06659, 2017. 3 [4] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi. Iqa: Visual question answering in interactive environments. arXiv preprint arXiv:1712.03316, 2017. 1 [5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 770\u2013778, 2016. 3 [6] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. 3 [7] I. Krasin, T. Duerig, N. Alldrin, V. Ferrari, S. Abu-El-Haija, A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit, S. Belongie, V. Gomes, A. Gupta, C. Sun, G. Chechik, D. Cai, Z. Feng, D. Narayanan, and K. Murphy. Open- images: A public dataset for large-scale multi-label and multi-class image classi\ufb01cation. Dataset available from https://github.com/openimages, 2017. 2 [8] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2117\u20132125, 2017. 2, 3 [9] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00b4ar. Focal loss for dense object detection. arXiv preprint arXiv:1708.02002, 2017. 1, 3, 4 [10] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00b4ar, and C. L. Zitnick. Microsoft coco: Com- mon objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014. 2 [11] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.- Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21\u201337. Springer, 2016. 3 [12] I. Newton. Philosophiae naturalis principia mathematica. William Dawson & Sons Ltd., London, 1687. 1 [13] J. Parham, J. Crall, C. Stewart, T. Berger-Wolf, and D. Rubenstein. Animal population censusing at scale with citizen science and photographic identi\ufb01cation. 2017. 4 [14] J. Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/darknet/, 2013\u20132016. 3 [15] J. Redmon and A. Farhadi. Yolo9000: Better, faster, stronger. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 6517\u20136525. IEEE, 2017. 1, 2, 3 [16] J. Redmon and A. Farhadi. Yolov3: An incremental improve- ment. arXiv, 2018. 4 [17] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To- wards real-time object detection with region proposal net- works. arXiv preprint arXiv:1506.01497, 2015. 2 [18] O. Russakovsky, L.-J. Li, and L. Fei-Fei. Best of both worlds: human-machine collaboration for object annotation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2121\u20132131, 2015. 4 [19] M. Scott. Smart camera gimbal bot scanlime:027, Dec 2017. 4 [20] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be- yond skip connections: Top-down modulation for object de- tection. arXiv preprint arXiv:1612.06851, 2016. 3 [21] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. 2017. 3 00 75 o YOLOv3 All the other slow ones \u00a9 * ao =? i 25 t) oO 50 100 150 200 Execution time (ms) 3 75 All the other slow ones \u00b0 YOLOv3 re) A we * \u00a3 FPS Figure 4. Zero-axis charts are probably more intellectually honest... and we can still screw with the variables to make ourselves look good!"
  },
  {
    "chunk_id": "d4087c90-2323-42c0-8e82-3fa0d9fdec51",
    "modality": "text",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": 6,
    "retrieval_text": "Rebuttal We would like to thank the Reddit commenters, labmates, emailers, and passing shouts in the hallway for their lovely, heart- felt words. If you, like me, are reviewing for ICCV then we know you probably have 37 other papers you could be reading that you\u2019ll invariably put off until the last week and then have some legend in the \ufb01eld email you about how you really should \ufb01nish those re- views execept it won\u2019t entirely be clear what they\u2019re saying and maybe they\u2019re from the future? Anyway, this paper won\u2019t have be- come what it will in time be without all the work your past selves will have done also in the past but only a little bit further forward, not like all the way until now forward. And if you tweeted about it I wouldn\u2019t know. Just sayin. Reviewer #2 AKA Dan Grossman (lol blinding who does that) insists that I point out here that our graphs have not one but two non-zero origins. You\u2019re absolutely right Dan, that\u2019s because it looks way better than admitting to ourselves that we\u2019re all just here battling over 2-3% mAP. But here are the requested graphs. I threw in one with FPS too because we look just like super good when we plot on FPS. Reviewer #4 AKA JudasAdventus on Reddit writes \u201cEntertain- ing read but the arguments against the MSCOCO metrics seem a bit weak\u201d. Well, I always knew you would be the one to turn on me Judas. You know how when you work on a project and it only comes out alright so you have to \ufb01gure out some way to justify how what you did actually was pretty cool? I was basically trying to do that and I lashed out at the COCO metrics a little bit. But now that I\u2019ve staked out this hill I may as well die on it. See here\u2019s the thing, mAP is already sort of broken so an up- date to it should maybe address some of the issues with it or at least justify why the updated version is better in some way. And that\u2019s the big thing I took issue with was the lack of justi\ufb01cation. For PASCAL VOC, the IOU threshold was \u201dset deliberately low to ac- count for inaccuracies in bounding boxes in the ground truth data\u201c [2]. Does COCO have better labelling than VOC? This is de\ufb01- nitely possible since COCO has segmentation masks maybe the labels are more trustworthy and thus we aren\u2019t as worried about inaccuracy. But again, my problem was the lack of justi\ufb01cation. precise bounding boxes are more important than better classi\ufb01- cation? A miss-classi\ufb01ed example is much more obvious than a bounding box that is slightly shifted. mAP is already screwed up because all that matters is per-class rank ordering. For example, if your test set only has these two images then according to mAP two detectors that produce these results are JUST AS GOOD: Figure 5. These two hypothetical detectors are perfect according to mAP over these two images. They are both perfect. Totally equal. Now this is OBVIOUSLY an over-exaggeration of the prob- lems with mAP but I guess my newly retconned point is that there are such obvious discrepancies between what people in the \u201creal world\u201d would care about and our current metrics that I think if we\u2019re going to come up with new metrics we should focus on these discrepancies. Also, like, it\u2019s already mean average preci- sion, what do we even call the COCO metric, average mean aver- age precision? Here\u2019s a proposal, what people actually care about is given an image and a detector, how well will the detector \ufb01nd and classify objects in the image. What about getting rid of the per-class AP and just doing a global average precision? Or doing an AP calcu- lation per-image and averaging over that? The COCO metric emphasizes better bounding boxes but that emphasis must mean it de-emphasizes something else, in this case classi\ufb01cation accuracy. Is there a good reason to think that more Boxes are stupid anyway though, I\u2019m probably a true believer in masks except I can\u2019t get YOLO to learn them.",
    "raw_text": "Rebuttal We would like to thank the Reddit commenters, labmates, emailers, and passing shouts in the hallway for their lovely, heart- felt words. If you, like me, are reviewing for ICCV then we know you probably have 37 other papers you could be reading that you\u2019ll invariably put off until the last week and then have some legend in the \ufb01eld email you about how you really should \ufb01nish those re- views execept it won\u2019t entirely be clear what they\u2019re saying and maybe they\u2019re from the future? Anyway, this paper won\u2019t have be- come what it will in time be without all the work your past selves will have done also in the past but only a little bit further forward, not like all the way until now forward. And if you tweeted about it I wouldn\u2019t know. Just sayin. Reviewer #2 AKA Dan Grossman (lol blinding who does that) insists that I point out here that our graphs have not one but two non-zero origins. You\u2019re absolutely right Dan, that\u2019s because it looks way better than admitting to ourselves that we\u2019re all just here battling over 2-3% mAP. But here are the requested graphs. I threw in one with FPS too because we look just like super good when we plot on FPS. Reviewer #4 AKA JudasAdventus on Reddit writes \u201cEntertain- ing read but the arguments against the MSCOCO metrics seem a bit weak\u201d. Well, I always knew you would be the one to turn on me Judas. You know how when you work on a project and it only comes out alright so you have to \ufb01gure out some way to justify how what you did actually was pretty cool? I was basically trying to do that and I lashed out at the COCO metrics a little bit. But now that I\u2019ve staked out this hill I may as well die on it. See here\u2019s the thing, mAP is already sort of broken so an up- date to it should maybe address some of the issues with it or at least justify why the updated version is better in some way. And that\u2019s the big thing I took issue with was the lack of justi\ufb01cation. For PASCAL VOC, the IOU threshold was \u201dset deliberately low to ac- count for inaccuracies in bounding boxes in the ground truth data\u201c [2]. Does COCO have better labelling than VOC? This is de\ufb01- nitely possible since COCO has segmentation masks maybe the labels are more trustworthy and thus we aren\u2019t as worried about inaccuracy. But again, my problem was the lack of justi\ufb01cation. precise bounding boxes are more important than better classi\ufb01- cation? A miss-classi\ufb01ed example is much more obvious than a bounding box that is slightly shifted. mAP is already screwed up because all that matters is per-class rank ordering. For example, if your test set only has these two images then according to mAP two detectors that produce these results are JUST AS GOOD: Figure 5. These two hypothetical detectors are perfect according to mAP over these two images. They are both perfect. Totally equal. Now this is OBVIOUSLY an over-exaggeration of the prob- lems with mAP but I guess my newly retconned point is that there are such obvious discrepancies between what people in the \u201creal world\u201d would care about and our current metrics that I think if we\u2019re going to come up with new metrics we should focus on these discrepancies. Also, like, it\u2019s already mean average preci- sion, what do we even call the COCO metric, average mean aver- age precision? Here\u2019s a proposal, what people actually care about is given an image and a detector, how well will the detector \ufb01nd and classify objects in the image. What about getting rid of the per-class AP and just doing a global average precision? Or doing an AP calcu- lation per-image and averaging over that? The COCO metric emphasizes better bounding boxes but that emphasis must mean it de-emphasizes something else, in this case classi\ufb01cation accuracy. Is there a good reason to think that more Boxes are stupid anyway though, I\u2019m probably a true believer in masks except I can\u2019t get YOLO to learn them."
  },
  {
    "chunk_id": "beba58f2-e958-4cc4-8c92-1dca2357a2ba",
    "modality": "image",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": null,
    "retrieval_text": " The image is a screenshot of a graphical user interface displaying a line graph with multiple lines representing different data sets over time. The x-axis of the graph is labeled \"Inference Time (s)\" and ranges from 0 to 250 seconds, while the y-axis is labeled \"YoloV3\" and indicates values ranging from 0 to 100. There are three lines on the graph: one in blue, one in red, and one in purple. Each line represents a different data set or model, as indicated by the labels \"yolov3\", \"retinanet\", and \"baseline\" respectively.\n\nAt the top of the image, there is a title that reads \"YoloV3\". Below the graph, there are two sets of numbers: on the left side, there's a sequence of numbers from 0 to 250, which likely represent the time intervals along the x-axis of the graph. On the right side, there's another set of numbers that correspond to the values of the \"YoloV3\" line at each time interval.\n\nThe image also contains a legend with three colored circles corresponding to the colors of the lines on the graph: blue, red, and purple. Each circle is labeled with the name of the data set or model it represents: \"yolov3\", \"retinanet\", and \"baseline\".\n\nIn the top right corner of the image, there's a small text that reads \"YoloV3 50% Retinanet 60% Baseline 40%\", which seems to be a comparison or summary of the performance of the different models at a certain point in time. The numbers next to each model name indicate their respective percentages, suggesting some form of comparison metric for the models' performance over time.\n\nThe overall style of the image is informational and technical, typical of data visualization used in machine learning or computer vision research to compare different models or algorithms. ",
    "raw_text": null
  },
  {
    "chunk_id": "9a9be067-16db-4594-8b84-43ba0288f397",
    "modality": "image",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": null,
    "retrieval_text": " The image is a black and white photograph of a piece of paper with mathematical equations and symbols on it. At the top left corner, there's a hand-drawn rectangle with a blue border, containing a series of mathematical expressions. Inside this rectangle, there are several mathematical notations including Greek letters (alpha, beta, gamma), subscripts (i, j, k), and various mathematical operators such as summation (\u2211) and product (\u220f). The equations involve variables like x_ij, y_ij, z_ij, and a matrix A.\n\nBelow the rectangle, there is another hand-drawn rectangle with a blue border, which appears to be a grid or matrix with rows and columns labeled from i=1 to i=n and j=1 to j=m. The cells of this grid contain mathematical expressions involving variables like x_ij, y_ij, z_ij, and the matrix A again.\n\nThe paper is placed on a surface that has a grid pattern, suggesting it might be a graph or a table with rows and columns labeled from i=1 to i=n and j=1 to j=m. The background of the image is not clearly visible due to the focus on the paper. There are no texts outside of the mathematical expressions within the rectangles. ",
    "raw_text": null
  },
  {
    "chunk_id": "28c336c5-80e7-4013-86f7-d50d30305387",
    "modality": "image",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": null,
    "retrieval_text": " The image is a graphical representation of data, specifically a line graph with a scatter plot overlaid on top. The graph has a title that reads \"YOLO_01,\" which suggests it may be related to an artificial intelligence model or project. The x-axis of the graph is labeled \"Cocoa_time (s)\" and ranges from 0 to 60 seconds, indicating time in seconds. The y-axis is labeled \"COCOA_mAh\" and ranges from 0 to 50 milliampere-hours (mAh), which likely refers to the capacity of a battery or energy source.\n\nThere are three lines on the graph: one in red, one in blue, and one in purple. Each line represents different data sets, as indicated by the legend at the top right corner of the image. The red line is labeled \"YOLO_01,\" the blue line is labeled \"RetinaNet_01,\" and the purple line is labeled \"Method_01.\" These labels suggest that the graph is comparing the performance or output of different models or methods over time.\n\nThe scatter plot on top of the line graph shows additional data points, each marked with a small circle. The circles are color-coded to match the lines they represent: red for YOLO_01, blue for RetinaNet_01, and purple for Method_01. Each point is labeled with a number, which could be an index or identifier for the data point.\n\nThe graph also includes a legend at the top right corner that explains the symbols used in the scatter plot: \"YOLO_01,\" \"RetinaNet_01,\" and \"Method_01.\" The numbers next to each symbol correspond to the data points on the scatter plot, indicating which model or method is represented by each point.\n\nThe overall style of the image is informational and technical, typical of scientific or engineering presentations that aim to compare different models or methods over time. ",
    "raw_text": null
  },
  {
    "chunk_id": "4cebcf9e-75c0-456f-a89c-aa0509861a79",
    "modality": "image",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": null,
    "retrieval_text": " The image is a screenshot of a computer screen displaying a graph with various data points and annotations. The graph appears to be a scatter plot with two sets of data represented by different symbols: black squares for \"YOLO\" and blue circles for \"VLO.\" There are also two lines on the graph, one in red and another in green, which seem to represent some form of trend or relationship between the two sets of data.\n\nThe x-axis of the graph is labeled with the word \"Execution time (s)\" and has numerical values ranging from 0 to approximately 15 seconds. The y-axis is labeled with \"YOLO\" at the top, indicating that it represents the YOLO (You Only Look Once) algorithm's performance metric, which is not specified but could be accuracy or another measure of performance.\n\nThere are annotations on the graph:\n- A red line with a label \"YOLO\" and a note \"the slowest worker.\"\n- A green line with a label \"VLO\" and a note \"the fastest worker.\"\n- Two black squares labeled \"YOLO\" with the values 0.15 and 0.25, indicating specific data points for YOLO.\n- Three blue circles labeled \"VLO\" with the values 0.30, 0.40, and 0.50, representing specific data points for VLO.\n\nThe image has a watermark or logo in the bottom right corner that reads \"captured by,\" but the rest of the text is cut off. The overall style of the image suggests it's a technical representation of performance metrics for two different algorithms or systems over time, with annotations highlighting specific points of interest. ",
    "raw_text": null
  },
  {
    "chunk_id": "49c88db8-3f38-4b8b-bd86-39ac275c68a2",
    "modality": "image",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of two photographs placed side by side, with an overlay of a computer vision system's output. On the left side of the image, there is a photograph showing a camel standing in a grassy field with mountains in the background. To the right of this photo, there is another photograph featuring a dog and a horse standing together on a grassy surface.\n\nOverlaying both photographs are bounding boxes with labels indicating the detected objects. In the left image, the bounding box around the camel has the label \"Camel\" with a confidence score of 0.987. The bounding box around the dog in the right image has the label \"Dog\" with a confidence score of 0.987.\n\nThe overlay also includes a text box at the top left corner that reads \"Detector #1,\" suggesting this is the first instance of an object detection system being used. The overall style of the image suggests it is a screenshot from a computer vision application demonstrating object detection capabilities, with the detected objects highlighted and labeled for identification. ",
    "raw_text": null
  },
  {
    "chunk_id": "232e8096-c4be-4bb7-8c5e-b47345c195a8",
    "modality": "image",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of several elements, primarily a photograph and an overlaid computer vision system's output. In the center of the image, there is a person riding a horse. The rider appears to be wearing traditional attire with a red and white patterned headscarf, and the horse has a saddle on its back.\n\nOverlaying this scene is a grid-like structure that seems to represent a computer vision system's output. This includes bounding boxes around objects within the image, each labeled with a unique identifier. The labels are as follows:\n\n1. \"bird\" - located in the top left corner of the image.\n2. \"person\" - located in the center of the image, encompassing both the rider and the horse.\n3. \"horse\" - located around the horse, indicating its presence within the scene.\n4. \"background\" - labeled with a green bounding box that encompasses the landscape behind the rider and horse.\n5. \"bird\" - labeled with a blue bounding box in the top left corner of the image, which is likely an error or mislabeling since there is no bird present in the scene.\n\nThe background shows a barren landscape with sparse vegetation, suggesting a desert environment. The sky is clear and blue, indicating good weather conditions. The overall style of the image suggests it may have been taken from a surveillance camera or a similar type of security equipment, given the overlay of bounding boxes and labels typically used in computer vision applications for object detection and tracking. ",
    "raw_text": null
  },
  {
    "chunk_id": "f6562f16-7574-4962-8b32-4263b952e9ed",
    "modality": "image",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of two photographs placed side by side, each depicting a different scene with an overlay of computer vision detection results. On the left side, there's a photograph showing a grassy field with a camel standing in the foreground and a horse in the background. In the right side, there's another photograph featuring a similar landscape but with a cow instead of a horse.\n\nOverlaying both images are bounding boxes with labels indicating detected objects. The left image has two labeled bounding boxes: one around the camel and one around the horse. The right image also has two labeled bounding boxes, one around the cow and another around the camel. Each box is colored differently to distinguish between them.\n\nAt the top of the image, there's a text that reads \"Detector 2,\" suggesting this is the second iteration or version of the object detection system being used. The style of the image suggests it is a screenshot from a computer vision application demonstrating the ability to detect and label animals in different environments. ",
    "raw_text": null
  },
  {
    "chunk_id": "4557c20e-3103-420a-b8e3-2543e433fa78",
    "modality": "image",
    "source_pdf": "YOLOv3 An Incremental Improvement (Redmon & Farhadi, 2018).pdf",
    "page_number": null,
    "retrieval_text": " The image is a composite of several photographs arranged in a grid format, each depicting different aspects of a scene. At the top left, there's an aerial view of a person riding a horse, with the rider wearing a red helmet and holding a lance. Below this, on the left side, there's a close-up image showing a person in a red helmet and a black outfit, seemingly interacting with the horse.\n\nIn the center of the grid, there's an image of a person standing next to a large bird, which appears to be an eagle due to its size and shape. The person is wearing a white shirt and has their back turned to the camera.\n\nOn the right side, at the top, there's a close-up image of a person in a red helmet and a black outfit, holding a lance and standing next to a horse. Below this, on the bottom right, there's an image showing a person in a red helmet and a black outfit, riding a horse with a large bird perched on its back.\n\nThe images are overlaid with a grid of bounding boxes, each box containing a different object or element from the scene. The objects are labeled with text that includes terms like \"person,\" \"horse,\" \"eagle,\" and \"lance.\" Each label is accompanied by a confidence score ranging from 0 to 1, indicating the certainty of the detection.\n\nThe image has a watermark at the bottom left corner that reads \"Image Copyright \u00a9 2023,\" suggesting the copyright holder's claim on the image. The style of the image suggests it is a screenshot from an object detection system used for analyzing and identifying objects within a scene, possibly for surveillance or security purposes. ",
    "raw_text": null
  }
]